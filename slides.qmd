---
title: "Initiation √† Spark avec R en mode cluster"
format: 
  revealjs:
    incremental: true
    slide-numbers : true
---

## Au programme {.smaller}

1.  MiDAS : une base de donn√©es volumineuse üíæ

2.  Manipuler un appariement : une op√©ration co√ªteuse üí≤

3.  Initiation au calcul distribu√© : quelles ressources r√©server ? üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è

4.  Sparklyr : la solution ergonomique de spark sous R üë®‚Äçüíª

5.  Pour aller plus loin ‚è©

## MiDAS : une base de donn√©es volumineuse {.smaller}

MiDAS croise trois bases de donn√©es administratives exhaustives :

-   les donn√©es sur **l'inscription et l'indemnisation des demandeurs d'emploi** de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;

-   les donn√©es sur les b√©n√©ficiaires de **minima sociaux** (RSA, PPA, AAH) et les caract√©ristiques des **m√©nages** de la CNAF : Allstat-FR6 ;

-   les donn√©es sur les **contrats salari√©s** de la DSN : MMO de la Dares.

## MiDAS : une base de donn√©es volumineuse {.smaller}

Chaque vague de MiDAS correspond √† environ **600 Go** de donn√©es au format sas. Les vagues fonctionnent par empilement :

-   le gain de **profondeur temporelle** et l'entr√©e dans le champ de nouvelles personnes

-   les **vagues sont appariables entre elles**

## MiDAS : une base de donn√©es volumineuse {.smaller}

MiDAS est l'une des bases de donn√©es les plus volumineuses du SSP :![Quelques bases du SSP](donnees_ssp.PNG)

Les administrations dont les donn√©es sont comparables √† MiDAS utilisent un cluster Spark : Insee, Drees, Acoss...

‚ñ∂Ô∏èLe cluster spark est la solution la plus efficiente pour traiter des donn√©es de cette ampleur. Apprendre √† l'utiliser pourra vous √™tre utile dans d'autres contextes que celui de la Dares.

## Structure de l'appariement {.smaller}

![](structure_midas.PNG){fig-align="center"}

::: callout-tip
## Pourquoi Spark ?

La manipulation des donn√©es MiDAS en l'√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.
:::

## Le format parquet {.smaller .scrollable}

Les donn√©es sont converties au **format parquet** d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l'espace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :

-   il **compresse** efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv

-   il est orient√© **colonnes**

-   il permet le chargement efficace **en m√©moire** des donn√©es

-   Il permet le **stockage partitionn√©** des donn√©es

-   il permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires **sur disque**

-   Il est **ind√©pendant du logiciel** utilis√© : il peut donc √™tre trait√© par spark et par R.

# Manipuler un appariement : une op√©ration co√ªteuse

## L'espace MiDares {.smaller .scrollable}

::: panel-tabset
### Ressources

Des ressources partag√©es entre tous les utilsateurs simultan√©s :

-   512 Go de m√©moire vive (ou RAM) : passage √† 256 Go

::: callout-note
## La m√©moire vive

La m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa **rapidit√©**, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa **volatilit√©** (toutes les donn√©es sont perdues si l'ordinateur n'est plus aliment√©) et par l'acc√®s direct aux informations qui y sont stock√©es, **quasi instantann√©**.
:::

-   Un processeur (ou CPU) compos√© de 32 coeurs : passage √† 16 coeurs

::: callout-note
## Le processeur

Le processeur permet d'**ex√©cuter des t√¢ches et des programmes** : convertir un fichier, ex√©cuter un logiciel... Il est compos√© d'un ou de plusieurs **coeurs** : un coeur ne peut ex√©cuter qu'une seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu'il a de coeurs. Un processeur se caract√©rise aussi par sa **fr√©quence** : elle est globalement proportionnelle au nombre d'op√©rations qu'il est capable d'effetuer par seconde.
:::

### Sch√©ma

![](schema_ordinateur.png)
:::

## Programmer en m√©moire vive {.smaller}

-   **R : la m√©moire vive, √©tat dans l'environnement**

-   SAS : lecture/√©criture sur le disque

-   MiDAS au format sas \>\> taille de la m√©moire vive disponible du serveur CASD --\> format `.parquet`

-   **Impossible de charger tout MiDAS en m√©moire vive**

    Des solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :

-   `arrow` (avec des requ√™tes `dplyr`)

-   `duckDB` : recommand√© par le SSPLab pour des donn√©es jusqu'√† 100Go

    ‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements.

## Les traitements co√ªteux en puissance de calcul {.smaller}

-   les jointures

-   les op√©rations en `group_by()`

-   `distinct()`

    ‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)

    ‚ñ∂Ô∏è Erreur "out of memory".

## Un traitement peu co√ªteux : un traitement MAP {.smaller}

![](formation%20sparklyr-Page-1.drawio.png){fig-align="center" width="800"}

Ce traitement est peu co√ªteux :

-   chargement d'une seule colonne en RAM : format parquet orient√© colonnes

-   peu de m√©moire d'ex√©cution : R est un langage vectoris√©

## Un traitement co√ªteux : un traitement REDUCE {.smaller}

![](formation%20sparklyr-Page-2.drawio.png){fig-align="center" width="850"}

Ce traitement n√©cessite :

-   le chargement de davantage de colonnes en m√©moire vive ;

-   davantage de m√©moire d'ex√©cution pour effectuer l'intersection (`inner_join()`).

# Initiation au calcul distribu√©

## Calcul distribu√© et calcul parall√®le {.smaller}

::: columns
::: {.column width="50%"}
### Calcul non distribu√©

Les probl√©matiques Big Data en R sont les suivantes :

-   la taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R

-   le temps de calcul : les √©tapes du traitement sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)

-   l'optimisation du programme
:::

::: {.column width="50%"}
### Calcul distribu√© spark

Le calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :

-   chargement des donn√©es en m√©moire parcimonieux et non syst√©matique

-   ex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents

-   optimisation automatique du code
:::
:::

## Un traitement MAP distribu√© {.smaller}

![](map_distribue.drawio.png)

Si les donn√©es sont stock√©es sur diff√©rents ordinateurs :

-   les calculs peuvent √™tre effectu√©s en parall√®le ;

-   gain de temps li√© √† l'augmentation des ressources informatiques pour effectuer le calcul et √† la parall√©lisation.

Les traitements MAP se pr√™tent parfaitement au calcul distribu√© et parall√®le.

## Un traitement REDUCE distribu√© {.smaller}

![](reduce_distribue.drawio.png)

Si les donn√©es sont stock√©es sur diff√©rents ordinateurs :

-   il faut les rappatrier au m√™me endroit pour effectuer la jointure ;

-   cet √©change est effectu√© en r√©seau entre les ordinateurs : l'envoi r√©seau a un co√ªt non n√©gligeable en temps.

Les traitements REDUCE ne se pr√™tent pas bien au calcul distribu√© et parall√®le.

## Spark {.smaller .scrollable}

-   Apache Spark : **librairie open source** d√©velopp√©e dans le langage `scala`

-   **Scala** : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java

    ```{r}
    #| eval: false
    #| echo: true

    val TopHorrorsIGN2022 = Seq(
      (9, "Pearl"),
      (6, "The Sadness"),
      (6, "Offseason"),
      (7, "Hatching"),
      (8, "x")
    ).toDF("IMDB Rating", "IGN Movie Picks")

    val TopHorrorsTheAVClub2022 = Seq(
      (7, "Nope"),
      (9, "Pearl"),
      (8, "x"),
      (5, "Barbarian"),
      (5, "Bones And All")
    ).toDF("IMDB Rating", "AVC Movie Picks")

    import org.apache.spark.sql.functions.col

    val cols = List(col("IGN Movie Picks"), col("AVC Movie Picks"))

    val query = TopHorrorsIGN2022(
      "IGN Movie Picks"
    ) === TopHorrorsTheAVClub2022("AVC Movie Picks")

    val outerJoin = TopHorrorsIGN2022
      .join(TopHorrorsTheAVClub2022, query, "outer")
      .select(cols: _*)

    outerJoin.show()
    ```

-   `scala` adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de `spark` et optimiser au maximum les traitements en `spark`

-   `spark` est **compatible avec les langages** `scala`, `R`, `python`, `java`, et peut interpr√©ter des commandes **SQL.**

-   Deux packages existent sous R :

    -   **sparkR** propos√© par Apache Spark

    -   **sparklyr**, qui permet d'utiliser directement des commandes dplyr traduites en spark par le package.

## Installation de spark sous CASD {.smaller}

Voir la fiche d√©di√©e sur le site

## La machine virtuelle Java {.smaller}

Spark est r√©dig√© en scala, un langage qui a besoin d'une machine virtuelle Java pour √™tre ex√©cut√©. La machine virtuelle Java est scalable : l'utilisateur peut choisir quelles ressources physiques elle a le droit d'utiliser sur l'ensemble des ressources physiques disponibles sur l'ordinateur. C'est un mini ordinateur cr√©√© par spark √† l'int√©rieur de notre propre ordinateur, qui utilise les ressources de ce dernier.

::: callout-note
## Machine virtuelle

Une machine virtuelle a les m√™mes caract√©ristiques qu'un ordinateur :

-   un syst√®me d'exploitation : Windows, Linux, MacOS

-   des ressources physiques : CPU, RAM et stockage disque

La diff√©rence avec un ordinateur : une machine virtuelle peut √™tre cr√©√©e sur un serveur physique en r√©servant une petite partie des ressources du serveur seulement, ce qui permet de cr√©er plusieurs ordinateurs diff√©rents sur une seule infractructure physique
:::

## Deux mani√®res d'utiliser Spark {.smaller .scrollable}

::: columns
::: {.column width="50%"}
### Avec un seul ordinateur

Ce mode est appel√© Spark local.

-   une unique machine virtuelle Java est cr√©√©e pour ex√©cuter le code spark

-   t√¢ches parall√©lis√©es sur les diff√©rents coeurs (CPU) du processeur de la machine virtuelle Java

-   l'ordinateur sur lequel est cr√©√©e cette machine virtuelle Java est la bulle MiDARES, √©quivalent d'un unique gros ordinateur
:::

::: {.column width="50%"}
### Sur un cluster de calcul

Un cluster de calcul est un ensemble d'ordinateurs ou machines virtuelles connect√©s en r√©seau.

-   une machine virtuelle Java est cr√©√©e par spark dans chaque ordinateur du cluster

-   t√¢ches parall√©lis√©es sur les diff√©rents ordinateurs du cluster

-   la session R reste sur la bulle MiDARES, le code R est traduit en scala puis envoy√© sur le cluster pour √™tre ex√©cut√©.
:::
:::

## Mode local : sch√©ma {.smaller}

![](mode_local.PNG)

## Mode local : √† √©viter {.smaller}

En mode local :

-   les ressources utilis√©es par la machine virtuelle sont celles de la bulle

-   il faut allouer suffisamment de coeurs √† la JVM pour parall√©liser

-   m√™me si l'utilisateur choisit des ressources faibles, les ressources r√©elles utilis√©es dans une session spark peuvent √™tre plus √©lev√©es : mauvaise gestion de l'allocation des ressources

-   acc√©l√©ration sensible par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources

-   Sur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local : l'utilisation simultan√©e de spark par plusieurs membres de la bulle entra√Ænent des ralentissements consid√©rables

    ‚ñ∂Ô∏èmode local √† √©viter absolument

## Le cluster de calcul Midares : pr√©sentation {.smaller}

![](schema_cluster.drawio.png){fig-align="center" width="691"}

## Se connecter √† Spark sur un cluster {.smaller}

Se connecter √† spark revient √† demander √† spark de cr√©er toutes les JVM demand√©es capables de lire du scala.

Pour se connecter √† spark depuis R avec le package `sparklyr` :

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "4"

library(sparklyr)

conf <- spark_config()
conf$spark.executor.instances <- 5
sc <- spark_connect(master = "yarn", config = conf)
    
```

Le param√®tre `spark.executor.instances` correspond au nombre d'ordinateurs sur lequel on souhaite parall√©liser le travail d'ex√©cution de code. Ici, l'utilisateur demande 5 ordinateurs du cluster.

Nous verrons plus loin quels param√®tres nous devons pr√©ciser dans le fichier de configuration.

## Une connexion {.smaller}

Toutes les JVM demand√©es (5) sont instanci√©es dans les ordinateurs du cluster, avec les param√®tres d√©finis.

![](schema_cluster.drawio.png){fig-align="center" width="691"}

## La vie d'un programme r√©dig√© en sparklyr {.smaller}

Avec `sparklyr`, il est possible de programmer directement en `dplyr` pour utiliser spark.

```{r}
#| eval: false
#| echo: true

# un data frame que j'envoie dans spark
un_df <- data.frame(c(1,2,3), c("A", "B", "C"))
names(un_df) <- c("col_num", "col_char")

# C'est maintenant un spark_data_frame
copy_to(sc, un_df)
    
un_df_transforme <- un_df %>%
    mutate(une_nouvelle_col = col_num*2)

```

Si j'ex√©cute ce programme, je ne pourrai pas ouvrir `un_df_transforme`, d'ailleurs, il ne se sera rien pass√©.

## La lazy evaluation {.smaller .scrollable}

Spark distingue deux types d'op√©rations :

-   **les transformations :** ce sont des op√©rations qui prennent en entr√©e un `spark_data_frame` et retournent un `spark_data_frame`, elles ne d√©clenchent aucun calcul lorsqu'elles sont appel√©es.

    Par exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d'ex√©cution :

    ```{r}
    #| eval: false
    #| echo: true

    une_transformation <- un_spark_data_frame %>%
      group_by(identifiant) %>%
      mutate(une_somme = sum(revenus))
    ```

-   **les actions :** ce sont des op√©rations qui demandent le calcul d'un r√©sultat et qui d√©clenchent le calcul et l'ex√©cution de toutes les transformations compil√©es jusqu'√† l'appel de l'action.

    Par exemple, le programme ci-dessous d√©clenche le calcul de la cellule `une_transformation` et de la moyenne des revenus :

    ```{r}
    #| eval: false
    #| echo: true
    revenu_moyen <- une_transformation %>%
      summarise(revenu_moyen = mean(une_somme)) %>%
      print()
    ```

    Les principales actions sont : `print()`, `collect()`, `head()`, `tbl_cache()` (√©crire un `spark_data_frame` en m√©moire pour le r√©utiliser).

## La vie d'un programme r√©dig√© en sparklyr

Prenons l'exemple d'un programme contenant une action.

```{r}
#| eval: false

# un data frame que j'envoie dans spark
un_df <- data.frame(c(1,2,3,4,5), c("A", "B", "C", "D", "E"))
names(un_df) <- c("col_num", "col_char")

# C'est maintenant un spark_data_frame
copy_to(sc, un_df)
    
un_df_transforme <- un_df %>%
    mutate(une_nouvelle_col = col_num*2) %>%
    head(3) %>%
    print()

```

## Le r√¥le du driver {.smaller}

![](schema_cluster.drawio.png){fig-align="center" width="400"}

-   Le programme R est traduit en scala gr√¢ce au package `sparklyr`

-   Le driver √©value le programme, il lit le code `scala` mais n'ex√©cute rien du tout

-   S'il remarque une erreur, l'erreur est envoy√©e directement √† l'utilisateur en session R avant l'ex√©cution du programme : c'est la force de la lazy evaluation.

## Le plan d'ex√©cution {.smaller}

![](catalyst.jpg)

source : documentation CASD disponible √† [Documentation Data Science](https://casd-eu.gitbook.io/data-science/)

AJOUTER UN DAG

## Le r√¥le du driver : Catalyst {.smaller .scrollable}

Le driver contient un programme nomm√© Catalyst qui optimise le code `scala` automatiquement.

Spark optimise automatiquement les programmes soumis :

1.  Compilation des transformations pour soulever les √©ventuelles erreurs

2.  Int√©gration dans un **plan d'ex√©cution** contenant les √©tapes n√©cessaires pour parvenir au r√©sultat demand√© par le programme

3.  Optimisation du plan logique par le module **Catalyst** (driver Spark)

Par exemple si j'√©cris le programme :

```{r}
#| eval: false 
#| echo: true  

non_optimal <- table_1 %>%   
    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %>%   
    filter(debut_contrat >= as.Date("2023-01-01"))
```

```         
Catalyst r√©√©crit :
```

```{r}
#| eval: false 
#| echo: true  

optimal <- table_1 %>%   
    filter(debut_contrat >= as.Date("2023-01-01")) %>%   
    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))
```

Cette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu'une action d√©clenche l'ex√©cution.

## Le r√¥le du driver : Catalyst {.smaller .scrollable}

![](dag.webp)

## Le r√¥le du driver : Catalyst {.smaller .scrollable}

4.  R√©alisation de plans physiques possibles et s√©lection du **meilleur plan physique** (au regard de la localisation des donn√©es requises). Le plan physique est la distribution des diff√©rents calculs aux machines du cluster.

5.  **D√©clencher le moins d'actions possibles** dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.

6.  Pour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique.

## Le r√¥le du cluster manager {.smaller}

![](schema_cluster.drawio.png){fig-align="center" width="400"}

Le cluster manager distribue les traitements physiques aux ordinateurs du cluster :

-   il conna√Æt le meilleur plan physique fourni par Catalyst ;

-   il conna√Æt les ressources disponibles et occup√©es par toutes les machines du cluster ;

-   il affecte les ressources disponibles √† la session spark.

## Le r√¥le du worker {.smaller}

![](schema_cluster.drawio.png){fig-align="center" width="400"}

Le worker effectue le morceau de programme qu'on lui affecte et renvoie le r√©sultat au driver, qui lui-m√™me affiche le r√©sultat en session R :

-   il ne conna√Æt que les t√¢ches qu'on lui a affect√©es ;

-   il peut communiquer avec le driver en r√©seau pour renvoyer un r√©sultat ;

-   il peut communiquer avec les autres workers en r√©seau pour partager des donn√©es ou des r√©sultats interm√©diaires : c'est un shuffle.

## O√π sont les donn√©es ? {.smaller}

![](schema_cluster.drawio.png){fig-align="center" width="400"}

## O√π sont les donn√©es ?

![](hdfs_browse.png)

## Transfert de la bulle √† HDFS {.smaller}

![](hdfs_dowload.PNG)

## Transfert de HDFS √† la bulle {.smaller}

![](hdfs_midas.PNG)

## Mais o√π sont r√©ellement les donn√©es ? HDFS {.smaller}

Hadoop Distributed File System (HDFS)

-   **stockage sur diff√©rentes machines :** ici les noeuds du cluster spark, c'est-√†-dire les diff√©rents ordinateurs workers du cluster

-   donn√©es divis√©es **en blocs** plus petits de taille fixe et r√©partis sur les machines : aucune table de MiDAS n'existe en entier sur le cluster

-   chaque bloc est **r√©pliqu√© trois fois** : il existe trois fois les 10 premi√®res lignes de la table FNA sur trois ordinateurs diff√©rents du cluster (r√©silience)

-   un **NameNode** supervise les **m√©tadonn√©es** et g√®re la structure du syst√®me de fichiers

-   les **DataNodes** stockent effectivement les blocs de donn√©es : les datanodes sont en fait les disques des workers du cluster, chaque ordinateur du cluster dispose d'un disque avec une partie des donn√©es MiDAS

-   le **syst√®me HDFS** est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local

# Programmer avec sparklyr

## Param√©trer sa session {.smaller .scrollable}

Il faut pr√©ciser quelles ressources r√©server pour chaque unit√© spark : le driver, le nombre d'ordinateurs workers (appel√©es instances), la RAM, le nombre de coeurs

La configuration par d√©faut est :

Il est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :

-   Les ressources physiques utilis√©es :

    1.  par le **driver** : avec `spark.driver.memory` **(avec parcimonie)**

    2.  par chaque **worker** avec `spark.executor.memory`(valeur max **140 Go**) et `spark.executor.cores` (valeur max **8 coeurs**)

    3.  le nombre de **workers** avec `spark.executor.instances` **(2 ou 3 suffisent)**

    4.  La **file** sur laquelle on travaille avec `spark.yarn.queue` **(prod ou dev)**

-   le nombre de **partitions** de chaque `spark_data_frame` avec `spark.sql.shuffle.partitions` **(200 par d√©faut)**

-   la **limite de taille des r√©sulats** qui peuvent √™tre **collect√©s** par le driver avec `spark.driver.maxResultSize` **(0 est la meilleure option)**

```{r}
#| eval: false
#| echo: true

conf <- spark_config()
conf["spark.driver.memory"] <- "40Go"
conf["spark.executor.memory"] <- "60Go"
conf["spark.executor.cores"] <- 4
conf["spark.executor.instances"] <- 2
cont["spark.yarn.queue"] <- "prod"
conf["spark.driver.maxResultSize"] <- 0
conf["spark.sql.shuffle.partitions"] <- 200

sc <- spark_connect(master = "yarn", config = conf)
```

## Mode cluster : non concurrence gr√¢ce au cluster manager {.smaller}

![](mode_cluster.PNG)

Le mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d'un serveur. Ces machines communiquent en r√©seau.

Capture d'√©cran r√©servation des ressources

Il est donc n√©cessaire de se d√©connecter pour lib√©rer les ressources : des ressources r√©serv√©es, m√™me lorsqu'aucun programme ne tourne, ne peuvent jamais √™tre affect√©es √† d'autres utilisateurs.

```{r}
#| eval: false

spark_disconnect_all()
```

## Importer les donn√©es depuis HDFS sous R {.smaller .scrollable}

Les donn√©es doivent √™tre disponibles dans les workers sous forme de `spark_data_frame` :

-   **cach√© en m√©moire directement** : si utilis√©es de tr√®s nombreuses fois pour gagner du temps

-   laiss√© sur disque tant qu'aucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire

    ‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.

-   Pour passer un `data.frame` R en spark_data_frame : `copy_to()`

```{r}
#| eval: false
#| echo: true

pjc_df_spark <- spark_read_parquet(sc,
                                  path = "hdfs:///dataset/MiDAS_v4/FNA/pjc.parquet",
                                  memory = TRUE)

pjc_filtree <- pjc_df_spark %>%
  filter(KDDPJ >= as.Date("2022-01-01"))

spark_write_parquet(pjc_filtree, "hdfs:///tmp/pjc_filtree.parquet")

pjc_df_spark <- copy_to(sc, "PJC")
```

## Les exports sur HDFS {.smaller .scrollable}

::: callout-caution
## Les exports sur HDFS

Lorsqu'on exporte une table depuis notre session R vers HDFS, celle-ci est **automatiquement partitionn√©e**, comme le reste des donn√©es.

Ainsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.

Il est possible de ma√Ætriser le nombre de partitions avec la commande `sdf_coalesce(partitions = 5)` du package `sparklyr`.

L'id√©al est d'**adapter le nombre de partitions √† la taille d'un bloc** : un bloc mesure 128 MB. Lorsqu'un bloc disque est utilis√©, m√™me √† 1%, il n'est pas utilisable pour un autre stockage.

Exporter un fichier de 1MB en 200 partitions r√©serve 200 blocs inutilement.
:::

## Les shuffles {.smaller}

![](reduce_distribue.drawio.png)

Comme nous l'avons vu, les traitements REDUCE ne se pr√™tent pas tr√®s bien au calcul distribu√© :

-   augmenter le nombre de workers augmente la probabilit√© de devoir effectuer des shuffles

-   il est recommand√© de se limiter √† deux workers comme dans la configuration propos√©e

-   r√©server d'autres ressources n'est souvent pas efficient et monopolise les ressources pour les autres utilisateurs.

## R√©cup√©rer un r√©sultat {.smaller}

Les r√©sultats qu'il est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :

-   **une table filtr√©e** avec les variables n√©cessaires √† l'√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;

-   des **statistiques descriptives synth√©tiques ;**

-   les **premi√®res lignes** de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;

-   une **table agr√©g√©e** pour un graphique par exemple, √† l'aide de la fonction `summarise()`.

## L'utilisation de la m√©moire du driver {.smaller}

![](collect.drawio.png)

## L'utilisation de la m√©moire du driver {.smaller .scrollable}

Lorsqu'il est n√©cessaire de collecter une table volumineuse, il faut donc pr√©voir assez de m√©moire RAM pour le driver.

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "2"

conf <- spark_config()
conf["spark.driver.memory"] <- "40Go"
conf["spark.executor.memory"] <- "80Go"
conf["spark.executor.cores"] <- 5
conf["spark.executor.instances"] <- 2
cont["spark.yarn.queue"] <- "prod"
conf["spark.driver.maxResultSize"] <- 0
conf["spark.sql.shuffle.partitions"] <- 200

sc <- spark_connect(master = "yarn", config = conf)
```

::: callout-caution
## Bonne pratique de partage des ressources

Le driver est instanci√© dans la bulle Midares, qui a vocation √† √™tre r√©duite suite √† la g√©n√©ralisation du cluster.

-   La bulle Midares a besoin de RAM minimale pour fonctionner, 100% des ressources ne sont donc pas disponibles pour `sparklyr`.

-   Pour permettre le **travail simultan√© fluide de 10 utilisateurs**, la m√©moire allou√©e au driver recommand√©e pour chaque utilisateur est de **15 Go**.

-   **L'export d'une table** `sdf` directement au format `.parquet` est une alternative plus rapide, plus efficiente et qui permet par la suite de charger ses donn√©es en R classique et de travailler sur un `df` R sans utiliser `sparklyr`.
:::

## Comment tester son code pour collecter le moins possible ? {.smaller .scrollable}

La programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu'√† la fin pour que Catalyst optimise l'ensemble du programme

La principale diff√©rence avec la programmation en R classique est que **la visualisation de tables compl√®tes volumineuses n'est pas recommand√©e** :

-   **goulets d'√©tranglement** m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;

-   **longue :** √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;

-   **beaucoup moins efficace que l'export direct en parquet** du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l'√©tude.

S'il est n√©cessaire de collecter, il faut pr√©voir **beaucoup de RAM pour le driver avec le param√®tre** `spark.driver.memory`**.**

# Sparklyr : la solution ergonomique de spark sous R

## Ce qui change pour l'utilisateur {.smaller}

La majorit√© des commandes `dplyr` fonctionnent sur un spark_data_frame avec le package `sparklyr`. Les divergences principales sont les suivantes :

| Fonctionnalit√©                 | tidyverse      | sparklyr                         |
|--------------------------|-----------------|-----------------------------|
| import d'un fichier `.parquet` | `read_parquet` | `spark_read_parquet()`           |
| tri d'un tableau               | `arrange()`    | `window_order()` ou `sdf_sort()` |
| op√©rations sur les dates       | `lubridate`    | fonctions Hive                   |
| empiler des tableaux           | `bind_rows()`  | `sdf_bind_rows()`                |
| nombre de lignes d'un tableau  | `nrow()`       | `sdf_nrow()`                     |
| faire pivoter un tableau       | `tidyr`        | `sdf_pivot()`                    |
| export d'un `spark_data_frame` |                | `spark_write_parquet()`          |

## Quelques fonctions sp√©cifiques {.smaller .scrollable}

::: panel-tabset
## Dates

Les fonctions de `lubridate()`ne sont pas adapt√©es au `spark_data_frames`.

-   Convertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date

    ```{r}
    #| eval: false
    #| echo: true

    date_1 <- as.Date("2024-05-26")

    ```

-   Calculer une dur√©e entre deux dates

    ```{r}
    #| eval: false
    #| echo: true

    PJC_spark <- spark_read_parquet(sc,
                                    path = "hdfs:///dataset/MiDAS_v4/pjc.parquet",
                                    memory = FALSE)

    duree_pjc_df <- PJC_spark %>%
      rename(date_fin_pjc = as.Date(KDFPJ),
             date_deb_pjc = as.Date(KDDPJ)) %>%
      mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %>%
      head(5)

    ```

-   Ajouter ou soustraire des jours ou des mois √† une date

    ```{r}
    #| eval: false
    #| echo: true


    duree_pjc_bis_df <- duree_pjc_df %>%
      mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),
             duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),
             duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %>%
      head(5)

    ```

::: callout-note
## Add_months

Si la date en entr√©e est le dernier jour d'un mois, la date retourn√©e avec `add_months(date_entree, int(1))` sera le dernier jour calendaire du mois suivant.
:::

::: callout-tip
## Format

Le `int()` est important car ces fonctions Hive n'accepte que les entiers pour l'ajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.
:::

## Tableau

-   Tri dans un groupe pour effectuer un calcul s√©quentiel

    ```{r}
    #| eval: false
    #| echo: true

    ODD_spark <- spark_read_parquet(sc,
                                    path = "hdfs:///dataset/MiDAS_v4/odd.parquet",
                                    memory = FALSE)

    ODD_premier <- ODD_spark %>%
      group_by(id_midas) %>%
      window_order(id_midas, KDPOD) %>%
      mutate(date_premier_droit = first(KDPOD)) %>%
      ungroup() %>%
      distinct(id_midas, KROD3, date_premier_droit) %>%
      head(5)
      
    ```

-   Tri pour une sortie : `sdf_sort()` , `arrange()` ne fonctionne pas

-   Concat√©ner les lignes (ou les colonnes `sdf_bind_cols()`)

    ```{r}
    #| eval: false
    #| echo: true

    ODD_1 <- ODD_spark %>%
      filter(KDPOD <= as.Date("2017-12-31")) %>%
      mutate(groupe = "temoins")

    ODD_2 <- ODD_spark %>%
      filter(KDPOD >= as.Date("2021-12-31")) %>%
      mutate(groupe = "traites")

    ODD_evaluation <- sdf_bind_rows(ODD_1, ODD_2)

    ```

-   D√©doublonner une table

    ```{r}
    #| eval: false
    #| echo: true

    droits_dans_PJC <- PJC_spark %>%
      sdf_distinct(id_midas, KROD3)

    print(head(droits_dans_PJC, 5))

    PJC_dedoublonnee <- PJC_spark %>%
      sdf_drop_duplicates()

    print(head(PJC_dedoublonnee, 5))

    ```

-   Pivot : les fonctions du packag `tidyr` ne fonctionnent pas sur donn√©es spark

    ```{r}
    #| eval: false
    #| echo: true

    ODD_sjr_moyen <- ODD_spark %>%
      mutate(groupe = ifelse(KDPOD <= as.Date("2020-12-31"), "controles", "traites")) %>%
      sdf_pivot(groupe ~ KCRGC,
        fun.aggregate = list(KQCSJP = "mean")
      )
    ```

## Statistiques

-   R√©sum√© statistique : `sdf_describe()` , `summary()`ne fonctionne pas.

-   Dimension : `sdf_dim`, la fonction `nrow()`ne fonctionne pas.

-   Quantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, `sdf_quantile()`

-   Echantillonnage al√©atoire : `sdf_random_split`
:::

## Quelques tips d'optimisation {.smaller .scrollable}

::: panel-tabset
### Jointures

Pour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse

![](join.png)

Solution rapide :

```{r}
#| eval: false
#| echo: true

table_finale <- table_volumineuse_comme_PJC %>%
  right_join(petite_table_mon_champ)
```

Solution lente :

```{r}
#| eval: false
#| echo: true

table_finale <- petite_table_mon_champ %>%
  left_join(table_volumineuse_comme_PJC)
```

### Persist

Lorsqu'une table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c'est-√†-dire enregistrer ce `spark_data_frame`sur le disque ou dans la m√©moire des noeuds.

```{r}
#| eval: false
#| echo: true

table_1 <- mon_champ %>%
  left_join(ODD, by = c("id_midas", "KROD3")) %>%
  rename(duree_potentielle_indemnisation = KPJDXP,
         SJR = KQCSJP,
         date_debut_indemnisation = KDPOD) %>%
  sdf_persist()

duree <- table_1 %>%
  summarise(duree_moy = mean(duree_potentielle_indemnisation),
            duree_med = median(duree_potentielle_indemnisation)) %>%
  collect()

SJR <- table_1 %>%
  summarise(SJR_moy = mean(SJR),
            SJR_med = median(SJR)) %>%
  collect()


```

### Chargement

Lorsqu'on charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.

Attention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l'ex√©cution.

### Export et partitions

Le format `.parquet` (avec `arrow`) et le framework `spark` permettent de g√©rer le partitionnement des donn√©es.

Si les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d'une m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul

```{r}
#| eval: false
#| echo: true

spark_write_parquet(DE, partition_by = c("REGIND"))

```

::: callout-warning
## Exports simultan√©s

HDFS supporte les exports simultan√©s, mais le temp d'export est plus long lorsque le NameNode est requ√™t√© par plusieurs personnes simultan√©ment : d'apr√®s les tests cluster

-   pour un petit export (5 minutes), le temps peut √™tre multipli√© par 4 ;

-   pour un gros export (15 minutes), le temps peut √™tre multipli√© par 2.
:::
:::

## Forcer le calcul {.smaller}

Quelques actions :

-   collecter la table enti√®re üõë

    ```{r}
    #| eval: false
    #| echo: true

    spark_data_frame_1 %>%
      collect()
    ```

-   afficher les premi√®res lignes

    ```{r}
    #| eval: false
    #| echo: true

    spark_data_frame_1 %>%
      head(10)
    ```

-   Mettre les donner en cache

    ```{r}
    #| eval: false
    #| echo: true

    spark_data_frame_1 %>%
      sdf_register() %>%
      tbl_cache()

    sc %>% spark_session() %>% invoke("catalog") %>% 
      invoke("clearCache")
    ```

## Les erreurs en sparklyr {.smaller}

`sparklyr` traduit le code `dplyr` fourni en `scala`, mais interpr√®te √©galement les messages d'erreurs envoy√©s du cluster vers la session R.

`sparklyr` n'est cependant pas performant pour interpr√©ter ces erreurs.

N'h√©sitez pas √† enregistrer le code g√©n√©rant un message d'erreur dans Documents publics/erreurs_sparklyr

Un test du code pas-√†-pas permet d'isoler le probl√®me.

## Bonnes pratiques {.smaller}

-   D√©connexion ou fermeture R pour lib√©rer les ressources üõë

-   Ne plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è

-   Pyspark ou Sparklyr pour la production ‚ùì

-   Utilisation parcimonieuse des ressources ‚öñÔ∏è

-   Envoi des erreurs sparklyr üì©

# Pour aller plus loin

## L'architecture Map Reduce

![](map_reduce.png)

## La gestion de la m√©moire avec spark {.smaller .scrollable}

Les **shuffles** sont les op√©rations les plus gourmandes en temps.

::: callout-note
## Qu'est-ce qu'un shuffle ?

Un shuffle est un **√©change de donn√©es entre diff√©rents noeuds** du cluster.

Nous avons vu qu'utiliser `spark` dans un cluster implique de distribuer √©galement le stockage des donn√©es.

Par exemple :

1.  je demande un traitement sur la table PJC du FNA

2.  si un noeud contenant d√©j√† les donn√©es de PJC est disponible, le cluster manager envoie le traitement √† ce noeud

3.  si tous les noeuds contenant les donn√©es de PJC sont d√©j√† r√©serv√©s, alors le cluster manager demande le traitement √† un autre noeud, par exemple le noeud 1

4.  il demande √† un noeud contenant les donn√©es PJC, par exemple le noeud 4, d'envoyer ces donn√©es au noeud 1 qui va ex√©cuter le traitement

5.  cet √©change de donn√©es est en r√©seau filaire : un √©change filaire est beaucoup plus lent qu'un envoi interne par le disque du noeud 1 √† la RAM du noeud 1

6.  c'est pourquoi pour optimiser un programme spark, il est possible de limiter les shuffles
:::

## L'utilisation de la m√©moire dans un worker {.smaller .scrollable}

::: columns
::: {.column width="50%"}
![](memoire_worker_1.drawio.png)
:::

::: {.column width="50%"}
![](memoire_worker_2.drawio.png)
:::
:::

::: callout-tip
Ne pas charger plusieurs fois les m√™mes donn√©es en cache, ou si besoin augmenter la part de la m√©moire allou√©e au stockage avec `spark.memory.storageFraction`.
:::

::: columns
::: {.column width="50%"}
```{r}
#| code-line-numbers: "4"
#| eval: false
#| echo: true

conf <- spark_config()
conf["spark.driver.memory"] <- "40Go"
conf["spark.executor.memory"] <- "80Go"
conf["spark.memory.fraction"] <- 0.8
conf["spark.executor.cores"] <- 5
conf["spark.executor.instances"] <- 2
cont["spark.yarn.queue"] <- "prod"
conf["spark.driver.maxResultSize"] <- 0
conf["spark.sql.shuffle.partitions"] <- 200

sc <- spark_connect(master = "yarn", config = conf)
```
:::

::: {.column width="50%"}
```{r}
#| code-line-numbers: "5"
#| eval: false
#| echo: true

conf <- spark_config()
conf["spark.driver.memory"] <- "40Go"
conf["spark.executor.memory"] <- "80Go"
conf["spark.memory.fraction"] <- 0.8
conf["spark.memory.storageFraction"] <- 0.4
conf["spark.executor.cores"] <- 5
conf["spark.executor.instances"] <- 2
cont["spark.yarn.queue"] <- "prod"
conf["spark.driver.maxResultSize"] <- 0
conf["spark.sql.shuffle.partitions"] <- 200

sc <- spark_connect(master = "yarn", config = conf)
```
:::
:::

## SparkUI : un outil d'optimisation {.smaller .scrollable}

Spark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d'optimiser les traitements :

::: panel-tabset
## DAG

![](dag.webp)

## GC

V√©rifier que le `gc time` est inf√©rieur √† 10% du temps pour ex√©cuter la t√¢che ‚úÖ

![](gc.png)

## M√©moire

V√©rifier que la `storage memory` ne sature pas la m√©moire ‚úÖ

![](gc.png)
:::

## Utiliser les interfaces {.smaller}

-   **yarn** : disponibilit√© des ressources

    ![](yarn_scheduler.PNG)

-   **Sparkhistory** pour des traitements de sessions ferm√©es

Le sparkhistory entra√Æne l'enregistrement de logs assez lourdes, il est donc d√©sactiv√© par d√©faut. Pour l'activer sur un programme :

```{r}
#| eval: false
#| echo: true

conf <- spark_config()
conf["spark.eventLog.enabled"] <- "true"
conf["spark.eventLog.dir"] <- "hdfs://midares-deb11-nn-01.midares.local:9000/spark-logs"
conf["appName"] <- "un_nom_de_traitement"

sc <- spark_connect(master = "yarn", config = conf)


```

## Ma session ne s'instancie jamais {.smaller}

Si l'instruction `sc <- spark_connect(master = "yarn", config = conf` prend plus de 10 minutes, il est utile d'ouvrir l'interface de yarn pour v√©rifier que la file n'est pas d√©j√† enti√®rement occup√©e. L'erreur peut ne survenir qu'au bout d'une vingtaine de minutes : le job est `ACCEPTED` dans yarn, ou `FAILED` si la session n'a pas pu √™tre instanci√©e par manque de ressources disponibles.

![](yarn_accepted.jpg)

## Exporter de HDFS au local {.smaller}

::: r-stack
![](hdfs_browse.png){.fragment width="1000" height="700"}

![](hdfs_download.png){.fragment width="1000" height="700"}
:::

## Pyspark : mode cluster

![](pyspark.drawio.png)

## Les avantages de pyspark {.smaller}

-   Mode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è

-   Spark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è

-   Plusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª

-   Stabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ

-   Lisibilit√© du code üëì

-   Temps de connexion et d'ex√©cution r√©duit ‚è≤Ô∏è

-   Utilisation optimale de SparkUI üìä

## Merci pour votre attention !
