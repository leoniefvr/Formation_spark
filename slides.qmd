---
title: "Initiation √† Spark avec R en mode cluster"
format: 
  revealjs:
    incremental: true
    slide-numbers : true
---

## Au programme {.smaller}

1.  MiDAS : une base de donn√©es volumineuse üíæ

2.  Manipuler un appariement : une op√©ration co√ªteuse üí≤

3.  Initiation au calcul distribu√© üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è

4.  Sparklyr : la solution ergonomique de sparkl sous R üë®‚Äçüíª

5.  Pour aller plus loin ‚è©

## MiDAS : une base de donn√©es volumineuse {.smaller}

MiDAS croise trois bases de donn√©es administratives exhaustives :

-   les donn√©es sur **l'inscription et l'indemnisation des demandeurs d'emploi** de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;

-   les donn√©es sur les b√©n√©ficiaires de **minima sociaux** (RSA, PPA, AAH) et les caract√©ristiques des **m√©nages** de la CNAF : Allstat-FR6 ;

-   les donn√©es sur les **contrats salari√©s** de la DSN : MMO de la Dares.

## MiDAS : une base de donn√©es volumineuse {.smaller}

Chaque vague de MiDAS correspond √† environ **600 Go** de donn√©es au format sas. Les vagues fonctionnent par empilement :

-   le gain de **profondeur temporelle** et l'entr√©e dans le champ de nouvelles personnes

-   **vagues sont appariables entre elles**

## MiDAS : une base de donn√©es volumineuse {.smaller}

MiDAS est l'une des bases de donn√©es les plus volumineuses du SSP :![Quelques bases du SSP](donnees_ssp.PNG)

## Structure de l'appariement {.smaller}

![](structure_midas.PNG){fig-align="center"}

::: callout-tip
## Pourquoi Spark ?

La manipulation des donn√©es MiDAS en l'√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.
:::

## Le format parquet {.smaller .scrollable}

Les donn√©es sont converties au **format parquet** d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l'espace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :

-   il **compresse** efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv

-   il est orient√© **colonnes**

-   il permet le chargement efficace **en m√©moire** des donn√©es

-   Il permet le **stockage partitionn√©** des donn√©es

-   il permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires **sur disque**

-   Il est **ind√©pendant du logiciel** utilis√© : il peut donc √™tre trait√© par spark et par R.

# Manipuler un appariement : une op√©ration co√ªteuse

## L'espace MiDares {.smaller .scrollable}

::: panel-tabset
### Ressources

Des ressources partag√©es entre tous les utilsateurs simultan√©s :

-   512 Go de m√©moire vive (ou RAM)

::: callout-note
## La m√©moire vive

La m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa volatilit√© (toutes les donn√©es sont perdues si l'ordinateur n'est plus aliment√©) et par l'acc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.
:::

-   Un processeur (ou CPU) compos√© de 32 coeurs

::: callout-note
## Le processeur

Le processeur permet d'ex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel... Il est compos√© d'un ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu'une seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu'il a de coeurs. Un processeur se caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d'op√©ration qu'il est capable d'effetuer par seconde.
:::

### Sch√©ma

![](schema_ordinateur.png)
:::

## Programmer en m√©moire vive {.smaller}

-   **R : la m√©moire vive, √©tat dans l'environnement**

-   SAS : lecture/√©criture sur le disque

-   MiDAS au format sas \>\> taille de la m√©moire vive disponible du serveur CASD --\> format parquet

-   **Impossible de charger tout MiDAS en m√©moire vive**

    Des solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :

-   arrow (avec des requ√™tes dplyr)

-   duckDB

    ‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements.

## Les traitements co√ªteux en puissance de calcul {.smaller}

-   les jointures

-   les op√©rations en group_by

-   distinct

    ‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)

    ‚ñ∂Ô∏è Erreur "out of memory".

## Un traitement peu co√ªteux {.smaller}

![](formation%20sparklyr-Page-1.drawio.png){fig-align="center" width="800"}

Ce traitement est peu co√ªteux :

-   chargement d'une seule colonne en RAM : format parquet orient√© colonnes

-   peu de m√©moire d'ex√©cution : R est un langage vectoris√©

## Un traitement co√ªteux {.smaller}

![](formation%20sparklyr-Page-2.drawio.png){fig-align="center" width="850"}

Ce traitement n√©cessite :

-   le chargement de davantage de colonnes en m√©moire vive ;

-   davantage de m√©moire d'ex√©cution pour effectuer l'intersection (inner_join).

# Initiation au calcul distribu√©

## Calcul distribu√© et calcul parall√®le {.smaller}

::: panel-tabset
### Calcul non distribu√©

Lorsqu'un traitement Big Data est demand√© par l'utilisateur dans la session R, plusieurs probl√®mes peuvent se poser :

-   la taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R

-   le temps de calcul : si plusieurs √©tapes sont n√©cessaires pour un traitement, elles sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)

-   l'optimisation du programme

### Calcul distribu√© avec spark

Le calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :

-   chargement des donn√©es en m√©moire parcimonieux et non syst√©matique

-   ex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents

-   optimisation automatique du code
:::

## Le cluster de calcul Midares : mode interactif {.smaller}

![](schema_cluster.drawio.png){fig-align="center" width="691"}

## Spark {.smaller}

-   Apache Spark : **librairie open source** d√©velopp√©e dans le langage Scala

-   **Scala** : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java

    Exemple Scala ?

-   Scala adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de Spark et optimiser au maximum les traitements en spark

-   Spark est **compatible avec les langages Scala, R, Python, Java**, et peut interpr√©ter des commandes **SQL.**

-   Deux packages existent sous R :

    -   **sparkR** propos√© par Apache Spark

    -   **sparklyr**, qui permet d'utiliser directement des commandes dplyr traduites en spark par le package.

## Mode local : concurrence {.smaller}

![](mode_local.PNG)

## Mode local : concurrence {.smaller}

En mode local :

-   une unique machine Java

-   parall√©lisation des t√¢ches sur diff√©rents coeurs de cette machine virtuelle

-   pas de stockage distribu√©, ca n'est pas du calcul distribu√© √† proprement parler

-   acc√©l√©ration par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources

-   Sur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local

    ‚ñ∂Ô∏èmode local √† √©viter absolument

## Mode cluster : non concurrence {.smaller}

![](mode_cluster.PNG)

Le mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d'un serveur. Ces machines communiquent en r√©seau.

## Installation de spark sous CASD

Voir la fiche d√©dier sur le site

## Le stockage distribu√© : HDFS {.smaller}

-   **stockage sur diff√©rentes machines :** ici les noeuds du cluster spark, c'est-√†-dire les diff√©rents ordinateurs workers du cluster

-   donn√©es divis√©es **en blocs** plus petits de taille fixe et r√©partis sur les machines

-   chaque bloc est **r√©pliqu√© trois fois** pour √™tre r√©silient face aux pannes

-   un **NameNode** supervise les **m√©tadonn√©es** et g√®re la structure du syst√®me de fichiers

-   les **DataNodes** stockent effectivement les blocs de donn√©es

-   le **syst√®me HDFS** est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local

    ::: callout-caution
    ## Les exports sur HDFS

    Lorsqu'on exporte une table depuis notre session R vers HDFS, celle-ci est **automatiquement partitionn√©e**, comme le reste des donn√©es.

    Ainsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.

    Il est possible de ma√Ætriser le nombre de partitions avec la commande **sdf_coalesce**(partitions = 5) du package sparklyr.
    :::

## Le stockage distribu√© : HDFS {.smaller .scrollable}

![](stockage_distribue.drawio.png){fig-align="center" width="2000"}

‚ñ∂Ô∏è Les r√©plications de donn√©es ont deux fonctions :

-   augementer la **flexibilit√© de la distribution** des traitements

-   augmenter la **r√©silience** en cas de panne d'un noeud

## La lazy evaluation {.smaller .scrollable}

Spark distingue deux types d'op√©rations :

-   **les transformations :** ce sont des op√©rations qui prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul lorsqu'elles sont appel√©es.

    Par exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d'ex√©cution :

    ```{r}
    #| eval: false
    #| echo: true

    une_transformation <- un_spark_data_frame %>%
      group_by(identifiant) %>%
      mutate(une_somme = sum(revenus))
    ```

-   **les actions :** ce sont des op√©rations qui demandent le calcul d'un r√©sultat et qui d√©clenchent le calcul et l'ex√©cution de toutes les transformations compil√©es jusqu'√† l'appel de l'action.

    Par exemple, le programme ci-dessous d√©clenche le calcul de la cellule \`une_transformation\` et de la moyenne des revenus :

    ```{r}
    #| eval: false
    #| echo: true
    revenu_moyen <- une_transformation %>%
      summarise(revenu_moyen = mean(une_somme)) %>%
      print()
    ```

    Les principales actions sont : print(), collect(), head(), tbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser).

## La lazy evaluation {.smaller .scrollable}

Spark optimise automatiquement les programmes soumis :

1.  Compilation des transformations

2.  Int√©gration dans un plan d'ex√©cution : √©ventuelles erreurs du programme soulev√©es avant l'ex√©cution

3.  Optimisation du plan logique par le module Catalyst (driver Spark)

    Par exemple si j'√©cris le programme :

    ```{r}
    #| eval: false
    #| echo: true

    non_optimal <- table_1 %>%
      mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %>%
      filter(debut_contrat >= as.Date("2023-01-01"))
    ```

    Catalyst r√©√©crit :

    ```{r}
    #| eval: false
    #| echo: true

    non_optimal <- table_1 %>%
      filter(debut_contrat >= as.Date("2023-01-01")) %>%
      mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))
    ```

    Cette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu'une action d√©clenche l'ex√©cution.

4.  D√©clencher le moins d'actions possibles dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.

5.  Pour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique.

## Le plan d'ex√©cution {.smaller}

![](catalyst.jpg)

source : documentation CASD disponible √† [Documentation Data Science](https://casd-eu.gitbook.io/data-science/)

## R√©cup√©rer un r√©sultat {.smaller}

Les r√©sultats qu'il est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :

-   **une table filtr√©e** avec les variables n√©cessaires √† l'√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;

-   des **statistiques descriptives synth√©tiques ;**

-   les **premi√®res lignes** de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;

-   une **table agr√©g√©e** pour un graphique par exemple, √† l'aide de la fonction summarise().

# Sparklyr : la solution ergonomique de spark sous R

## Sparklyr et SparkR {.smaller}

Deux packages permettent de programmer avec Spark sous R :

-   **SparkR :** ce package, maintenu par Apache Spark, permet d'utiliser une syntaxe proche de spark, scala, ou directement du code SQL pour manipuler des donn√©es dans une session R.

-   **Sparklyr :** ce package permet d'utiliser directement la syntaxe dplyr dans une session Spark sous R.

**Sparklyr** fonctionne selon ces √©tapes :

1.  La **JVM driver spark est instanci√©e dans la bulle Midares** pour utiliser sparklyr.

2.  Les instructions dplyr appel√©es sur un spark_data_frame sont **traduites par les fonctions du package sparklyr en scala**, puis envoy√©es au driver.

3.  Le programme en scala est ex√©cut√© sur le **cluster**.

4.  Si une **erreur** est renvoy√©e par le driver, elle est interpr√©t√©e par R avant d'√™tre affich√©e en session R.

## Configuration cluster {.smaller .scrollable}

Deux √©tapes majeures dans le traitement de donn√©es sous R diff√®re en sparklyr par rapport √† une programmation classique en dplyr :

::: panel-tabset
### Configuration

Il est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :

-   Les ressources physiques utilis√©es :

    1.  par le driver : avec **spark.driver.memory**

    2.  par chaque worker avec **spark.executor.memory** et **spark.executor.cores**

    3.  le nombre de worker avec **spark.executor.instances**

    4.  La file sur laquelle on travaille avec **spark.yarn.queue**

-   le nombre de partitions de chaque spark_data_frame avec **spark.sql.shuffle.partitions**

-   la limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec **spark.driver.maxResultSize**

```{r}
#| eval: false
#| echo: true

conf <- spark_config()
conf["spark.driver.memory"] <- "40Go"
conf["spark.executor.memory"] <- "80Go"
conf["spark.executor.cores"] <- 5
conf["spark.executor.instances"] <- 2
cont["spark.yarn.queue"] <- "prod"
conf["spark.driver.maxResultSize"] <- 0
conf["spark.sql.shuffle.partitions"] <- 200

sc <- spark_connect(master = "yarn", config = conf)
```

### Import-export

Les donn√©es doivent √™tre disponibles dans les workers sous forme de **spark_data_frame** :

-   cach√© en m√©moire directement : si utilis√©es plusieurs fois pour gagner du temps

-   laiss√© sur disque tant qu'aucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire

    ‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.

-   Pour passer un data.frame R en spark_data_frame : **copy_to()**

```{r}
#| eval: false

pjc_df_spark <- spark_read_parquet(sc,
                                  path = "hdfs:///dataset/MiDAS_v4/FNA/pjc.parquet",
                                  memory = TRUE)

pjc_filtree <- pjc_df_spark %>%
  filter(KDDPJ >= as.Date("2022-01-01"))

spark_write_parquet(pjc_filtree, "hdfs:///tmp/pjc_filtree.parquet")

pjc_df_spark <- copy_to(sc, "PJC")
```
:::

## L'utilisation de la m√©moire dans un worker {.smaller}

::: columns
::: {.column width="50%"}
![](memoire_worker_1.drawio.png)
:::

::: {.column width="50%"}
![](memoire_worker_2.drawio.png)
:::
:::

::: callout-tip
Ne pas charger plusieurs fois les m√™mes donn√©es en cache
:::

## L'utilisation de la m√©moire du driver {.smaller}

![](collect.drawio.png)

## Ce qui change pour l'utilisateur {.smaller}

La majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences sont les suivantes :

-   pour effectuer des op√©rations avec les dates, il faut utiliser les fonctions Hive sp√©cifiques.

-   arrange() ne fonctionne pas sur un spark_data_frame, il faut lui substituer window_order.

-   des fonctions sp√©cifiques aux spark data frames : sdf_bind_rows pour empiler les lignes par exemple.

## Quelques fonctions sp√©cifiques {.smaller .scrollable}

::: panel-tabset
## Dates

Les fonctions de `lubridate()`ne sont pas adapt√©es au spark_data_frames.

-   Convertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date

    ```{r}
    #| eval: false
    #| echo: true

    date_1 <- as.Date("2024-05-26")

    ```

-   Calculer une dur√©e entre deux dates

    ```{r}
    #| eval: false
    #| echo: true

    PJC_spark <- spark_read_parquet(sc,
                                    path = "hdfs:///dataset/MiDAS_v4/pjc.parquet",
                                    memory = FALSE)

    duree_pjc_df <- PJC_spark %>%
      rename(date_fin_pjc = as.Date(KDFPJ),
             date_deb_pjc = as.Date(KDDPJ)) %>%
      mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %>%
      head(5)

    ```

-   Ajouter ou soustraire des jours ou des mois √† une date

    ```{r}
    #| eval: false
    #| echo: true


    duree_pjc_bis_df <- duree_pjc_df %>%
      mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),
             duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),
             duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %>%
      head(5)

    ```

::: callout-note
## Add_months

Si la date en entr√©e est le dernier jour d'un mois, la date retourn√©e avec `add_months(date_entree, int(1))` sera le dernier jour calendaire du mois suivant.
:::

::: callout-tip
## Format

Le int() est important car ces fonctions Hive n'accepte que les entiers pour l'ajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.
:::

## Tableau

-   Tri dans un groupe pour effectuer un calcul s√©quentiel

    ```{r}
    #| eval: false
    #| echo: true

    ODD_spark <- spark_read_parquet(sc,
                                    path = "hdfs:///dataset/MiDAS_v4/odd.parquet",
                                    memory = FALSE)

    ODD_premier <- ODD_spark %>%
      group_by(id_midas) %>%
      window_order(id_midas, KDPOD) %>%
      mutate(date_premier_droit = first(KDPOD)) %>%
      ungroup() %>%
      distinct(id_midas, KROD3, date_premier_droit) %>%
      head(5)
      
    ```

-   Tri pour une sortie : `sdf_sort()` , `arrange()` ne fonctionne pas

-   Concat√©ner les lignes (ou les colonnes `sdf_bind_cols()`)

    ```{r}
    #| eval: false
    #| echo: true

    ODD_1 <- ODD_spark %>%
      filter(KDPOD <= as.Date("2017-12-31")) %>%
      mutate(groupe = "temoins")

    ODD_2 <- ODD_spark %>%
      filter(KDPOD >= as.Date("2021-12-31")) %>%
      mutate(groupe = "traites")

    ODD_evaluation <- sdf_bind_rows(ODD_1, ODD_2)

    ```

-   D√©doublonner une table

    ```{r}
    #| eval: false
    #| echo: true

    droits_dans_PJC <- PJC_spark %>%
      sdf_distinct(id_midas, KROD3)

    print(head(droits_dans_PJC, 5))

    PJC_dedoublonnee <- PJC_spark %>%
      sdf_drop_duplicates()

    print(head(PJC_dedoublonnee, 5))

    ```

-   Pivot : les fonctions du packag `tidyr` ne fonctionnent pas sur donn√©es spark

    ```{r}
    #| eval: false
    #| echo: true

    ODD_sjr_moyen <- ODD_spark %>%
      mutate(groupe = ifelse(KDPOD <= as.Date("2020-12-31"), "controles", "traites")) %>%
      sdf_pivot(groupe ~ KCRGC,
        fun.aggregate = list(KQCSJP = "mean")
      )
    ```

## Statistiques

-   R√©sum√© statistique : `sdf_describe()` , `summary()`ne fonctionne pas.

-   Dimension : `sdf_dim`, la fonction `nrow()`ne fonctionne pas.

-   Quantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, `sdf_quantile()`

-   Echantillonnage al√©atoire : `sdf_random_split`
:::

## Une r√®gle d'or : tester son code pour collecter le moins possible {.smaller .scrollable}

La programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu'√† la fin pour que Catalyst optimise l'ensemble du programme

La principale diff√©rence avec la programmation en R classique est que **la visualisation de tables compl√®tes volumineuses n'est pas recommand√©e** :

-   **goulets d'√©tranglement** m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;

-   **longue :** √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;

-   **beaucoup moins efficace que l'export direct en parquet** du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l'√©tude.

S'il est n√©cessaire de collecter, il faut pr√©voir **beaucoup de RAM pour le driver avec le param√®tre "spark.driver.memory".**

## Quelques tips d'optimisation {.smaller .scrollable}

::: panel-tabset
## Jointures

Pour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse

![](join.png)

Solution rapide :

```{r}
#| eval: false
#| echo: true

table_finale <- table_volumineuse_comme_PJC %>%
  right_join(petite_table_mon_champ)
```

Solution lente :

```{r}
#| eval: false
#| echo: true

table_finale <- petite_table_mon_champ %>%
  left_join(table_volumineuse_comme_PJC)
```

## Persist

Lorsqu'une table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c'est-√†-dire enregistrer ce spark_data_frame sur le disque ou dans la m√©moire des noeuds.

```{r}
#| eval: false
#| echo: true

table_1 <- mon_champ %>%
  left_join(ODD, by = c("id_midas", "KROD3")) %>%
  rename(duree_potentielle_indemnisation = KPJDXP,
         SJR = KQCSJP,
         date_debut_indemnisation = KDPOD) %>%
  sdf_persist()

duree <- table_1 %>%
  summarise(duree_moy = mean(duree_potentielle_indemnisation),
            duree_med = median(duree_potentielle_indemnisation)) %>%
  collect()

SJR <- table_1 %>%
  summarise(SJR_moy = mean(SJR),
            SJR_med = median(SJR)) %>%
  collect()


```

## Chargement

Lorsqu'on charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.

Attention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l'ex√©cution.

## Export et partitions

Le format parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.

Si les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d'une m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul

```{r}
#| eval: false
#| echo: true

spark_write_parquet(DE, partition_by = c("REGIND"))
sdf_coalesce
```
:::

## Forcer le calcul {.smaller}

Quelques actions :

-   collecter la table enti√®re üõë

    ```{r}
    #| eval: false
    #| echo: true

    spark_data_frame_1 %>%
      collect()
    ```

-   afficher les premi√®res lignes

    ```{r}
    #| eval: false
    #| echo: true

    spark_data_frame_1 %>%
      head(10)
    ```

-   Mettre les donner en cache

    ```{r}
    #| eval: false
    #| echo: true

    spark_data_frame_1 %>%
      sdf_register() %>%
      tbl_cache()

    sc %>% spark_session() %>% invoke("catalog") %>% 
      invoke("clearCache")
    ```

## Les erreurs en sparklyr {.smaller}

Sparklyr traduit le code dplyr fourni en scala, mais interpr√®te √©galement les messages d'erreurs envoy√©s du cluster vers la session R.

Sparklyr n'est cependant pas performant pour interpr√©ter ces erreurs.

N'h√©sitez pas √† enregistrer le code g√©n√©rant un message d'erreur dans Documents publics/erreurs_sparklyr

Un test du code pas-√†-pas permet d'isoler le probl√®me.

## Bonnes pratiques {.smaller}

-   D√©connexion ou fermeture R pour lib√©rer les ressources üõë

-   Ne plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è

-   Pyspark ou Sparklyr pour la production ‚ùì

-   Utilisation parcimonieuse des ressources ‚öñÔ∏è

-   Envoi des erreurs sparklyr üì©

# Pour aller plus loin

## L'architecture Map Reduce

![](map_reduce.png)

## La gestion de la m√©moire avec spark {.smaller .scrollable}

Les shuffles sont les op√©rations les plus gourmandes en temps.

Spark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d'optimiser les traitements :

::: panel-tabset
## DAG

![](dag.webp)

## GC

![](gc.png)

## M√©moire

![](gc.png)
:::

## Utiliser les interfaces {.smaller}

-   **yarn** : disponibilit√© des ressources

    ![](yarn_scheduler.png){width="600"}

-   **Sparkhistory** pour des traitements de sessions ferm√©es

## Exporter de HDFS au local {.smaller}

![](hdfs_browse.png)

## Pyspark : mode cluster

![](pyspark.drawio.png)

## Les avantages de pyspark {.smaller}

-   Mode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è

-   Spark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è

-   Plusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª

-   Stabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ

-   Lisibilit√© du code üëì

-   Temps de connexion et d'ex√©cution r√©duit ‚è≤Ô∏è

-   Utilisation optimale de SparkUI üìä

## Merci pour votre attention !
