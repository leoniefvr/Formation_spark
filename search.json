[
  {
    "objectID": "dates.html",
    "href": "dates.html",
    "title": "Les dates avec sparklyr",
    "section": "",
    "text": "Les fonctions de lubridate()ne sont pas adapt√©es aux spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date : m√™me fonction qu‚Äôen R base\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates : fonction datediff()\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date : date_add(), date_sub() et add_months().\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nTipFormat\n\n\n\nLe int() est important car ces fonctions Hive n‚Äôacceptent que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\n\n\nNoteAdd months\n\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\nSi tu as besoin d‚Äôautres fonctions de dates, tu peux consulter le site fonctions dates spark hive",
    "crumbs": [
      "Les dates avec sparklyr"
    ]
  },
  {
    "objectID": "dates.html#dates",
    "href": "dates.html#dates",
    "title": "Les dates avec sparklyr",
    "section": "",
    "text": "Les fonctions de lubridate()ne sont pas adapt√©es aux spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date : m√™me fonction qu‚Äôen R base\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates : fonction datediff()\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date : date_add(), date_sub() et add_months().\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nTipFormat\n\n\n\nLe int() est important car ces fonctions Hive n‚Äôacceptent que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\n\n\nNoteAdd months\n\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\nSi tu as besoin d‚Äôautres fonctions de dates, tu peux consulter le site fonctions dates spark hive",
    "crumbs": [
      "Les dates avec sparklyr"
    ]
  },
  {
    "objectID": "oom.html",
    "href": "oom.html",
    "title": "Out of memory",
    "section": "",
    "text": "L‚Äôexport direct d‚Äôun spark data frame en parquet sur HDFS est une meilleure alternative : tu peux ensuite charger la table parquet en data frame R si tu souhaites la traiter en m√©moire vive.\nContrairement au collect(), quelle que soit la taille de la table, exporter sous HDFS ne renverra jamais d‚Äôerreur, tandis que le collect()ne fonctionne pas pour les tables volumineuses.\nCet export direct n‚Äôest pas fonctionnel en spark local : encore une raison de passer sur le cluster Spark !\nSi tu souhaites voir ta table, tu peux collecter les premi√®res lignes de celle-ci en combinant les fonctions head()et collect().\n\n\n\n\n\nSi tu as utilis√© des fonctions telles que compute() dans ton programme, elles peuvent √™tre √† l‚Äôorigine d‚Äôune erreur Out of memory : ces commandes ne sont pas bien adapt√©es √† Spark et les r√©sultats temporaires stock√©s en m√©moire vive ne sont pas visibles dans l‚Äôespace d√©di√© sur SparkUI onglet Storage, il semblerait que ces r√©sultats temporaires soient stock√©s ‚Äúau mauvais endroit‚Äù et occupent de l‚Äôespace sans que l‚Äôutilisateur y ait acc√®s.\nPrivil√©gier les fonctions tbl_cache() avec sdf_register()pour forcer l‚Äôex√©cution du programme.\n\n\n\n\n\nForcer l‚Äôex√©cution du programme le plus tard possible pour permettre √† Spark d‚Äôoptimiser tout le programme et d‚Äôutiiliser les ressources de la mani√®re la plus parcimonieuse possible.\nTester son programme sur une toute petite partie des tables, √©tape par √©tape, en for√ßant l‚Äôex√©cution (appel d‚Äôune action telle que print() ou collect() ) √† chaque √©tape pour la phase de d√©buggage, puis supprimer toutes les actions interm√©diaires non n√©cessaires du programme pour que Spark optimise tout le programme.\n\n\n\n\nPar souci de parcimonie et pour faciliter le travail de nos coll√®gues sur des ressources partag√©es, cette option doit rester l‚Äôultime recours pour √©viter l‚Äôerreur Out of memory ü§ù",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#collecter-le-moins-possible",
    "href": "oom.html#collecter-le-moins-possible",
    "title": "Out of memory",
    "section": "",
    "text": "L‚Äôexport direct d‚Äôun spark data frame en parquet sur HDFS est une meilleure alternative : tu peux ensuite charger la table parquet en data frame R si tu souhaites la traiter en m√©moire vive.\nContrairement au collect(), quelle que soit la taille de la table, exporter sous HDFS ne renverra jamais d‚Äôerreur, tandis que le collect()ne fonctionne pas pour les tables volumineuses.\nCet export direct n‚Äôest pas fonctionnel en spark local : encore une raison de passer sur le cluster Spark !\nSi tu souhaites voir ta table, tu peux collecter les premi√®res lignes de celle-ci en combinant les fonctions head()et collect().",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#eviter-les-fonctions-arrow",
    "href": "oom.html#eviter-les-fonctions-arrow",
    "title": "Out of memory",
    "section": "",
    "text": "Si tu as utilis√© des fonctions telles que compute() dans ton programme, elles peuvent √™tre √† l‚Äôorigine d‚Äôune erreur Out of memory : ces commandes ne sont pas bien adapt√©es √† Spark et les r√©sultats temporaires stock√©s en m√©moire vive ne sont pas visibles dans l‚Äôespace d√©di√© sur SparkUI onglet Storage, il semblerait que ces r√©sultats temporaires soient stock√©s ‚Äúau mauvais endroit‚Äù et occupent de l‚Äôespace sans que l‚Äôutilisateur y ait acc√®s.\nPrivil√©gier les fonctions tbl_cache() avec sdf_register()pour forcer l‚Äôex√©cution du programme.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#laisser-spark-travailler-pour-nous",
    "href": "oom.html#laisser-spark-travailler-pour-nous",
    "title": "Out of memory",
    "section": "",
    "text": "Forcer l‚Äôex√©cution du programme le plus tard possible pour permettre √† Spark d‚Äôoptimiser tout le programme et d‚Äôutiiliser les ressources de la mani√®re la plus parcimonieuse possible.\nTester son programme sur une toute petite partie des tables, √©tape par √©tape, en for√ßant l‚Äôex√©cution (appel d‚Äôune action telle que print() ou collect() ) √† chaque √©tape pour la phase de d√©buggage, puis supprimer toutes les actions interm√©diaires non n√©cessaires du programme pour que Spark optimise tout le programme.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#si-tout-ceci-ne-fonctionne-pas-passer-√†-la-configuration-traitement-tr√®s-lourd",
    "href": "oom.html#si-tout-ceci-ne-fonctionne-pas-passer-√†-la-configuration-traitement-tr√®s-lourd",
    "title": "Out of memory",
    "section": "",
    "text": "Par souci de parcimonie et pour faciliter le travail de nos coll√®gues sur des ressources partag√©es, cette option doit rester l‚Äôultime recours pour √©viter l‚Äôerreur Out of memory ü§ù",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "Il est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark (appel√©e spark connection). Les param√®tres √† d√©finir sont :\n\nLes ressources physiques utilis√©es :\n\npar le driver : avec spark.driver.memory\npar chaque worker avec spark.executor.memory et spark.executor.cores\nle nombre de worker avec spark.executor.instances\nLa file sur laquelle on travaille avec spark.yarn.queue\n\nla limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec spark.driver.maxResultSize. 0 correspond √† l‚Äôabsence de limite.\n\n\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(dbplyr)\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"20Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 3\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\n\n\nImportantTemps de connexion\n\n\n\nPour se connecter au cluster, il faut environ 5 minutes, √† chaque connexion. Spark cluster n‚Äôest pas du tout adapt√© √† des traitements l√©gers (moins de 10 minutes).",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "configuration.html#traitement-normal-en-spark",
    "href": "configuration.html#traitement-normal-en-spark",
    "title": "Configuration",
    "section": "",
    "text": "conf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "configuration.html#traitement-tr√®s-lourd",
    "href": "configuration.html#traitement-tr√®s-lourd",
    "title": "Configuration",
    "section": "",
    "text": "library(sparklyr)\nlibrary(dplyr)\nlibrary(dbplyr)\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"20Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 3\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\n\n\nImportantTemps de connexion\n\n\n\nPour se connecter au cluster, il faut environ 5 minutes, √† chaque connexion. Spark cluster n‚Äôest pas du tout adapt√© √† des traitements l√©gers (moins de 10 minutes).",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "ressources.html",
    "href": "ressources.html",
    "title": "Ressources",
    "section": "",
    "text": "Documentation üìö\n\nDocumentation CASD : Documentation Data Science\nDocumentation MiDAS : Documentation en ligne MiDAS\nDocumentation du package sparklyr : Package sparklyr\nManipulation des dates : fonctions dates spark hive\n\n\n\nExemple de traitement\nSur la bulle, tu trouveras dans C:\\Users\\Public\\Docuemnts\\TUTORIEL_CLUSTER un document quarto avec un exemple de traitement minimal de MiDAS en spark sur le cluster.",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "fonctions_specifiques.html",
    "href": "fonctions_specifiques.html",
    "title": "Fonctions sp√©cifiques",
    "section": "",
    "text": "La majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences principales sont les suivantes :",
    "crumbs": [
      "Fonctions sp√©cifiques √† sparklyr"
    ]
  },
  {
    "objectID": "fonctions_specifiques.html#tableau",
    "href": "fonctions_specifiques.html#tableau",
    "title": "Fonctions sp√©cifiques",
    "section": "Tableau",
    "text": "Tableau\n\nTri dans un groupe pour effectuer un calcul s√©quentiel\n\nODD_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                memory = FALSE)\n\nODD_premier &lt;- ODD_spark %&gt;%\n  group_by(id_midas) %&gt;%\n  window_order(id_midas, KDPOD) %&gt;%\n  mutate(date_premier_droit = first(KDPOD)) %&gt;%\n  ungroup() %&gt;%\n  distinct(id_midas, KROD3, date_premier_droit) %&gt;%\n  head(5)\n\nTri pour une sortie : sdf_sort() , arrange() ne fonctionne pas\nConcat√©ner les lignes (ou les colonnes sdf_bind_cols())\n\nODD_1 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &lt;= as.Date(\"2017-12-31\")) %&gt;%\n  mutate(groupe = \"temoins\")\n\nODD_2 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &gt;= as.Date(\"2021-12-31\")) %&gt;%\n  mutate(groupe = \"traites\")\n\nODD_evaluation &lt;- sdf_bind_rows(ODD_1, ODD_2)\n\nD√©doublonner une table\n\ndroits_dans_PJC &lt;- PJC_spark %&gt;%\n  sdf_distinct(id_midas, KROD3)\n\nprint(head(droits_dans_PJC, 5))\n\nPJC_dedoublonnee &lt;- PJC_spark %&gt;%\n  sdf_drop_duplicates()\n\nprint(head(PJC_dedoublonnee, 5))\n\nPivot : les fonctions du packag tidyr ne fonctionnent pas sur donn√©es spark\n\nODD_sjr_moyen &lt;- ODD_spark %&gt;%\n  mutate(groupe = ifelse(KDPOD &lt;= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %&gt;%\n  sdf_pivot(groupe ~ KCRGC,\n    fun.aggregate = list(KQCSJP = \"mean\")\n  )",
    "crumbs": [
      "Fonctions sp√©cifiques √† sparklyr"
    ]
  },
  {
    "objectID": "fonctions_specifiques.html#statistiques",
    "href": "fonctions_specifiques.html#statistiques",
    "title": "Fonctions sp√©cifiques",
    "section": "Statistiques",
    "text": "Statistiques\n\nR√©sum√© statistique : sdf_describe() , summary()ne fonctionne pas.\nDimension : sdf_dim, la fonction nrow()ne fonctionne pas.\nQuantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, sdf_quantile()",
    "crumbs": [
      "Fonctions sp√©cifiques √† sparklyr"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html",
    "href": "bonnes_pratiques.html",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "Spark et le mode local :\n\nun seul ordinateur alors que spark est fait pour plusieurs ordinateurs distincts\nbeaucoup moins de ressources disponibles sur la bulle que sur le cluster\nmauvaise gestion de l‚Äôallocation des ressources entre utilisateurs : pas faite pour plusieurs utilisateurs\nralentissements consid√©rables et bugs : bloque les autres utilisateurs\n‚ñ∂Ô∏èspark n‚Äôest adapt√© que pour le cluster de calcul, la bulle pour faire du R sans spark sur des donn√©es peu volumineuses\n\n\n\n\n\n\n\n\n\nLes ordinateurs du cluster ont besoin de s‚Äôenvoyer des donn√©es par le r√©seau : c‚Äôest la partie la plus lente d‚Äôun programme spark !\nSi j‚Äôaugmente les ressources : par exemple, je r√©serve 3 ordinateurs du cluster plut√¥t que 2\n\nEffet puissance de calcul : plus de ressources pour faire les calculs = r√©duction du temps de calcul\nEffet augmentation des √©changes r√©seau (shuffles) : augmentation du temps de calcul\nG√™ne des autre utilisateurs\n\n\n\n\n\n\n\n\n\n\nNoteCollecter, c‚Äôest quoi ?\n\n\n\nCollecter c‚Äôest utiliser l‚Äôinstruction collect(). Elle permet de rapatrier l‚Äôensemble des r√©sultats du cluster vers la bulle et la session R de l‚Äôutilisateur en format R, par exemple des data.frames.\nCollect() :\n\nest une action : elle d√©clencher tous les calculs\nimplique des √©changes r√©seau tr√®s importants : entre ordinateurs du cluster et du cluster vers la bulle : c‚Äôest extr√™mement long, moins efficient que l‚Äôenregistrement sur disque directement depuis spark\nrappatrie les r√©sultats (une table) dans la m√©moire vive de R, qui est sur la bulle : si le r√©sultat est volumineux, cela bloque les autres utilisateurs\n\n\n\nRecommandations :\n\nNe pas collecter des tables de plus de 15 Go\nUtiliser les autres m√©thodes propos√©es pour ne pas bloquer les utilisateurs qui ont besoin de R en mode classique\nNe pas changer les configurations\n\n\n\n\nIl faut imp√©rativement fermer sa session spark apr√®s une session de travail. Deux moyens pour √ßa :\n\nfermer R Studio\nsi on ne ferme pas RStudio, utiliser la fonction spark_disconnect_all() dans son code\n\nSi on souhaite lancer un code le soir en partant, on n‚Äôoublie pas le spark_disconnect_all() √† la fin du code.\n\n\n\n\n\n\nWarningPartage des ressources\n\n\n\nLes ressources r√©serv√©s par un utilisateur ne sont lib√©r√©es pour les autres que lorsqu‚Äôil se d√©connecte. Ne pas se d√©connecter, c‚Äôest bloquer les ressources. Si j‚Äôai r√©serv√© deux ordinateurs du cluster sur 15, personne d‚Äôautres ne peut les r√©server tant que je n‚Äôai pas d√©connecter ma session spark.\nNous fermerons les sessions ouvertes trop longtemps (d√©part de cong√©s sans d√©connexion) si des utilisateurs pr√©sents en ont besoin : risque de perte du travail non enregistr√©.\n\n\nPour ne pas bloquer les coll√®gues üë®‚Äçüíª\n\n\n\nYarn permet de consulter la r√©servation des ressources par les utilisateurs.\nOn peut y acc√©der en copiant le lien suivant dans Google chrome sur la bulle (mettre en favori) : midares-deb11-nn-01.midares.local:8088/cluster\nV√©rifier que notre session est ferm√©e et qu‚Äôon ne prend pas trop de ressources : yarn\n\n\n\n\n\nAide au passage d‚Äôun code sur le cluster\nProgrammer entre coll√®gues\nContributions √† la documentation MiDAS : section fiches, √† l‚Äôaide de pull requests sur github",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html#mode-local-inadapt√©-et-mauvaise-pratique",
    "href": "bonnes_pratiques.html#mode-local-inadapt√©-et-mauvaise-pratique",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "Spark et le mode local :\n\nun seul ordinateur alors que spark est fait pour plusieurs ordinateurs distincts\nbeaucoup moins de ressources disponibles sur la bulle que sur le cluster\nmauvaise gestion de l‚Äôallocation des ressources entre utilisateurs : pas faite pour plusieurs utilisateurs\nralentissements consid√©rables et bugs : bloque les autres utilisateurs\n‚ñ∂Ô∏èspark n‚Äôest adapt√© que pour le cluster de calcul, la bulle pour faire du R sans spark sur des donn√©es peu volumineuses",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html#utiliser-les-configuratoins-recommand√©es",
    "href": "bonnes_pratiques.html#utiliser-les-configuratoins-recommand√©es",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "Les ordinateurs du cluster ont besoin de s‚Äôenvoyer des donn√©es par le r√©seau : c‚Äôest la partie la plus lente d‚Äôun programme spark !\nSi j‚Äôaugmente les ressources : par exemple, je r√©serve 3 ordinateurs du cluster plut√¥t que 2\n\nEffet puissance de calcul : plus de ressources pour faire les calculs = r√©duction du temps de calcul\nEffet augmentation des √©changes r√©seau (shuffles) : augmentation du temps de calcul\nG√™ne des autre utilisateurs",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html#ne-pas-collecter",
    "href": "bonnes_pratiques.html#ne-pas-collecter",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "NoteCollecter, c‚Äôest quoi ?\n\n\n\nCollecter c‚Äôest utiliser l‚Äôinstruction collect(). Elle permet de rapatrier l‚Äôensemble des r√©sultats du cluster vers la bulle et la session R de l‚Äôutilisateur en format R, par exemple des data.frames.\nCollect() :\n\nest une action : elle d√©clencher tous les calculs\nimplique des √©changes r√©seau tr√®s importants : entre ordinateurs du cluster et du cluster vers la bulle : c‚Äôest extr√™mement long, moins efficient que l‚Äôenregistrement sur disque directement depuis spark\nrappatrie les r√©sultats (une table) dans la m√©moire vive de R, qui est sur la bulle : si le r√©sultat est volumineux, cela bloque les autres utilisateurs\n\n\n\nRecommandations :\n\nNe pas collecter des tables de plus de 15 Go\nUtiliser les autres m√©thodes propos√©es pour ne pas bloquer les utilisateurs qui ont besoin de R en mode classique\nNe pas changer les configurations",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html#fermer-sa-session",
    "href": "bonnes_pratiques.html#fermer-sa-session",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "Il faut imp√©rativement fermer sa session spark apr√®s une session de travail. Deux moyens pour √ßa :\n\nfermer R Studio\nsi on ne ferme pas RStudio, utiliser la fonction spark_disconnect_all() dans son code\n\nSi on souhaite lancer un code le soir en partant, on n‚Äôoublie pas le spark_disconnect_all() √† la fin du code.\n\n\n\n\n\n\nWarningPartage des ressources\n\n\n\nLes ressources r√©serv√©s par un utilisateur ne sont lib√©r√©es pour les autres que lorsqu‚Äôil se d√©connecte. Ne pas se d√©connecter, c‚Äôest bloquer les ressources. Si j‚Äôai r√©serv√© deux ordinateurs du cluster sur 15, personne d‚Äôautres ne peut les r√©server tant que je n‚Äôai pas d√©connecter ma session spark.\nNous fermerons les sessions ouvertes trop longtemps (d√©part de cong√©s sans d√©connexion) si des utilisateurs pr√©sents en ont besoin : risque de perte du travail non enregistr√©.\n\n\nPour ne pas bloquer les coll√®gues üë®‚Äçüíª",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html#yarn",
    "href": "bonnes_pratiques.html#yarn",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "Yarn permet de consulter la r√©servation des ressources par les utilisateurs.\nOn peut y acc√©der en copiant le lien suivant dans Google chrome sur la bulle (mettre en favori) : midares-deb11-nn-01.midares.local:8088/cluster\nV√©rifier que notre session est ferm√©e et qu‚Äôon ne prend pas trop de ressources : yarn",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html#mutualiser-les-exp√©riences",
    "href": "bonnes_pratiques.html#mutualiser-les-exp√©riences",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "Aide au passage d‚Äôun code sur le cluster\nProgrammer entre coll√®gues\nContributions √† la documentation MiDAS : section fiches, √† l‚Äôaide de pull requests sur github",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "slides.html#au-programme",
    "href": "slides.html#au-programme",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Au programme",
    "text": "Au programme\n\nMiDAS : une base de donn√©es volumineuse üíæ\nManipuler un appariement : une op√©ration co√ªteuse üí≤\nInitiation au calcul distribu√© : quelles ressources r√©server ? üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nSparklyr : la solution ergonomique de spark sous R üë®‚Äçüíª\nPour aller plus loin ‚è©"
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS croise trois bases de donn√©es administratives exhaustives :\n\nles donn√©es sur l‚Äôinscription et l‚Äôindemnisation des demandeurs d‚Äôemploi de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;\nles donn√©es sur les b√©n√©ficiaires de minima sociaux (RSA, PPA, AAH) et les caract√©ristiques des m√©nages de la CNAF : Allstat-FR6 ;\nles donn√©es sur les contrats salari√©s de la DSN : MMO de la Dares."
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nChaque vague de MiDAS correspond √† environ 600 Go de donn√©es au format sas. Les vagues fonctionnent par empilement :\n\nle gain de profondeur temporelle et l‚Äôentr√©e dans le champ de nouvelles personnes\nles vagues sont appariables entre elles"
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS est l‚Äôune des bases de donn√©es les plus volumineuses du SSP :\nLes administrations dont les donn√©es sont comparables √† MiDAS utilisent un cluster Spark : Insee, Drees, Acoss‚Ä¶\n‚ñ∂Ô∏èLe cluster spark est la solution la plus efficiente pour traiter des donn√©es de cette ampleur. Apprendre √† l‚Äôutiliser pourra vous √™tre utile dans d‚Äôautres contextes que celui de la Dares."
  },
  {
    "objectID": "slides.html#structure-de-lappariement",
    "href": "slides.html#structure-de-lappariement",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Structure de l‚Äôappariement",
    "text": "Structure de l‚Äôappariement\n\n\n\n\n\n\n\nPourquoi Spark ?\n\n\nLa manipulation des donn√©es MiDAS en l‚Äô√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains."
  },
  {
    "objectID": "slides.html#le-format-parquet",
    "href": "slides.html#le-format-parquet",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le format parquet",
    "text": "Le format parquet\nLes donn√©es sont converties au format parquet d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l‚Äôespace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :\n\nil compresse efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv\nil est orient√© colonnes\nil permet le chargement efficace en m√©moire des donn√©es\nIl permet le stockage partitionn√© des donn√©es\nil permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires sur disque\nIl est ind√©pendant du logiciel utilis√© : il peut donc √™tre trait√© par spark et par R."
  },
  {
    "objectID": "slides.html#lespace-midares",
    "href": "slides.html#lespace-midares",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôespace MiDares",
    "text": "L‚Äôespace MiDares\n\nRessourcesSch√©ma\n\n\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n512 Go de m√©moire vive (ou RAM) : passage √† 256 Go\n\n\n\n\n\n\n\nLa m√©moire vive\n\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa volatilit√© (toutes les donn√©es sont perdues si l‚Äôordinateur n‚Äôest plus aliment√©) et par l‚Äôacc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.\n\n\n\n\nUn processeur (ou CPU) compos√© de 32 coeurs : passage √† 16 coeurs\n\n\n\n\n\n\n\nLe processeur\n\n\nLe processeur permet d‚Äôex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel‚Ä¶ Il est compos√© d‚Äôun ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu‚Äôune seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu‚Äôil a de coeurs. Un processeur se caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d‚Äôop√©rations qu‚Äôil est capable d‚Äôeffetuer par seconde."
  },
  {
    "objectID": "slides.html#programmer-en-m√©moire-vive",
    "href": "slides.html#programmer-en-m√©moire-vive",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer en m√©moire vive",
    "text": "Programmer en m√©moire vive\n\nR : la m√©moire vive, √©tat dans l‚Äôenvironnement\nSAS : lecture/√©criture sur le disque\nMiDAS au format sas &gt;&gt; taille de la m√©moire vive disponible du serveur CASD ‚Äì&gt; format .parquet\nImpossible de charger tout MiDAS en m√©moire vive\nDes solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :\narrow (avec des requ√™tes dplyr)\nduckDB : recommand√© par le SSPLab pour des donn√©es jusqu‚Äô√† 100Go\n‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements."
  },
  {
    "objectID": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "href": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les traitements co√ªteux en puissance de calcul",
    "text": "Les traitements co√ªteux en puissance de calcul\n\nles jointures\nles op√©rations en group_by()\ndistinct()\n‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n‚ñ∂Ô∏è Erreur ‚Äúout of memory‚Äù."
  },
  {
    "objectID": "slides.html#un-traitement-peu-co√ªteux-un-traitement-map",
    "href": "slides.html#un-traitement-peu-co√ªteux-un-traitement-map",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement peu co√ªteux : un traitement MAP",
    "text": "Un traitement peu co√ªteux : un traitement MAP\n\nCe traitement est peu co√ªteux :\n\nchargement d‚Äôune seule colonne en RAM : format parquet orient√© colonnes\npeu de m√©moire d‚Äôex√©cution : R est un langage vectoris√©"
  },
  {
    "objectID": "slides.html#un-traitement-co√ªteux-un-traitement-reduce",
    "href": "slides.html#un-traitement-co√ªteux-un-traitement-reduce",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement co√ªteux : un traitement REDUCE",
    "text": "Un traitement co√ªteux : un traitement REDUCE\n\nCe traitement n√©cessite :\n\nle chargement de davantage de colonnes en m√©moire vive ;\ndavantage de m√©moire d‚Äôex√©cution pour effectuer l‚Äôintersection (inner_join())."
  },
  {
    "objectID": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "href": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√© et calcul parall√®le",
    "text": "Calcul distribu√© et calcul parall√®le\n\n\nCalcul non distribu√©\nLes probl√©matiques Big Data en R sont les suivantes :\n\nla taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R\nle temps de calcul : les √©tapes du traitement sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)\nl‚Äôoptimisation du programme\n\n\nCalcul distribu√© spark\nLe calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :\n\nchargement des donn√©es en m√©moire parcimonieux et non syst√©matique\nex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents\noptimisation automatique du code"
  },
  {
    "objectID": "slides.html#un-traitement-map-distribu√©",
    "href": "slides.html#un-traitement-map-distribu√©",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement MAP distribu√©",
    "text": "Un traitement MAP distribu√©\n\nSi les donn√©es sont stock√©es sur diff√©rents ordinateurs :\n\nles calculs peuvent √™tre effectu√©s en parall√®le ;\ngain de temps li√© √† l‚Äôaugmentation des ressources informatiques pour effectuer le calcul et √† la parall√©lisation.\n\nLes traitements MAP se pr√™tent parfaitement au calcul distribu√© et parall√®le."
  },
  {
    "objectID": "slides.html#un-traitement-reduce-distribu√©",
    "href": "slides.html#un-traitement-reduce-distribu√©",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement REDUCE distribu√©",
    "text": "Un traitement REDUCE distribu√©\n\nSi les donn√©es sont stock√©es sur diff√©rents ordinateurs :\n\nil faut les rappatrier au m√™me endroit pour effectuer la jointure ;\ncet √©change est effectu√© en r√©seau entre les ordinateurs : l‚Äôenvoi r√©seau a un co√ªt non n√©gligeable en temps.\n\nLes traitements REDUCE ne se pr√™tent pas bien au calcul distribu√© et parall√®le."
  },
  {
    "objectID": "slides.html#spark",
    "href": "slides.html#spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Spark",
    "text": "Spark\n\nApache Spark : librairie open source d√©velopp√©e dans le langage scala\nScala : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java\n\nval TopHorrorsIGN2022 = Seq(\n  (9, \"Pearl\"),\n  (6, \"The Sadness\"),\n  (6, \"Offseason\"),\n  (7, \"Hatching\"),\n  (8, \"x\")\n).toDF(\"IMDB Rating\", \"IGN Movie Picks\")\n\nval TopHorrorsTheAVClub2022 = Seq(\n  (7, \"Nope\"),\n  (9, \"Pearl\"),\n  (8, \"x\"),\n  (5, \"Barbarian\"),\n  (5, \"Bones And All\")\n).toDF(\"IMDB Rating\", \"AVC Movie Picks\")\n\nimport org.apache.spark.sql.functions.col\n\nval cols = List(col(\"IGN Movie Picks\"), col(\"AVC Movie Picks\"))\n\nval query = TopHorrorsIGN2022(\n  \"IGN Movie Picks\"\n) === TopHorrorsTheAVClub2022(\"AVC Movie Picks\")\n\nval outerJoin = TopHorrorsIGN2022\n  .join(TopHorrorsTheAVClub2022, query, \"outer\")\n  .select(cols: _*)\n\nouterJoin.show()\n\nscala adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de spark et optimiser au maximum les traitements en spark\nspark est compatible avec les langages scala, R, python, java, et peut interpr√©ter des commandes SQL.\nDeux packages existent sous R :\n\nsparkR propos√© par Apache Spark\nsparklyr, qui permet d‚Äôutiliser directement des commandes dplyr traduites en spark par le package."
  },
  {
    "objectID": "slides.html#installation-de-spark-sous-casd",
    "href": "slides.html#installation-de-spark-sous-casd",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Installation de spark sous CASD",
    "text": "Installation de spark sous CASD\nVoir la fiche d√©di√©e sur le site"
  },
  {
    "objectID": "slides.html#la-machine-virtuelle-java",
    "href": "slides.html#la-machine-virtuelle-java",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La machine virtuelle Java",
    "text": "La machine virtuelle Java\nSpark est r√©dig√© en scala, un langage qui a besoin d‚Äôune machine virtuelle Java pour √™tre ex√©cut√©. La machine virtuelle Java est scalable : l‚Äôutilisateur peut choisir quelles ressources physiques elle a le droit d‚Äôutiliser sur l‚Äôensemble des ressources physiques disponibles sur l‚Äôordinateur. C‚Äôest un mini ordinateur cr√©√© par spark √† l‚Äôint√©rieur de notre propre ordinateur, qui utilise les ressources de ce dernier.\n\n\n\n\n\n\nMachine virtuelle\n\n\nUne machine virtuelle a les m√™mes caract√©ristiques qu‚Äôun ordinateur :\n\nun syst√®me d‚Äôexploitation : Windows, Linux, MacOS\ndes ressources physiques : CPU, RAM et stockage disque\n\nLa diff√©rence avec un ordinateur : une machine virtuelle peut √™tre cr√©√©e sur un serveur physique en r√©servant une petite partie des ressources du serveur seulement, ce qui permet de cr√©er plusieurs ordinateurs diff√©rents sur une seule infractructure physique"
  },
  {
    "objectID": "slides.html#deux-mani√®res-dutiliser-spark",
    "href": "slides.html#deux-mani√®res-dutiliser-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Deux mani√®res d‚Äôutiliser Spark",
    "text": "Deux mani√®res d‚Äôutiliser Spark\n\n\nAvec un seul ordinateur\nCe mode est appel√© Spark local.\n\nune unique machine virtuelle Java est cr√©√©e pour ex√©cuter le code spark\nt√¢ches parall√©lis√©es sur les diff√©rents coeurs (CPU) du processeur de la machine virtuelle Java\nl‚Äôordinateur sur lequel est cr√©√©e cette machine virtuelle Java est la bulle MiDARES, √©quivalent d‚Äôun unique gros ordinateur\n\n\nSur un cluster de calcul\nUn cluster de calcul est un ensemble d‚Äôordinateurs ou machines virtuelles connect√©s en r√©seau.\n\nune machine virtuelle Java est cr√©√©e par spark dans chaque ordinateur du cluster\nt√¢ches parall√©lis√©es sur les diff√©rents ordinateurs du cluster\nla session R reste sur la bulle MiDARES, le code R est traduit en scala puis envoy√© sur le cluster pour √™tre ex√©cut√©."
  },
  {
    "objectID": "slides.html#mode-local-sch√©ma",
    "href": "slides.html#mode-local-sch√©ma",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : sch√©ma",
    "text": "Mode local : sch√©ma"
  },
  {
    "objectID": "slides.html#mode-local-√†-√©viter",
    "href": "slides.html#mode-local-√†-√©viter",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : √† √©viter",
    "text": "Mode local : √† √©viter\nEn mode local :\n\nles ressources utilis√©es par la machine virtuelle sont celles de la bulle\nil faut allouer suffisamment de coeurs √† la JVM pour parall√©liser\nm√™me si l‚Äôutilisateur choisit des ressources faibles, les ressources r√©elles utilis√©es dans une session spark peuvent √™tre plus √©lev√©es : mauvaise gestion de l‚Äôallocation des ressources\nacc√©l√©ration sensible par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources\nSur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local : l‚Äôutilisation simultan√©e de spark par plusieurs membres de la bulle entra√Ænent des ralentissements consid√©rables\n‚ñ∂Ô∏èmode local √† √©viter absolument"
  },
  {
    "objectID": "slides.html#le-cluster-de-calcul-midares-pr√©sentation",
    "href": "slides.html#le-cluster-de-calcul-midares-pr√©sentation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le cluster de calcul Midares : pr√©sentation",
    "text": "Le cluster de calcul Midares : pr√©sentation"
  },
  {
    "objectID": "slides.html#se-connecter-√†-spark-sur-un-cluster",
    "href": "slides.html#se-connecter-√†-spark-sur-un-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Se connecter √† Spark sur un cluster",
    "text": "Se connecter √† Spark sur un cluster\nSe connecter √† spark revient √† demander √† spark de cr√©er toutes les JVM demand√©es capables de lire du scala.\nPour se connecter √† spark depuis R avec le package sparklyr :\n\nlibrary(sparklyr)\n\nconf &lt;- spark_config()\nconf$spark.executor.instances &lt;- 5\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\nLe param√®tre spark.executor.instances correspond au nombre d‚Äôordinateurs sur lequel on souhaite parall√©liser le travail d‚Äôex√©cution de code. Ici, l‚Äôutilisateur demande 5 ordinateurs du cluster.\nNous verrons plus loin quels param√®tres nous devons pr√©ciser dans le fichier de configuration."
  },
  {
    "objectID": "slides.html#une-connexion",
    "href": "slides.html#une-connexion",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une connexion",
    "text": "Une connexion\nToutes les JVM demand√©es (5) sont instanci√©es dans les ordinateurs du cluster, avec les param√®tres d√©finis."
  },
  {
    "objectID": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr",
    "href": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La vie d‚Äôun programme r√©dig√© en sparklyr",
    "text": "La vie d‚Äôun programme r√©dig√© en sparklyr\nAvec sparklyr, il est possible de programmer directement en dplyr pour utiliser spark.\n\n# un data frame que j'envoie dans spark\nun_df &lt;- data.frame(c(1,2,3), c(\"A\", \"B\", \"C\"))\nnames(un_df) &lt;- c(\"col_num\", \"col_char\")\n\n# C'est maintenant un spark_data_frame\ncopy_to(sc, un_df)\n    \nun_df_transforme &lt;- un_df %&gt;%\n    mutate(une_nouvelle_col = col_num*2)\n\nSi j‚Äôex√©cute ce programme, je ne pourrai pas ouvrir un_df_transforme, d‚Äôailleurs, il ne se sera rien pass√©."
  },
  {
    "objectID": "slides.html#la-lazy-evaluation",
    "href": "slides.html#la-lazy-evaluation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark distingue deux types d‚Äôop√©rations :\n\nles transformations : ce sont des op√©rations qui prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul lorsqu‚Äôelles sont appel√©es.\nPar exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d‚Äôex√©cution :\n\nune_transformation &lt;- un_spark_data_frame %&gt;%\n  group_by(identifiant) %&gt;%\n  mutate(une_somme = sum(revenus))\n\nles actions : ce sont des op√©rations qui demandent le calcul d‚Äôun r√©sultat et qui d√©clenchent le calcul et l‚Äôex√©cution de toutes les transformations compil√©es jusqu‚Äô√† l‚Äôappel de l‚Äôaction.\nPar exemple, le programme ci-dessous d√©clenche le calcul de la cellule une_transformation et de la moyenne des revenus :\n\nrevenu_moyen &lt;- une_transformation %&gt;%\n  summarise(revenu_moyen = mean(une_somme)) %&gt;%\n  print()\n\nLes principales actions sont : print(), collect(), head(), tbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser)."
  },
  {
    "objectID": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr-1",
    "href": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La vie d‚Äôun programme r√©dig√© en sparklyr",
    "text": "La vie d‚Äôun programme r√©dig√© en sparklyr\nPrenons l‚Äôexemple d‚Äôun programme contenant une action."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver",
    "href": "slides.html#le-r√¥le-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver",
    "text": "Le r√¥le du driver\n\n\nLe programme R est traduit en scala gr√¢ce au package sparklyr\nLe driver √©value le programme, il lit le code scala mais n‚Äôex√©cute rien du tout\nS‚Äôil remarque une erreur, l‚Äôerreur est envoy√©e directement √† l‚Äôutilisateur en session R avant l‚Äôex√©cution du programme : c‚Äôest la force de la lazy evaluation."
  },
  {
    "objectID": "slides.html#le-plan-dex√©cution",
    "href": "slides.html#le-plan-dex√©cution",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le plan d‚Äôex√©cution",
    "text": "Le plan d‚Äôex√©cution\n\nsource : documentation CASD disponible √† Documentation Data Science\nAJOUTER UN DAG"
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver-catalyst",
    "href": "slides.html#le-r√¥le-du-driver-catalyst",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver : Catalyst",
    "text": "Le r√¥le du driver : Catalyst\nLe driver contient un programme nomm√© Catalyst qui optimise le code scala automatiquement.\nSpark optimise automatiquement les programmes soumis :\n\nCompilation des transformations pour soulever les √©ventuelles erreurs\nInt√©gration dans un plan d‚Äôex√©cution contenant les √©tapes n√©cessaires pour parvenir au r√©sultat demand√© par le programme\nOptimisation du plan logique par le module Catalyst (driver Spark)\n\nPar exemple si j‚Äô√©cris le programme :\n\nnon_optimal &lt;- table_1 %&gt;%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\"))\n\nCatalyst r√©√©crit :\n\noptimal &lt;- table_1 %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\")) %&gt;%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))\n\nCette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu‚Äôune action d√©clenche l‚Äôex√©cution."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver-catalyst-1",
    "href": "slides.html#le-r√¥le-du-driver-catalyst-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver : Catalyst",
    "text": "Le r√¥le du driver : Catalyst"
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver-catalyst-2",
    "href": "slides.html#le-r√¥le-du-driver-catalyst-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver : Catalyst",
    "text": "Le r√¥le du driver : Catalyst\n\nR√©alisation de plans physiques possibles et s√©lection du meilleur plan physique (au regard de la localisation des donn√©es requises). Le plan physique est la distribution des diff√©rents calculs aux machines du cluster.\nD√©clencher le moins d‚Äôactions possibles dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.\nPour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-cluster-manager",
    "href": "slides.html#le-r√¥le-du-cluster-manager",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du cluster manager",
    "text": "Le r√¥le du cluster manager\n\nLe cluster manager distribue les traitements physiques aux ordinateurs du cluster :\n\nil conna√Æt le meilleur plan physique fourni par Catalyst ;\nil conna√Æt les ressources disponibles et occup√©es par toutes les machines du cluster ;\nil affecte les ressources disponibles √† la session spark."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-worker",
    "href": "slides.html#le-r√¥le-du-worker",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du worker",
    "text": "Le r√¥le du worker\n\nLe worker effectue le morceau de programme qu‚Äôon lui affecte et renvoie le r√©sultat au driver, qui lui-m√™me affiche le r√©sultat en session R :\n\nil ne conna√Æt que les t√¢ches qu‚Äôon lui a affect√©es ;\nil peut communiquer avec le driver en r√©seau pour renvoyer un r√©sultat ;\nil peut communiquer avec les autres workers en r√©seau pour partager des donn√©es ou des r√©sultats interm√©diaires : c‚Äôest un shuffle."
  },
  {
    "objectID": "slides.html#o√π-sont-les-donn√©es",
    "href": "slides.html#o√π-sont-les-donn√©es",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π sont les donn√©es ?",
    "text": "O√π sont les donn√©es ?"
  },
  {
    "objectID": "slides.html#o√π-sont-les-donn√©es-1",
    "href": "slides.html#o√π-sont-les-donn√©es-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π sont les donn√©es ?",
    "text": "O√π sont les donn√©es ?"
  },
  {
    "objectID": "slides.html#transfert-de-la-bulle-√†-hdfs",
    "href": "slides.html#transfert-de-la-bulle-√†-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Transfert de la bulle √† HDFS",
    "text": "Transfert de la bulle √† HDFS"
  },
  {
    "objectID": "slides.html#transfert-de-hdfs-√†-la-bulle",
    "href": "slides.html#transfert-de-hdfs-√†-la-bulle",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Transfert de HDFS √† la bulle",
    "text": "Transfert de HDFS √† la bulle"
  },
  {
    "objectID": "slides.html#mais-o√π-sont-r√©ellement-les-donn√©es-hdfs",
    "href": "slides.html#mais-o√π-sont-r√©ellement-les-donn√©es-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mais o√π sont r√©ellement les donn√©es ? HDFS",
    "text": "Mais o√π sont r√©ellement les donn√©es ? HDFS\nHadoop Distributed File System (HDFS)\n\nstockage sur diff√©rentes machines : ici les noeuds du cluster spark, c‚Äôest-√†-dire les diff√©rents ordinateurs workers du cluster\ndonn√©es divis√©es en blocs plus petits de taille fixe et r√©partis sur les machines : aucune table de MiDAS n‚Äôexiste en entier sur le cluster\nchaque bloc est r√©pliqu√© trois fois : il existe trois fois les 10 premi√®res lignes de la table FNA sur trois ordinateurs diff√©rents du cluster (r√©silience)\nun NameNode supervise les m√©tadonn√©es et g√®re la structure du syst√®me de fichiers\nles DataNodes stockent effectivement les blocs de donn√©es : les datanodes sont en fait les disques des workers du cluster, chaque ordinateur du cluster dispose d‚Äôun disque avec une partie des donn√©es MiDAS\nle syst√®me HDFS est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local"
  },
  {
    "objectID": "slides.html#param√©trer-sa-session",
    "href": "slides.html#param√©trer-sa-session",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Param√©trer sa session",
    "text": "Param√©trer sa session\nIl faut pr√©ciser quelles ressources r√©server pour chaque unit√© spark : le driver, le nombre d‚Äôordinateurs workers (appel√©es instances), la RAM, le nombre de coeurs\nLa configuration par d√©faut est :\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\nLes ressources physiques utilis√©es :\n\npar le driver : avec spark.driver.memory (avec parcimonie)\npar chaque worker avec spark.executor.memory(valeur max 140 Go) et spark.executor.cores (valeur max 8 coeurs)\nle nombre de workers avec spark.executor.instances (2 ou 3 suffisent)\nLa file sur laquelle on travaille avec spark.yarn.queue (prod ou dev)\n\nle nombre de partitions de chaque spark_data_frame avec spark.sql.shuffle.partitions (200 par d√©faut)\nla limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec spark.driver.maxResultSize (0 est la meilleure option)\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)"
  },
  {
    "objectID": "slides.html#mode-cluster-non-concurrence-gr√¢ce-au-cluster-manager",
    "href": "slides.html#mode-cluster-non-concurrence-gr√¢ce-au-cluster-manager",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode cluster : non concurrence gr√¢ce au cluster manager",
    "text": "Mode cluster : non concurrence gr√¢ce au cluster manager\n\nLe mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d‚Äôun serveur. Ces machines communiquent en r√©seau.\nCapture d‚Äô√©cran r√©servation des ressources\nIl est donc n√©cessaire de se d√©connecter pour lib√©rer les ressources : des ressources r√©serv√©es, m√™me lorsqu‚Äôaucun programme ne tourne, ne peuvent jamais √™tre affect√©es √† d‚Äôautres utilisateurs."
  },
  {
    "objectID": "slides.html#importer-les-donn√©es-depuis-hdfs-sous-r",
    "href": "slides.html#importer-les-donn√©es-depuis-hdfs-sous-r",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Importer les donn√©es depuis HDFS sous R",
    "text": "Importer les donn√©es depuis HDFS sous R\nLes donn√©es doivent √™tre disponibles dans les workers sous forme de spark_data_frame :\n\ncach√© en m√©moire directement : si utilis√©es de tr√®s nombreuses fois pour gagner du temps\nlaiss√© sur disque tant qu‚Äôaucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire\n‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.\nPour passer un data.frame R en spark_data_frame : copy_to()\n\n\npjc_df_spark &lt;- spark_read_parquet(sc,\n                                  path = \"hdfs:///dataset/MiDAS_v4/FNA/pjc.parquet\",\n                                  memory = TRUE)\n\npjc_filtree &lt;- pjc_df_spark %&gt;%\n  filter(KDDPJ &gt;= as.Date(\"2022-01-01\"))\n\nspark_write_parquet(pjc_filtree, \"hdfs:///tmp/pjc_filtree.parquet\")\n\npjc_df_spark &lt;- copy_to(sc, \"PJC\")"
  },
  {
    "objectID": "slides.html#les-exports-sur-hdfs",
    "href": "slides.html#les-exports-sur-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les exports sur HDFS",
    "text": "Les exports sur HDFS\n\n\n\n\n\n\nLes exports sur HDFS\n\n\nLorsqu‚Äôon exporte une table depuis notre session R vers HDFS, celle-ci est automatiquement partitionn√©e, comme le reste des donn√©es.\nAinsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\nIl est possible de ma√Ætriser le nombre de partitions avec la commande sdf_coalesce(partitions = 5) du package sparklyr.\nL‚Äôid√©al est d‚Äôadapter le nombre de partitions √† la taille d‚Äôun bloc : un bloc mesure 128 MB. Lorsqu‚Äôun bloc disque est utilis√©, m√™me √† 1%, il n‚Äôest pas utilisable pour un autre stockage.\nExporter un fichier de 1MB en 200 partitions r√©serve 200 blocs inutilement."
  },
  {
    "objectID": "slides.html#les-shuffles",
    "href": "slides.html#les-shuffles",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les shuffles",
    "text": "Les shuffles\n\nComme nous l‚Äôavons vu, les traitements REDUCE ne se pr√™tent pas tr√®s bien au calcul distribu√© :\n\naugmenter le nombre de workers augmente la probabilit√© de devoir effectuer des shuffles\nil est recommand√© de se limiter √† deux workers comme dans la configuration propos√©e\nr√©server d‚Äôautres ressources n‚Äôest souvent pas efficient et monopolise les ressources pour les autres utilisateurs."
  },
  {
    "objectID": "slides.html#r√©cup√©rer-un-r√©sultat",
    "href": "slides.html#r√©cup√©rer-un-r√©sultat",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "R√©cup√©rer un r√©sultat",
    "text": "R√©cup√©rer un r√©sultat\nLes r√©sultats qu‚Äôil est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :\n\nune table filtr√©e avec les variables n√©cessaires √† l‚Äô√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;\ndes statistiques descriptives synth√©tiques ;\nles premi√®res lignes de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;\nune table agr√©g√©e pour un graphique par exemple, √† l‚Äôaide de la fonction summarise()."
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "href": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver"
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-du-driver-1",
    "href": "slides.html#lutilisation-de-la-m√©moire-du-driver-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver\nLorsqu‚Äôil est n√©cessaire de collecter une table volumineuse, il faut donc pr√©voir assez de m√©moire RAM pour le driver.\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\n\n\nBonne pratique de partage des ressources\n\n\nLe driver est instanci√© dans la bulle Midares, qui a vocation √† √™tre r√©duite suite √† la g√©n√©ralisation du cluster.\n\nLa bulle Midares a besoin de RAM minimale pour fonctionner, 100% des ressources ne sont donc pas disponibles pour sparklyr.\nPour permettre le travail simultan√© fluide de 10 utilisateurs, la m√©moire allou√©e au driver recommand√©e pour chaque utilisateur est de 15 Go.\nL‚Äôexport d‚Äôune table sdf directement au format .parquet est une alternative plus rapide, plus efficiente et qui permet par la suite de charger ses donn√©es en R classique et de travailler sur un df R sans utiliser sparklyr."
  },
  {
    "objectID": "slides.html#comment-tester-son-code-pour-collecter-le-moins-possible",
    "href": "slides.html#comment-tester-son-code-pour-collecter-le-moins-possible",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Comment tester son code pour collecter le moins possible ?",
    "text": "Comment tester son code pour collecter le moins possible ?\nLa programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu‚Äô√† la fin pour que Catalyst optimise l‚Äôensemble du programme\nLa principale diff√©rence avec la programmation en R classique est que la visualisation de tables compl√®tes volumineuses n‚Äôest pas recommand√©e :\n\ngoulets d‚Äô√©tranglement m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;\nlongue : √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;\nbeaucoup moins efficace que l‚Äôexport direct en parquet du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l‚Äô√©tude.\n\nS‚Äôil est n√©cessaire de collecter, il faut pr√©voir beaucoup de RAM pour le driver avec le param√®tre spark.driver.memory."
  },
  {
    "objectID": "slides.html#ce-qui-change-pour-lutilisateur",
    "href": "slides.html#ce-qui-change-pour-lutilisateur",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ce qui change pour l‚Äôutilisateur",
    "text": "Ce qui change pour l‚Äôutilisateur\nLa majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences principales sont les suivantes :\n\n\n\n\n\n\n\n\nFonctionnalit√©\ntidyverse\nsparklyr\n\n\n\n\nimport d‚Äôun fichier .parquet\nread_parquet\nspark_read_parquet()\n\n\ntri d‚Äôun tableau\narrange()\nwindow_order() ou sdf_sort()\n\n\nop√©rations sur les dates\nlubridate\nfonctions Hive\n\n\nempiler des tableaux\nbind_rows()\nsdf_bind_rows()\n\n\nnombre de lignes d‚Äôun tableau\nnrow()\nsdf_nrow()\n\n\nfaire pivoter un tableau\ntidyr\nsdf_pivot()\n\n\nexport d‚Äôun spark_data_frame\n\nspark_write_parquet()"
  },
  {
    "objectID": "slides.html#quelques-fonctions-sp√©cifiques",
    "href": "slides.html#quelques-fonctions-sp√©cifiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques fonctions sp√©cifiques",
    "text": "Quelques fonctions sp√©cifiques\n\nDatesTableauStatistiques\n\n\nLes fonctions de lubridate()ne sont pas adapt√©es au spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nAdd_months\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\n\n\n\n\n\n\n\nFormat\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\nTri dans un groupe pour effectuer un calcul s√©quentiel\n\nODD_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                memory = FALSE)\n\nODD_premier &lt;- ODD_spark %&gt;%\n  group_by(id_midas) %&gt;%\n  window_order(id_midas, KDPOD) %&gt;%\n  mutate(date_premier_droit = first(KDPOD)) %&gt;%\n  ungroup() %&gt;%\n  distinct(id_midas, KROD3, date_premier_droit) %&gt;%\n  head(5)\n\nTri pour une sortie : sdf_sort() , arrange() ne fonctionne pas\nConcat√©ner les lignes (ou les colonnes sdf_bind_cols())\n\nODD_1 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &lt;= as.Date(\"2017-12-31\")) %&gt;%\n  mutate(groupe = \"temoins\")\n\nODD_2 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &gt;= as.Date(\"2021-12-31\")) %&gt;%\n  mutate(groupe = \"traites\")\n\nODD_evaluation &lt;- sdf_bind_rows(ODD_1, ODD_2)\n\nD√©doublonner une table\n\ndroits_dans_PJC &lt;- PJC_spark %&gt;%\n  sdf_distinct(id_midas, KROD3)\n\nprint(head(droits_dans_PJC, 5))\n\nPJC_dedoublonnee &lt;- PJC_spark %&gt;%\n  sdf_drop_duplicates()\n\nprint(head(PJC_dedoublonnee, 5))\n\nPivot : les fonctions du packag tidyr ne fonctionnent pas sur donn√©es spark\n\nODD_sjr_moyen &lt;- ODD_spark %&gt;%\n  mutate(groupe = ifelse(KDPOD &lt;= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %&gt;%\n  sdf_pivot(groupe ~ KCRGC,\n    fun.aggregate = list(KQCSJP = \"mean\")\n  )\n\n\n\n\n\nR√©sum√© statistique : sdf_describe() , summary()ne fonctionne pas.\nDimension : sdf_dim, la fonction nrow()ne fonctionne pas.\nQuantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, sdf_quantile()\nEchantillonnage al√©atoire : sdf_random_split"
  },
  {
    "objectID": "slides.html#quelques-tips-doptimisation",
    "href": "slides.html#quelques-tips-doptimisation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques tips d‚Äôoptimisation",
    "text": "Quelques tips d‚Äôoptimisation\n\nJointuresPersistChargementExport et partitions\n\n\nPour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse\n\nSolution rapide :\n\ntable_finale &lt;- table_volumineuse_comme_PJC %&gt;%\n  right_join(petite_table_mon_champ)\n\nSolution lente :\n\ntable_finale &lt;- petite_table_mon_champ %&gt;%\n  left_join(table_volumineuse_comme_PJC)\n\n\n\nLorsqu‚Äôune table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c‚Äôest-√†-dire enregistrer ce spark_data_framesur le disque ou dans la m√©moire des noeuds.\n\ntable_1 &lt;- mon_champ %&gt;%\n  left_join(ODD, by = c(\"id_midas\", \"KROD3\")) %&gt;%\n  rename(duree_potentielle_indemnisation = KPJDXP,\n         SJR = KQCSJP,\n         date_debut_indemnisation = KDPOD) %&gt;%\n  sdf_persist()\n\nduree &lt;- table_1 %&gt;%\n  summarise(duree_moy = mean(duree_potentielle_indemnisation),\n            duree_med = median(duree_potentielle_indemnisation)) %&gt;%\n  collect()\n\nSJR &lt;- table_1 %&gt;%\n  summarise(SJR_moy = mean(SJR),\n            SJR_med = median(SJR)) %&gt;%\n  collect()\n\n\n\nLorsqu‚Äôon charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.\nAttention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l‚Äôex√©cution.\n\n\nLe format .parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.\nSi les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d‚Äôune m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul\n\nspark_write_parquet(DE, partition_by = c(\"REGIND\"))\n\n\n\n\n\n\n\nExports simultan√©s\n\n\nHDFS supporte les exports simultan√©s, mais le temp d‚Äôexport est plus long lorsque le NameNode est requ√™t√© par plusieurs personnes simultan√©ment : d‚Äôapr√®s les tests cluster\n\npour un petit export (5 minutes), le temps peut √™tre multipli√© par 4 ;\npour un gros export (15 minutes), le temps peut √™tre multipli√© par 2."
  },
  {
    "objectID": "slides.html#forcer-le-calcul",
    "href": "slides.html#forcer-le-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Forcer le calcul",
    "text": "Forcer le calcul\nQuelques actions :\n\ncollecter la table enti√®re üõë\n\nspark_data_frame_1 %&gt;%\n  collect()\n\nafficher les premi√®res lignes\n\nspark_data_frame_1 %&gt;%\n  head(10)\n\nMettre les donner en cache\n\nspark_data_frame_1 %&gt;%\n  sdf_register() %&gt;%\n  tbl_cache()\n\nsc %&gt;% spark_session() %&gt;% invoke(\"catalog\") %&gt;% \n  invoke(\"clearCache\")"
  },
  {
    "objectID": "slides.html#les-erreurs-en-sparklyr",
    "href": "slides.html#les-erreurs-en-sparklyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les erreurs en sparklyr",
    "text": "Les erreurs en sparklyr\nsparklyr traduit le code dplyr fourni en scala, mais interpr√®te √©galement les messages d‚Äôerreurs envoy√©s du cluster vers la session R.\nsparklyr n‚Äôest cependant pas performant pour interpr√©ter ces erreurs.\nN‚Äôh√©sitez pas √† enregistrer le code g√©n√©rant un message d‚Äôerreur dans Documents publics/erreurs_sparklyr\nUn test du code pas-√†-pas permet d‚Äôisoler le probl√®me."
  },
  {
    "objectID": "slides.html#bonnes-pratiques",
    "href": "slides.html#bonnes-pratiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\n\nD√©connexion ou fermeture R pour lib√©rer les ressources üõë\nNe plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nPyspark ou Sparklyr pour la production ‚ùì\nUtilisation parcimonieuse des ressources ‚öñÔ∏è\nEnvoi des erreurs sparklyr üì©"
  },
  {
    "objectID": "slides.html#larchitecture-map-reduce",
    "href": "slides.html#larchitecture-map-reduce",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôarchitecture Map Reduce",
    "text": "L‚Äôarchitecture Map Reduce"
  },
  {
    "objectID": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "href": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La gestion de la m√©moire avec spark",
    "text": "La gestion de la m√©moire avec spark\nLes shuffles sont les op√©rations les plus gourmandes en temps.\n\n\n\n\n\n\nQu‚Äôest-ce qu‚Äôun shuffle ?\n\n\nUn shuffle est un √©change de donn√©es entre diff√©rents noeuds du cluster.\nNous avons vu qu‚Äôutiliser spark dans un cluster implique de distribuer √©galement le stockage des donn√©es.\nPar exemple :\n\nje demande un traitement sur la table PJC du FNA\nsi un noeud contenant d√©j√† les donn√©es de PJC est disponible, le cluster manager envoie le traitement √† ce noeud\nsi tous les noeuds contenant les donn√©es de PJC sont d√©j√† r√©serv√©s, alors le cluster manager demande le traitement √† un autre noeud, par exemple le noeud 1\nil demande √† un noeud contenant les donn√©es PJC, par exemple le noeud 4, d‚Äôenvoyer ces donn√©es au noeud 1 qui va ex√©cuter le traitement\ncet √©change de donn√©es est en r√©seau filaire : un √©change filaire est beaucoup plus lent qu‚Äôun envoi interne par le disque du noeud 1 √† la RAM du noeud 1\nc‚Äôest pourquoi pour optimiser un programme spark, il est possible de limiter les shuffles"
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "href": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire dans un worker",
    "text": "L‚Äôutilisation de la m√©moire dans un worker\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nNe pas charger plusieurs fois les m√™mes donn√©es en cache, ou si besoin augmenter la part de la m√©moire allou√©e au stockage avec spark.memory.storageFraction.\n\n\n\n\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.memory.fraction\"] &lt;- 0.8\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.memory.fraction\"] &lt;- 0.8\nconf[\"spark.memory.storageFraction\"] &lt;- 0.4\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)"
  },
  {
    "objectID": "slides.html#sparkui-un-outil-doptimisation",
    "href": "slides.html#sparkui-un-outil-doptimisation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "SparkUI : un outil d‚Äôoptimisation",
    "text": "SparkUI : un outil d‚Äôoptimisation\nSpark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d‚Äôoptimiser les traitements :\n\nDAGGCM√©moire\n\n\n\n\n\nV√©rifier que le gc time est inf√©rieur √† 10% du temps pour ex√©cuter la t√¢che ‚úÖ\n\n\n\nV√©rifier que la storage memory ne sature pas la m√©moire ‚úÖ"
  },
  {
    "objectID": "slides.html#utiliser-les-interfaces",
    "href": "slides.html#utiliser-les-interfaces",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Utiliser les interfaces",
    "text": "Utiliser les interfaces\n\nyarn : disponibilit√© des ressources\n\nSparkhistory pour des traitements de sessions ferm√©es\n\nLe sparkhistory entra√Æne l‚Äôenregistrement de logs assez lourdes, il est donc d√©sactiv√© par d√©faut. Pour l‚Äôactiver sur un programme :\n\nconf &lt;- spark_config()\nconf[\"spark.eventLog.enabled\"] &lt;- \"true\"\nconf[\"spark.eventLog.dir\"] &lt;- \"hdfs://midares-deb11-nn-01.midares.local:9000/spark-logs\"\nconf[\"appName\"] &lt;- \"un_nom_de_traitement\"\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)"
  },
  {
    "objectID": "slides.html#ma-session-ne-sinstancie-jamais",
    "href": "slides.html#ma-session-ne-sinstancie-jamais",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ma session ne s‚Äôinstancie jamais",
    "text": "Ma session ne s‚Äôinstancie jamais\nSi l‚Äôinstruction sc &lt;- spark_connect(master = \"yarn\", config = conf prend plus de 10 minutes, il est utile d‚Äôouvrir l‚Äôinterface de yarn pour v√©rifier que la file n‚Äôest pas d√©j√† enti√®rement occup√©e. L‚Äôerreur peut ne survenir qu‚Äôau bout d‚Äôune vingtaine de minutes : le job est ACCEPTED dans yarn, ou FAILED si la session n‚Äôa pas pu √™tre instanci√©e par manque de ressources disponibles."
  },
  {
    "objectID": "slides.html#exporter-de-hdfs-au-local",
    "href": "slides.html#exporter-de-hdfs-au-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Exporter de HDFS au local",
    "text": "Exporter de HDFS au local"
  },
  {
    "objectID": "slides.html#pyspark-mode-cluster",
    "href": "slides.html#pyspark-mode-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pyspark : mode cluster",
    "text": "Pyspark : mode cluster"
  },
  {
    "objectID": "slides.html#les-avantages-de-pyspark",
    "href": "slides.html#les-avantages-de-pyspark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les avantages de pyspark",
    "text": "Les avantages de pyspark\n\nMode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è\nSpark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è\nPlusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª\nStabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ\nLisibilit√© du code üëì\nTemps de connexion et d‚Äôex√©cution r√©duit ‚è≤Ô∏è\nUtilisation optimale de SparkUI üìä"
  },
  {
    "objectID": "slides.html#merci-pour-votre-attention",
    "href": "slides.html#merci-pour-votre-attention",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Merci pour votre attention !",
    "text": "Merci pour votre attention !"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atelier d‚Äôinitiation √† spark avec R en mode cluster",
    "section": "",
    "text": "Bienvenue ! üëã\nCe site est une aide √† l‚Äôutilisation de spark avec R sur un cluster de calcul Spark √† destination de d√©butants en R ne connaissant pas du tout le fonctionnement et la programmation en spark.\nTu vas apprendre ici (je l‚Äôesp√®re üôÇ) :\n\nl‚Äôint√©r√™t d‚Äôutiliser Spark pour manipuler des donn√©es volumineuses telles que l‚Äôappariement MiDAS ‚è≥\nle fonctionnement de Spark sur un cluster de calcul üî¢\nl‚Äôutilisation de Spark sous R avec le package sparklyr, tr√®s facile si tu connais dplyr üë®‚Äçüíª\nquelques pistes d‚Äôoptimisation d‚Äôun programme avec Spark üí°\n\nDe quoi profiter ensuite du temps gagn√© gr√¢ce au calcul distribu√© ! üöÄ",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "slides_v.html#au-programme",
    "href": "slides_v.html#au-programme",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Au programme",
    "text": "Au programme\n\nMiDAS : une base de donn√©es volumineuse üìö\nUtiliser MiDAS avec R : un d√©fi üí≠\nSparklyr : l‚Äôoutil ergonomique de spark en R üë®‚Äçüíª\nLes bonnes pratiques sur une infrastructure partag√©e üñ•Ô∏è\nOptimiser la m√©moire : pourquoi et comment ‚è≥\nPour aller plus loin üí°",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#quest-ce-que-midas",
    "href": "slides_v.html#quest-ce-que-midas",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Qu‚Äôest-ce que MiDAS ?",
    "text": "Qu‚Äôest-ce que MiDAS ?",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#une-des-bases-les-plus-volumineuses-du-ssp",
    "href": "slides_v.html#une-des-bases-les-plus-volumineuses-du-ssp",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une des bases les plus volumineuses du SSP",
    "text": "Une des bases les plus volumineuses du SSP\n\nLes administrations dont les donn√©es sont comparables √† MiDAS utilisent un cluster Spark : Insee, Drees, Acoss, UNEDIC, Cnaf\n‚ñ∂Ô∏èLe cluster spark est une solution tr√®s efficiente pour traiter des donn√©es de cette ampleur.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#concr√®tement-quest-ce-que-midas",
    "href": "slides_v.html#concr√®tement-quest-ce-que-midas",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Concr√®tement, qu‚Äôest-ce que MiDAS ?",
    "text": "Concr√®tement, qu‚Äôest-ce que MiDAS ?\n\n\n\n\n\n\n\nPourquoi Spark ?\n\n\nLa manipulation des donn√©es MiDAS en l‚Äô√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#o√π-est-midas-sur-la-bulle",
    "href": "slides_v.html#o√π-est-midas-sur-la-bulle",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π est MiDAS sur la bulle ?",
    "text": "O√π est MiDAS sur la bulle ?\nDisponible dans l‚Äôespace commun (= Documents publics) : C:\\Users\\Public\\Documents\\MiDAS_parquet\\Vague X\n\nAu format parquet :\n\ncompression efficace des donn√©es : taux de compression de 5 √† 10 par rapport au format csv\norient√© colonnes\nchargement efficace en m√©moire des donn√©es\nstockage partitionn√© des donn√©es avec write_dataset()\ntraiter des donn√©es sur disque\nind√©pendant du logiciel utilis√© : R, python, spark‚Ä¶",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-documentation-en-ligne",
    "href": "slides_v.html#la-documentation-en-ligne",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La documentation en ligne",
    "text": "La documentation en ligne\n\n\n\nDocumentation en ligne\n\nDictionnaire des donn√©es\nFiches pr√©sentant les concepts de l‚Äôindemnisation, du retour √† l‚Äôemploi\nExemples d‚Äôimpl√©mentation en R\nConseils qualit√© des variables\n\n\n\n\n\n\n\n\n\nContribuer avec github, montrer le github, dire que sur le read me conseils pour contribuer et mettre des codes, espace de partage de codes √† venir",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#une-bulle-casd",
    "href": "slides_v.html#une-bulle-casd",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une bulle CASD",
    "text": "Une bulle CASD\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n256 Go de m√©moire vive (ou RAM)\nUn processeur (ou CPU) compos√© de 16 coeurs\n\n\n\nBulle CASD = un gros ordinateur partag√© par plusieurs utilisateurs, besoin du voc ordinateur pour comprendre spark",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#une-bulle-casd-1",
    "href": "slides_v.html#une-bulle-casd-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une bulle CASD",
    "text": "Une bulle CASD\n\nLe disque durLa m√©moire viveLe processeur\n\n\nLe disque dur, aussi appel√© Hard Disk Drive (HDD), est une solution de stockage permanente :\n\nles donn√©es sont conserv√©es m√™me apr√®s l‚Äôarr√™t de l‚Äôappareil\nl‚Äôespace de stockage est volumineux\nmais les op√©rations d‚Äô√©criture et de lecture ne sont pas du tout instantann√©es\n\n\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) :\n\npar sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs\npar sa volatilit√© (toutes les donn√©es sont perdues si l‚Äôordinateur n‚Äôest plus aliment√©)\npar l‚Äôacc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.\n\n\n\nLe processeur :\n\npermet d‚Äôex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel\nest compos√© d‚Äôun ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu‚Äôune seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu‚Äôil a de coeurs\nse caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d‚Äôop√©rations qu‚Äôil est capable d‚Äôeffetuer par seconde.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#traiter-midas-en-r-les-limites",
    "href": "slides_v.html#traiter-midas-en-r-les-limites",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Traiter MiDAS en R : les limites",
    "text": "Traiter MiDAS en R : les limites\n\nCharger les donn√©es en m√©moire vive\n\n\n  path_fna &lt;- \"C:/Users/Public/Documents/MiDAS_parquet/Vague 4/FNA/\"\n  \n  PJC &lt;- read_parquet(paste0(path_fna, \"pjc.parquet\"), memory = TRUE)\n  ODD &lt;- read_parquet(paste0(path_fna, \"odd.parquet\"), memory = TRUE)\n\n\n\nR√©aliser des op√©rations co√ªteuses en ressources\n\n\njointure &lt;- PJC %&gt;%\n  rename(KROD1 = KROD3) %&gt;%\n  left_join(ODD, by = c(\"id_midas\", \"KROD1\"))\n\n\n\n\nLe partage des ressources de la bulle\n\nChaque utilisateur peut mobiliser toutes les ressouces de la bulle.\n\nDonn√©es &gt; RAM et R fonctionne dans la m√©moire vive (pour √ßa que plus rapide que SAS) Jointures co√ªteux : on va voir pourquoi apr√®s tlm sur la m√™me bulle sans allocation des ressources = ralentissements",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#traitement-l√©ger-versus-co√ªteux",
    "href": "slides_v.html#traitement-l√©ger-versus-co√ªteux",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Traitement l√©ger versus co√ªteux",
    "text": "Traitement l√©ger versus co√ªteux\n\nMAP = l√©gerREDUCE = co√ªteuxREDUCE en R\n\n\n\n\n\n\n\n\nCe traitement est peu co√ªteux :\n\nchargement d‚Äôune seule colonne en RAM : format parquet orient√© colonnes\npeu de m√©moire d‚Äôex√©cution : R est un langage vectoris√©\n\n\n\n\n\n\n\n\n\n\nCe traitement n√©cessite :\n\nle chargement de davantage de colonnes en m√©moire vive ;\ndavantage de m√©moire d‚Äôex√©cution pour effectuer l‚Äôintersection (inner_join()).\n\n\n\n\n\nles jointures\nles op√©rations en group_by()\nles op√©rations de tri avec arrange()\ndistinct()\n‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n‚ñ∂Ô∏è Erreur ‚Äúout of memory‚Äù.\n\n\n\n\n\nParquet orient√© colonne donc ne charge que les colonnes n√©cessaires en m√©moire R vectoris√© : op√©ration appliqu√©e √† tout le vecteur = traitement rapide\nJointure co√ªteuse parce que comparaison ligne √† ligne\nwindow funcions",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#pourquoi-spark-1",
    "href": "slides_v.html#pourquoi-spark-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pourquoi spark ?",
    "text": "Pourquoi spark ?\n\n\n\n\n\n\n\n\n\nSolution test√©e\nAvantage\nDestination d‚Äôusage\n\n\n\n\nPackage ¬´ data.table ¬ª\nCalculs parall√©lis√©s\npour bases &lt; RAM\n\n\nFormat ¬´ parquet ¬ª +\npackage ¬´ arrow ¬ª\nStockage moins lourd\nChargement efficient\npour bases &lt; RAM\n\n\nDuckDB\nGestionnaire de BDD\nPour des bases &lt; 100 Go\n\n\nSpark\nTraitements distribu√©s plus rapides\nTraitement de donn√©es volumineuses\nPour des bases &gt; 100 Go",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#un-gain-de-temps-consid√©rable",
    "href": "slides_v.html#un-gain-de-temps-consid√©rable",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un gain de temps consid√©rable",
    "text": "Un gain de temps consid√©rable\n\n\n\n\n\n\n\n\n\n\nCalcul de la dur√©e moyenne du premier contrat pour tous les individus MiDAS\nRetour √† l‚Äôemploi salari√© des indemnisables\n\n\n\n\nClassique R\n4 heures\nCrash\n\n\nDuckdb\n8 minutes\n3 heures seul sur la bulle\n\n\nSpark\n1 minute\n2 minutes\n\n\n\nMais alors, pourquoi le cluster ? ü§î",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#une-bonne-allocation-des-ressources-entre-utilisateurs",
    "href": "slides_v.html#une-bonne-allocation-des-ressources-entre-utilisateurs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une bonne allocation des ressources entre utilisateurs",
    "text": "Une bonne allocation des ressources entre utilisateurs",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#o√π-est-midas-2√®me-√©dition",
    "href": "slides_v.html#o√π-est-midas-2√®me-√©dition",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π est Midas, 2√®me √©dition",
    "text": "O√π est Midas, 2√®me √©dition\nLe cluster a son propre explorateur de fichiers √† mettre en favori dans son navigateur : https://midares-deb11-nn-01.midares.local:9870/\n\n\n\n\n\n\n\n\n\n\n\n\n\nSorte de disque du cluster",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#et-mes-bases-sur-la-bulle",
    "href": "slides_v.html#et-mes-bases-sur-la-bulle",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Et mes bases sur la bulle ?",
    "text": "Et mes bases sur la bulle ?\nIl est possible de charger des bases enregistr√©es n‚Äôimporte o√π sur la bulle sur HDFS : depuis vos documents personnels, depuis l‚Äôespace commun‚Ä¶",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#un-cluster-de-calcul",
    "href": "slides_v.html#un-cluster-de-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un cluster de calcul",
    "text": "Un cluster de calcul\n\n\nPlusieurs ordinateurs, noter le voc au tableau noeud = worker = ordinateur = datanode = executeur = instance Session dans la bulle donc charger grosses donn√©es en session = limit√© par la taille de la bulle",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#connexion",
    "href": "slides_v.html#connexion",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Connexion",
    "text": "Connexion\n\nTraitement normalTraitement tr√®s lourd\n\n\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(dbplyr)\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"20Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(dbplyr)\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"20Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 3\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\n\n\n\n\n\nTemps de connexion\n\n\nPour se connecter au cluster, il faut environ 5 minutes, √† chaque connexion. Spark cluster n‚Äôest pas du tout adapt√© √† des traitements l√©gers (moins de 10 minutes).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#quizz-traitement-nomal-ou-traitement-tr√®s-lourd",
    "href": "slides_v.html#quizz-traitement-nomal-ou-traitement-tr√®s-lourd",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quizz : traitement nomal ou traitement tr√®s lourd",
    "text": "Quizz : traitement nomal ou traitement tr√®s lourd\n\nAppariement de 2 bases mensuelles de la CNAF entre elles (4 millions de lignes par base)\n\n\n\nRep√©rage de la situation en emploi (MMO) d‚Äôun champ de b√©n√©ficiaires RSA un mois donn√© (2 millions de lignes)\n\n\n\n\nCalcul de la dur√©e d‚Äôinscription (FHS, table DE), de la dur√©e d‚Äôindemnisation (FNA, table PJC) et du retour √† l‚Äôemploi d‚Äôun champ de 2 millions de demandeurs d‚Äôemploi\n\n\n\n\nCalcul de la dur√©e d‚Äôindemnisation (FNA, table PJC) d‚Äôun champ de 20 millions de demandeurs d‚Äôemploi\n\n\n\n\nCalcul du retour √† l‚Äôemploi d‚Äôun champ de demandeur d‚Äôemploi en fin d‚Äôun mois donn√© (5 millions de DEFM)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#chargement-des-donn√©es-en-spark",
    "href": "slides_v.html#chargement-des-donn√©es-en-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Chargement des donn√©es en spark",
    "text": "Chargement des donn√©es en spark\n\n\n### Depuis HDFS\nmmo_17_df_spark &lt;- spark_read_parquet(sc,\n                                  path = \"hdfs:///dataset/MiDAS_v4/mmo/mmo_2017.parquet\",\n                                  memory = FALSE)\n\n### Passer un dataframe R en spark\nmon_data_frame &lt;- data.frame(c(\"Anna\", \"Paul\"), c(15, 20))\nmon_data_frame_spark &lt;- copy_to(sc, \"mon_data_frame\")\n\n‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : par d√©faut, memory = FALSE\n\n\n\n\n\n\nSpark data frames\n\n\nmmo_17_df_spark est un spark data frame (sdf) : il ne peut pas √™tre ouvert comme un data frame R classique, il n‚Äôest pas dans la session R.\n\n\n\n\nspark est lazy, paresseux, il ne fait rien tant qu‚Äôon ne le force pas",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#comment-voir-ma-table",
    "href": "slides_v.html#comment-voir-ma-table",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Comment voir ma table ?",
    "text": "Comment voir ma table ?\nR√©cup√©rer une partie de la table : pas plus de 500 lignes\n\nune_partie_de_ma_table &lt;- ma_table %&gt;% \n  head(100) %&gt;%\n  collect()\n\n\nune_partie_de_ma_table &lt;- ma_table %&gt;% \n  filter(id_midas %in% ma_liste_id_midas) %&gt;%\n  collect()\n\n# une_partie_de_ma_table est ici un data.frame R classique que vous pouvez ouvrir!\n\n\n\n\n\n\n\nSpark data frames\n\n\nune_partie_de_ma_table est un data frame R : il peut pas √™tre ouvert, il est dans la session R. Cela signifie qu‚Äôil se situe sur la bulle",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#un-cluster-de-calcul-1",
    "href": "slides_v.html#un-cluster-de-calcul-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un cluster de calcul",
    "text": "Un cluster de calcul",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#sparklyr-cest-comme-dplyr",
    "href": "slides_v.html#sparklyr-cest-comme-dplyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Sparklyr, c‚Äôest comme dplyr",
    "text": "Sparklyr, c‚Äôest comme dplyr\n\nEnsuite, vous pouvez programmer avec dplyr !\n\n\nmmo_17_df_spark &lt;- mmo_17_df_spark %&gt;%\n  \n  rename(debut_contrat = DebutCTT) %&gt;%\n  \n  filter(debut_contrat &gt;= as.Date(\"2017-01-01\") & debut_contrat &lt; as.Date(\"2017-02-01\")) %&gt;%\n  \n  mutate(mois_debut_contrat = substr(debut_contrat,6,7))",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-lazy-evaluation",
    "href": "slides_v.html#la-lazy-evaluation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark distingue deux types d‚Äôop√©rations :\n\nles transformations : prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul\nPar exemple, le programme ci-dessous ne d√©clenche pas d‚Äôex√©cution :\n\n\nmmo_17_df_spark_mois &lt;- mmo_17_df_spark %&gt;%\n  rename(debut_contrat = DebutCTT) %&gt;%\n  filter(debut_contrat &gt;= as.Date(\"2017-01-01\") & debut_contrat &lt; as.Date(\"2017-06-01\")) %&gt;%\n  mutate(mois_debut_contrat = substr(debut_contrat,6,7))\n\n\nles actions : forcent le calcul d‚Äôun r√©sultat pour le r√©cup√©rer et d√©clenchent l‚Äôex√©cution de toutes les transformations compil√©es jusqu‚Äô√† l‚Äôappel de l‚Äôaction.\nPar exemple, le programme ci-dessous d√©clenche le calcul de toute la cellule pr√©c√©dente :\n\n\nnb_debut_contrat_fev_17 &lt;- mmo_17_df_spark_mois %&gt;%\n  group_by(mois_debut_contrat) %&gt;%\n  summarise(nb_contrats = n()) %&gt;%\n  print()",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-lazy-evaluation-un-gain-de-temps-consid√©rable",
    "href": "slides_v.html#la-lazy-evaluation-un-gain-de-temps-consid√©rable",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation : un gain de temps consid√©rable",
    "text": "La lazy evaluation : un gain de temps consid√©rable\n\n\n\n\n\n\n\nLa gestion des erreurs\n\n\nEn r√©alit√©, lorsqu‚Äôon appuie sur le bouton run, il ne se passe pas ‚Äúrien‚Äù. Le code est compil√© par spark : les erreurs sont rep√©r√©es avant m√™me que le code soit ex√©cut√© !",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#r√©cup√©rer-un-r√©sultat",
    "href": "slides_v.html#r√©cup√©rer-un-r√©sultat",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "R√©cup√©rer un r√©sultat",
    "text": "R√©cup√©rer un r√©sultat\nLes principales actions sont :\n\nprint()\nhead() + collect()\n‚ö†Ô∏è collect() pour de petites tables : ne fonctionne pas sur des grosses tables\ntbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser)\n\n\n\n\n\n\n\nLe bouton stop\n\n\nIl est recommand√© de ne pas utiliser ce bouton en programmant en sparklyr : il rend la session spark inutilisable par la suite, il faut fermer RStudio et rouvrir ensuite.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#les-erreurs-spark",
    "href": "slides_v.html#les-erreurs-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les erreurs spark",
    "text": "Les erreurs spark\nLes erreurs de programmation sont soulev√©es avant que les calculs commencent.\nDes erreurs peuvent survenir pendant l‚Äôex√©cution du code, quelques minutes apr√®s l‚Äôappel d‚Äôune action par exemple.\n\n\n\n\n\n\nCollecter des donn√©es trop volumineuses\n\n\nUne source fr√©quente d‚Äôerreur pendant l‚Äôex√©cution est l‚Äôappel d‚Äôun collect() sur des donn√©es trop volumineuses pour √™tre collect√©es. La premi√®re √©tape du d√©buggage consiste √† limiter les collect().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes erreurs sparklyr\n\n\nLes erreurs envoy√©es par spark sont ‚Äútraduites‚Äù par sparklyr pour √™tre affich√©es dans la console de R. Elles ne sont pas toujours tr√®s lisibles, ou tr√®s pr√©cises sur la nature de l‚Äôerreur/sa source.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#presque-tout-comme-dplyr",
    "href": "slides_v.html#presque-tout-comme-dplyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "‚Ä¶ presque tout comme dplyr",
    "text": "‚Ä¶ presque tout comme dplyr\nLa majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences principales sont les suivantes :\n\n\n\n\n\n\n\n\nFonctionnalit√©\ntidyverse\nsparklyr\n\n\n\n\nimport d‚Äôun fichier .parquet\nread_parquet\nspark_read_parquet()\n\n\ntri d‚Äôun tableau\narrange()\nwindow_order() ou sdf_sort()\n\n\nop√©rations sur les dates\nlubridate\nfonctions Hive\n\n\nempiler des tableaux\nbind_rows()\nsdf_bind_rows()\n\n\nnombre de lignes d‚Äôun tableau\nnrow()\nsdf_nrow()\n\n\nfaire pivoter un tableau\ntidyr\nsdf_pivot()\n\n\nexport d‚Äôun spark_data_frame\n\nspark_write_parquet()",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#quelques-fonctions-r-pas-encore-traduites",
    "href": "slides_v.html#quelques-fonctions-r-pas-encore-traduites",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques fonctions R pas encore traduites",
    "text": "Quelques fonctions R pas encore traduites\n\nDatesTableauStatistiques\n\n\nLes fonctions de lubridate()ne sont pas adapt√©es au spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\n\n    date_1 &lt;- as.Date(\"2024-05-26\")\n\n\nCalculer une dur√©e entre deux dates\n\n\n    PJC_spark &lt;- spark_read_parquet(sc,\n                                    path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                    memory = FALSE)\n\n    duree_pjc_df &lt;- PJC_spark %&gt;%\n      rename(date_fin_pjc = as.Date(KDFPJ),\n             date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n      mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n      head(5)\n\n\nAjouter ou soustraire des jours ou des mois √† une date\n\n\n    duree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n      mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n             duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n             duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n      head(5)\n\n\n\n\n\n\n\nAdd_months\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\n\n\n\n\n\n\n\nFormat\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\nTri dans un groupe pour effectuer un calcul s√©quentiel\n\n\n    ODD_spark &lt;- spark_read_parquet(sc,\n                                    path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                    memory = FALSE)\n\n    ODD_premier &lt;- ODD_spark %&gt;%\n      group_by(id_midas) %&gt;%\n      window_order(id_midas, KDPOD) %&gt;%\n      mutate(date_premier_droit = first(KDPOD)) %&gt;%\n      ungroup() %&gt;%\n      distinct(id_midas, KROD3, date_premier_droit) %&gt;%\n      head(5)\n\n\nTri pour une sortie : sdf_sort() , arrange() ne fonctionne pas\nConcat√©ner les lignes (ou les colonnes sdf_bind_cols())\n\nODD_1 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &lt;= as.Date(\"2017-12-31\")) %&gt;%\n  mutate(groupe = \"temoins\")\n\nODD_2 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &gt;= as.Date(\"2021-12-31\")) %&gt;%\n  mutate(groupe = \"traites\")\n\nODD_evaluation &lt;- sdf_bind_rows(ODD_1, ODD_2)\n\nD√©doublonner une table\n\n\n    droits_dans_PJC &lt;- PJC_spark %&gt;%\n      sdf_distinct(id_midas, KROD3)\n\n    print(head(droits_dans_PJC, 5))\n\n    PJC_dedoublonnee &lt;- PJC_spark %&gt;%\n      sdf_drop_duplicates()\n\n    print(head(PJC_dedoublonnee, 5))\n\n\nPivot : les fonctions du packag tidyr ne fonctionnent pas sur donn√©es spark\n\n\n    ODD_sjr_moyen &lt;- ODD_spark %&gt;%\n      mutate(groupe = ifelse(KDPOD &lt;= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %&gt;%\n      sdf_pivot(groupe ~ KCRGC,\n        fun.aggregate = list(KQCSJP = \"mean\")\n      )\n\n\n\n\nR√©sum√© statistique : sdf_describe() , summary()ne fonctionne pas.\nDimension : sdf_dim, la fonction nrow()ne fonctionne pas.\nQuantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, sdf_quantile()\nEchantillonnage al√©atoire : sdf_random_split",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#je-veux-voir-ma-table",
    "href": "slides_v.html#je-veux-voir-ma-table",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Je veux voir ma table",
    "text": "Je veux voir ma table\n\nV√©rifier le nombre de lignes sans collecter\n\n\nma_table %&gt;% \n  sdf_nrow()\n\n\n\nV√©rifier la pr√©sence de doublons\n\n\nnb_doublons &lt;- ma_table %&gt;% \n  group_by(id_midas) %&gt;%\n  summarise(nb_ligne_ind = n()) %&gt;%\n  ungroup() %&gt;%\n  filter(nb_ligne_ind &gt; 1) %&gt;%\n  sdf_nrow()\n\n\n\n\nR√©cup√©rer une partie de la table : pas plus de 500 lignes\n\n\nune_partie_de_ma_table &lt;- ma_table %&gt;% \n  head(100) %&gt;%\n  collect()\n\n\nune_partie_de_ma_table &lt;- ma_table %&gt;% \n  filter(id_midas %in% ma_liste_id_midas) %&gt;%\n  collect()\n\n# une_partie_de_ma_table est ici un data.frame R classique que vous pouvez ouvrir!",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#exporter-des-donn√©es-sur-disque",
    "href": "slides_v.html#exporter-des-donn√©es-sur-disque",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Exporter des donn√©es sur disque",
    "text": "Exporter des donn√©es sur disque\nSur la pause d√©jeuner par exemple üòâ\nPourquoi ‚ùì Pour des donn√©es qui ne peuvent pas √™tre collect√©es en m√©moire vive\nExport des spark data frames directement sous HDFS : √† aucun moment on n‚Äôouvre la table : on peut traiter des donn√©es beaucoup plus volumnieuses que la m√©moire RAM !\n\nma_table_spark &lt;- MMO %&gt;%\n  \n  right_join(mon_champ_individuel, by = c(\"id_midas\")) %&gt;%\n  \n  mutate(fin_ctt_bis = ifelse(is.na(FinCTT), as.Date(\"2023-12-31\"), FinCTT)) %&gt;%\n  \n  mutate(duree_ctt = DATEDIFF(FinCTT, DebutCTT) + 1)\n\nspark_write_parquet(ma_table_spark, \"hdfs:///resultats/ma_table.parquet\")\n\nPossibilit√© de r√©cup√©rer ce fichier sur la bulle MiDARES = en local.\n\n\n\n\n\n\nExports simultan√©s\n\n\nHDFS supporte les exports simultan√©s, mais le temp d‚Äôexport est plus long lorsque le NameNode est requ√™t√© par plusieurs personnes simultan√©ment",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#si-on-souhaite-la-r√©cup√©rer-en-local",
    "href": "slides_v.html#si-on-souhaite-la-r√©cup√©rer-en-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Si on souhaite la r√©cup√©rer en local",
    "text": "Si on souhaite la r√©cup√©rer en local\n\n\n\n\n\n\nLes exports sur HDFS\n\n\nLorsqu‚Äôon exporte une table depuis notre session R vers HDFS, celle-ci est automatiquement partitionn√©e, comme le reste des donn√©es.\nAinsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\nIl est possible de ma√Ætriser le nombre de partitions avec la commande sdf_coalesce(partitions = 1) du package sparklyr.\nAvec sdf_coalesce(partitions = 1), on n‚Äôaura qu‚Äôun seul fichier √† t√©l√©charger depuis HDFS.\nAvec sdf_coalesce(partitions = 200), on aura 200 morceaux de notre fichier √† t√©l√©charger √† la main (pas possible de faire tout s√©lectionner sous HDFS !).\nL‚Äôid√©al est d‚Äôadapter le nombre de partitions √† la taille d‚Äôun bloc : un bloc mesure 128 MB.\n\n\n\n\nma_table &lt;- data.frame(c(\"Anne\", \"Paul\"), c(25,30))\n\nma_table_spark &lt;- copy_to(sc, ma_table) %&gt;%\n  sdf_coalesce(partitions = 1)\n\nspark_write_parquet(ma_table_spark, \"hdfs:///resultats/ma_table.parquet\")",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#fermer-sa-session",
    "href": "slides_v.html#fermer-sa-session",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Fermer sa session",
    "text": "Fermer sa session\nIl faut imp√©rativement fermer sa session spark apr√®s une session de travail. Deux moyens pour √ßa :\n\nfermer R Studio\nsi on ne ferme pas RStudio, utiliser la fonction spark_disconnect_all() dans son code\n\nSi on souhaite lancer un code le soir en partant, on n‚Äôoublie pas le spark_disconnect_all() √† la fin du code.\n\n\n\n\n\n\nPartage des ressources\n\n\nLes ressources r√©serv√©s par un utilisateur ne sont lib√©r√©es pour les autres que lorsqu‚Äôil se d√©connecte. Ne pas se d√©connecter, c‚Äôest bloquer les ressources. Si j‚Äôai r√©serv√© deux ordinateurs du cluster sur 15, personne d‚Äôautres ne peut les r√©server tant que je n‚Äôai pas d√©connecter ma session spark.\nNous fermerons les sessions ouvertes trop longtemps (d√©part de cong√©s sans d√©connexion) si des utilisateurs pr√©sents en ont besoin : risque de perte du travail non enregistr√©.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#t√©l√©charger-des-donn√©es-en-local",
    "href": "slides_v.html#t√©l√©charger-des-donn√©es-en-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "T√©l√©charger des donn√©es en local",
    "text": "T√©l√©charger des donn√©es en local",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#et-ensuite",
    "href": "slides_v.html#et-ensuite",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Et ensuite ?",
    "text": "Et ensuite ?\nSpark est un outil de traitement de donn√©es volumineuses. Il n‚Äôest pas toujours adapt√© :\n\npour de petites tables : il ne va pas engendrer de gain de temps, voire augmenter le temps\npour faire de l‚Äô√©conom√©trie pouss√©e : tous les packages R ne sont pas traduits en spark\npour ouvrir sa table : on perd les avantages de spark si on collecte toute la table en m√©moire RAM\n\nConseils :\n\nCr√©er sa table d‚Äô√©tude en appariant les tables de MiDAS avec le cluster spark\nL‚Äôexporter sous HDFS\nLa t√©l√©charger en local\nLa charger en R classique pour faire de l‚Äô√©conom√©trie",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#quizz-spark-ou-pas-spark",
    "href": "slides_v.html#quizz-spark-ou-pas-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quizz : spark ou pas spark ?",
    "text": "Quizz : spark ou pas spark ?\n\nfaire des statistiques descriptives sur une unique table de 1 million d‚Äôindividus et 30 variables d√©j√† cr√©√©e\n\n\n\ncr√©er une table de 5 millions demandeurs d‚Äôemploi avec leur situation au regard de l‚Äôemploi (MMO), leur dur√©e d‚Äôinscription (DE du FHS)\n\n\n\n\napparier 4 tables mensuelles de la CNAF pour rep√©rer la liste des id_midas b√©n√©ficiaires de minima sociaux 4 mois donn√©s (4 millions chaque mois)\n\n\n\n\nfaire de l‚Äô√©conom√©trie sur une unique table d√©j√† cr√©√©e\n\n\n\n\n\n\n\n\n\nEconom√©trie et Machine Learning avec Spark\n\n\nIl existe des outils pour faire de l‚Äô√©conom√©trie avec spark, la librairie Apache Spark MLlib. Elle rel√®ve d‚Äôune utilisation plus avanc√©e de spark que nous ne traitons pas ici. Elle ne contient pas autant de mod√®les que le CRAN R pour la recherche en √©conom√©trie.\nIl vous est conseill√© de cr√©er une unique table d‚Äô√©tude puis de la traiter en R classique pour l‚Äô√©conom√©trie.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#mode-local-sch√©ma",
    "href": "slides_v.html#mode-local-sch√©ma",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : sch√©ma",
    "text": "Mode local : sch√©ma",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#mode-local-inadapt√©-et-mauvaise-pratique",
    "href": "slides_v.html#mode-local-inadapt√©-et-mauvaise-pratique",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : inadapt√© et mauvaise pratique",
    "text": "Mode local : inadapt√© et mauvaise pratique\n\nSpark et le mode local :\n\nun seul ordinateur alors que spark est fait pour plusieurs ordinateurs distincts\nbeaucoup moins de ressources disponibles sur la bulle que sur le cluster\nmauvaise gestion de l‚Äôallocation des ressources entre utilisateurs : pas faite pour plusieurs utilisateurs\nralentissements consid√©rables et bugs : bloque les autres utilisateurs\n‚ñ∂Ô∏èspark n‚Äôest adapt√© que pour le cluster de calcul, la bulle pour faire du R sans spark sur des donn√©es peu volumineuses",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#inefficient-de-prendre-beaucoup-de-ressources",
    "href": "slides_v.html#inefficient-de-prendre-beaucoup-de-ressources",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Inefficient de prendre beaucoup de ressources",
    "text": "Inefficient de prendre beaucoup de ressources\nLes ordinateurs du cluster ont besoin de s‚Äôenvoyer des donn√©es par le r√©seau : c‚Äôest la partie la plus lente d‚Äôun programme spark !\nSi j‚Äôaugmente les ressources : par exemple, je r√©serve 3 ordinateurs du cluster plut√¥t que 2\n\nEffet puissance de calcul : plus de ressources pour faire les calculs = r√©duction du temps de calcul\nEffet augmentation des √©changes r√©seau (shuffles) : augmentation du temps de calcul\nG√™ne des autre utilisateurs",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#ne-pas-collecter",
    "href": "slides_v.html#ne-pas-collecter",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ne pas collecter",
    "text": "Ne pas collecter\n\n\n\n\n\n\nCollecter, c‚Äôest quoi ?\n\n\nCollecter c‚Äôest utiliser l‚Äôinstruction collect(). Elle permet de rapatrier l‚Äôensemble des r√©sultats du cluster vers la bulle et la session R de l‚Äôutilisateur en format R, par exemple des data.frames.\nCollect() :\n\nest une action : elle d√©clencher tous les calculs\nimplique des √©changes r√©seau tr√®s importants : entre ordinateurs du cluster et du cluster vers la bulle : c‚Äôest extr√™mement long, moins efficient que l‚Äôenregistrement sur disque directement depuis spark\nrappatrie les r√©sultats (une table) dans la m√©moire vive de R, qui est sur la bulle : si le r√©sultat est volumineux, cela bloque les autres utilisateurs\n\n\n\n\nRecommandations :\n\nNe pas collecter des tables de plus de 15 Go\nUtiliser les autres m√©thodes propos√©es pour ne pas bloquer les utilisateurs qui ont besoin de R en mode classique\nNe pas changer les configurations",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#fermer-sa-session-1",
    "href": "slides_v.html#fermer-sa-session-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Fermer sa session",
    "text": "Fermer sa session\n\n\n\nPour ne pas bloquer les coll√®gues üë®‚Äçüíª",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#yarn",
    "href": "slides_v.html#yarn",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Yarn",
    "text": "Yarn\nYarn permet de consulter la r√©servation des ressources par les utilisateurs.\nOn peut y acc√©der en copiant le lien suivant dans Google chrome sur la bulle (mettre en favori) : midares-deb11-nn-01.midares.local:8088/cluster\nV√©rifier que notre session est ferm√©e et qu‚Äôon ne prend pas trop de ressources : yarn",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#mutualiser-les-exp√©riences",
    "href": "slides_v.html#mutualiser-les-exp√©riences",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mutualiser les exp√©riences",
    "text": "Mutualiser les exp√©riences\n\nAide au passage d‚Äôun code sur le cluster\nProgrammer entre coll√®gues\nContributions √† la documentation MiDAS : section fiches, √† l‚Äôaide de pull requests sur github",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#comment-fonctionne-spark",
    "href": "slides_v.html#comment-fonctionne-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Comment fonctionne spark ?",
    "text": "Comment fonctionne spark ?\n\nApache Spark : librairie open source d√©velopp√©e dans le langage scala\n\nval TopHorrorsIGN2022 = Seq(\n  (9, \"Pearl\"),\n  (6, \"The Sadness\"),\n  (6, \"Offseason\"),\n  (7, \"Hatching\"),\n  (8, \"x\")\n).toDF(\"IMDB Rating\", \"IGN Movie Picks\")\n\nimport org.apache.spark.sql.functions.col\n\nval cols = List(col(\"IGN Movie Picks\"), col(\"AVC Movie Picks\"))\n\nval query = TopHorrorsIGN2022(\n  \"IGN Movie Picks\"\n) === TopHorrorsTheAVClub2022(\"AVC Movie Picks\")\n\nval outerJoin = TopHorrorsIGN2022\n  .join(TopHorrorsTheAVClub2022, query, \"outer\")\n  .select(cols: _*)\n\nouterJoin.show()\n\nscala adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de spark et optimiser au maximum les traitements en spark\nspark est compatible avec les langages scala, R, python, java, et peut interpr√©ter des commandes SQL.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#le-driver-en-sparklyr",
    "href": "slides_v.html#le-driver-en-sparklyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le driver en sparklyr",
    "text": "Le driver en sparklyr\n\n\nLe programme R est traduit en scala gr√¢ce au package sparklyr\nLe driver √©value le programme, il lit le code scala mais n‚Äôex√©cute rien du tout\nS‚Äôil remarque une erreur, l‚Äôerreur est envoy√©e directement √† l‚Äôutilisateur en session R avant l‚Äôex√©cution du programme : c‚Äôest la force de la lazy evaluation.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#pas-besoin-doptimiser-son-code",
    "href": "slides_v.html#pas-besoin-doptimiser-son-code",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pas besoin d‚Äôoptimiser son code !",
    "text": "Pas besoin d‚Äôoptimiser son code !\n\nsource : documentation CASD disponible √† Documentation Data Science",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#catalyst-optimise-le-code-pour-nous",
    "href": "slides_v.html#catalyst-optimise-le-code-pour-nous",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Catalyst optimise le code pour nous",
    "text": "Catalyst optimise le code pour nous\nLe driver contient un programme nomm√© Catalyst qui optimise le code scala automatiquement.\nSpark optimise automatiquement les programmes soumis :\n\nCompilation des transformations pour soulever les √©ventuelles erreurs\nInt√©gration dans un plan d‚Äôex√©cution contenant les √©tapes n√©cessaires pour parvenir au r√©sultat demand√© par le programme\nOptimisation du plan logique par le module Catalyst (driver Spark)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#catalyst-optimise-le-code-pour-nous-1",
    "href": "slides_v.html#catalyst-optimise-le-code-pour-nous-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Catalyst optimise le code pour nous",
    "text": "Catalyst optimise le code pour nous",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#catalyst-optimise-le-code-pour-nous-2",
    "href": "slides_v.html#catalyst-optimise-le-code-pour-nous-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Catalyst optimise le code pour nous",
    "text": "Catalyst optimise le code pour nous\nPar exemple si j‚Äô√©cris le programme :\n\nnon_optimal &lt;- table_1 %&gt;%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\"))\n\n\nCatalyst r√©√©crit :\n\n\noptimal &lt;- table_1 %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\")) %&gt;%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))\n\nCette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu‚Äôune action d√©clenche l‚Äôex√©cution.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#catalyst-optimise-le-code-pour-nous-laissons-le-travailler",
    "href": "slides_v.html#catalyst-optimise-le-code-pour-nous-laissons-le-travailler",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Catalyst optimise le code pour nous : laissons-le travailler !",
    "text": "Catalyst optimise le code pour nous : laissons-le travailler !\nD√©clencher le moins d‚Äôactions possibles dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.\nPour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique. On √©vite quoi ?\n\nOn √©vite :\n\nde mettre des collect()sur chaque table interm√©diaire\nde collect() une table enti√®re\nde print() √† chaque √©tape\n\n\n\nSinon Catalyst n‚Äôa pas assez de code pour optimiser !",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#catalyst-optimise-le-code-pour-nous-laissons-le-travailler-1",
    "href": "slides_v.html#catalyst-optimise-le-code-pour-nous-laissons-le-travailler-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Catalyst optimise le code pour nous : laissons-le travailler !",
    "text": "Catalyst optimise le code pour nous : laissons-le travailler !\n\nnon_optimal &lt;- table_1 %&gt;% \n    collect() %&gt;%\n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\"))\n\n\nversus\n\noptimal &lt;- table_1 %&gt;%\n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\")) %&gt;%\n    head(5) %&gt;% \n    collect()",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-longueur-du-plan-logique",
    "href": "slides_v.html#la-longueur-du-plan-logique",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La longueur du plan logique",
    "text": "La longueur du plan logique\nFournir un plan logique tr√®s long sans d√©clencher d‚Äôaction peut cr√©er une erreur en spark : spark ‚Äúrefuse‚Äù d‚Äôoptimiser un plan si long.\nLa bonne pratique consiste √† ‚Äúcacher‚Äù des r√©sultats interm√©diaires, pour d√©clencher l‚Äôex√©cution r√©guli√®rement et conserver les r√©sultats en m√©moire, tout en nettoyant la m√©moire des r√©sultat interm√©diaires pr√©c√©dents :\n\ntable_1 &lt;- mon_champ %&gt;%\n  left_join(table_2) %&gt;%\n  mutate(variable_1 = indicatrice_1 + indicatrice_2,\n         regroupement_variable_2 = case_when(variable_2 %in% c(1,2,3) ~ \"A\",\n                                             variable_2 %in% c(5,8,9) ~ \"B\",\n                                             TRUE ~ \"C\")) %&gt;%\n  left_join(table_3) %&gt;%\n  sdf_register(\"table_1\")\n\ntbl_cache(sc, \"table_1\")\n\ntable_4 &lt;- table_1 %&gt;%\n  left_join(table_5) %&gt;%\n  mutate(variable_y = ifelse(variable_x &gt; 50, 1, 0)) %&gt;%\n  sdf_register(\"table_4\")\n\ntbl_cache(sc, \"table_4\")\n\ntbl_uncache(sc, \"table_1\")",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#le-r√¥le-du-cluster-manager",
    "href": "slides_v.html#le-r√¥le-du-cluster-manager",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du cluster manager",
    "text": "Le r√¥le du cluster manager\n\nLe cluster manager distribue les traitements physiques aux ordinateurs du cluster :\n\nil conna√Æt le meilleur plan physique fourni par Catalyst ;\nil conna√Æt les ressources disponibles et occup√©es par toutes les machines du cluster ;\nil affecte les ressources disponibles √† la session spark.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#le-r√¥le-du-worker",
    "href": "slides_v.html#le-r√¥le-du-worker",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du worker",
    "text": "Le r√¥le du worker\n\nLe worker effectue le morceau de programme qu‚Äôon lui affecte :\n\nil ne conna√Æt que les t√¢ches qu‚Äôon lui a affect√©es ;\nil peut communiquer avec le driver en r√©seau pour renvoyer un r√©sultat ;\nil peut communiquer avec les autres workers en r√©seau pour partager des donn√©es ou des r√©sultats interm√©diaires : c‚Äôest un shuffle.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#calcul-distribu√©-et-r√©cup√©ration-des-r√©sultats",
    "href": "slides_v.html#calcul-distribu√©-et-r√©cup√©ration-des-r√©sultats",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√© et r√©cup√©ration des r√©sultats",
    "text": "Calcul distribu√© et r√©cup√©ration des r√©sultats\n\n\n\n\n\n\n\nLe r√©seau\n\n\n\nLes workers communiquent avec le driver de la bulle MiDARES en r√©seau\nLes workers communiquent entre eux en r√©seau pour s‚Äô√©changer des donn√©es\nLe r√©seau est un mode de communication lent\n\n\n\n\n\nMtn un peu de th√©orie pour comprendre le calcul distribu√© et mieux l‚Äôutiliser",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#traitement-map-distribu√©",
    "href": "slides_v.html#traitement-map-distribu√©",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Traitement MAP distribu√©",
    "text": "Traitement MAP distribu√©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#traitement-reduce-distribu√©",
    "href": "slides_v.html#traitement-reduce-distribu√©",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Traitement REDUCE distribu√©",
    "text": "Traitement REDUCE distribu√©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#le-stockage-distribu√©-avec-hdfs",
    "href": "slides_v.html#le-stockage-distribu√©-avec-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le stockage distribu√© avec HDFS",
    "text": "Le stockage distribu√© avec HDFS",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#calcul-distribu√©-calcul-vectoriel",
    "href": "slides_v.html#calcul-distribu√©-calcul-vectoriel",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√©, calcul vectoriel",
    "text": "Calcul distribu√©, calcul vectoriel\n\nLes op√©rations les plus co√ªteuses en spark sont :\n\nles op√©rations par groupe de lignes, qui impliquent des shuffles, ou √©changes de donn√©es entre workers via le r√©seau\nles op√©rations d‚Äô√©criture sur disque avec spark_write_parquet()\nles op√©rations üõënon vectoris√©esüõë, qui entra√Ænent des shuffles lourds et inutiles : boucles for, jointures volumineuses‚Ä¶\n\n\nLes donn√©es MiDAS sont structur√©es de mani√®re proche d‚Äôune base de donn√©es relationnelles : leur traitement n√©cessite des jointures. Une partie des donn√©es sont mensuelles : cette structure peut inciter √† programmer en boucle for sur le mois, ce qui est long et inefficient.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#calcul-distribu√©-calcul-vectoriel-boucles-for",
    "href": "slides_v.html#calcul-distribu√©-calcul-vectoriel-boucles-for",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√©, calcul vectoriel : boucles for",
    "text": "Calcul distribu√©, calcul vectoriel : boucles for\n\nCas d‚Äôusage : je veux rep√©rer si un groupe d‚Äôindividus est au RSA un mois, deux mois, trois mois etc. apr√®s la sortie de l‚Äôassurance-ch√¥mage\nJ‚Äôutilise :\n\nle FNA, dont j‚Äôextraie une table individu avec le mois de sortie de l‚Äôassurance-ch√¥mage, table sorties\nles tables mensuelles de la CNAF cnaf_prestations_mois_m",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#calcul-distribu√©-calcul-vectoriel-boucles-for-1",
    "href": "slides_v.html#calcul-distribu√©-calcul-vectoriel-boucles-for-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√©, calcul vectoriel : boucles for",
    "text": "Calcul distribu√©, calcul vectoriel : boucles for\n\nSolution 1Solution 2Solution 3\n\n\nLance 12 X tous les mois de sortie jobs spark, beaucoup de shuffles\n\nlibrary(dplyr)\nlibrary(sparklyr)\n\nres_all &lt;- NULL\n\nfor (mois in mois_sortie_vec) { # 1√®re boucle sur les mois de sorties\n  # Sous-ensemble des sortants de ce mois\n  sorties_mois &lt;- sorties %&gt;%\n    filter(mois_sortie == !!mois) %&gt;%\n    select(id, mois_sortie)\n\n  for (h in 1:12) { # 2√®me boucle sur les 12 mois d'horizon\n    \n    mois_cible &lt;- as.Date(mois) + months(h)\n    nom_tbl &lt;- paste0(\"cnaf_prestations_\", format(mois_cible, \"%Y_%m\"))\n    table_mois_cnaf &lt;- spark_read_parquet(paste0(chemin_table, nom_tbl))\n\n    perception_RSA_mois_h &lt;- sorties_mois %&gt;%\n      mutate(mois_h = sql(paste0(\"add_months(mois_sortie, \", h, \")\"))) %&gt;%\n      inner_join(table_mois_cnaf, by = c(\"id\" = \"id\", \"mois_h\" = \"mois\")) %&gt;%\n      mutate(perception_RSA = ifelse(RSAVERS == \"C\" & MTRSAVER &gt; 0, 1, 0)) %&gt;%\n      transmute(id, mois_sortie, h = !!h, perception_RSA)\n\n    # empiler et mettre en cache le r√©sultat\n    res_all &lt;- if (is.null(res_all)) perception_RSA_mois_h else sdf_bind_rows(res_all, perception_RSA_mois_h) %&gt;% sdf_register(paste0(\"temp\", mois, h))\n  tbl_cache(sc, paste0(\"temp\", mois, h))\n  }\n}\n\n\n\nLance 12 jobs et une action √† chaque tour\n\nfor (mois in liste_mois) {\n  \n  nom_tbl &lt;- paste0(\"cnaf_prestations_\", format(mois, \"%Y_%m\"))\n  table_mois_cnaf &lt;- spark_read_parquet(paste0(chemin_table, nom_tbl))\n\n  # Jointure globale, calcul de l'horizon h \n  perception_RSA_mois &lt;- table_mois_cnaf %&gt;%\n    inner_join(sorties, by = \"id\") %&gt;%\n    mutate(h = sql(\"cast(months_between(mois, mois_sortie) as int)\")) %&gt;%\n    filter(h &gt;= 1, h &lt;= 12) %&gt;%\n    select(id, mois_sortie, h, prest)\n\n  # Cache pour enregistrer le r√©sultat en m√©moire\n  perception_RSA_mois_cache &lt;- perception_RSA_mois %&gt;% sdf_register(paste0(\"temp\", mois))\n  tbl_cache(sc, paste0(\"temp\", mois))\n\n  res_stack &lt;- if (is.null(res_stack)) joined_cached else sdf_bind_rows(res_stack, joined_cached)\n}\n\n\n\nMeilleure solution : un seul plan logique que Catalyst peut enti√®rement optimsier, en limitant les shuffles, beaucoup plus rapide\n\n# Charger la premi√®re table CNAF\nnom_1 &lt;- paste0(\"cnaf_prestations_\", format(as.Date(\"2023-01-01\"), \"%Y_%m\"))\ncnaf_total &lt;- spark_read_parquet(paste0(chemin_table, nom_1))\n\n# Empiler les autres\nfor (mois in liste_mois) {\n  \n  nom_tbl &lt;- paste0(\"cnaf_prestations_\", format(mois, \"%Y_%m\"))\n  table_mois_cnaf &lt;- spark_read_parquet(paste0(chemin_table, nom_tbl))\n  \n  cnaf_total &lt;- sdf_bind_rows(cnaf_total, table_mois_cnaf) %&gt;%\n    sdf_register(\"cnaf_total\")\n  tbl_cache(sc, \"cnaf_total\")\n}\n\n\n# Joindre UNE fois avec la table des sorties (broadcast si petit)\nsorties_b &lt;- sdf_broadcast(sorties %&gt;%\n  select(id, mois_sortie)\n)\n\nperception_RSA_horizon &lt;- cnaf_total %&gt;%\n  inner_join(sorties_b, by = \"id\") %&gt;%\n  mutate(\n    h = sql(\"cast(months_between(mois, mois_sortie) as int)\")\n  ) %&gt;%\n  filter(h &gt;= 1, h &lt;= 12) %&gt;%\n  select(id, mois_sortie, h, prest)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#calcul-distribu√©-calcul-vectoriel-jointures",
    "href": "slides_v.html#calcul-distribu√©-calcul-vectoriel-jointures",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√©, calcul vectoriel : jointures",
    "text": "Calcul distribu√©, calcul vectoriel : jointures\nUne jointure implique pour spark de rappatrier les lignes avec les m√™mes valeurs de clef sur le m√™me worker : les jointures engendrent des shuffles.\n\nLorsque l‚Äôon joint une grosse table avec une petite table, Spark peut optimiser le calcul en utilisant un broadcast join : la petite table est diffus√©e en entier sur chaque worker, ce qui √©vite un shuffle massif.\n\nres &lt;- grosse_table %&gt;%\n  inner_join(sdf_broadcast(petite_table), by = \"clef\")\n\n\nüëâ Ici petite_table est diffus√©e (broadcast) sur tous les workers : chaque partition de grosse_table fait alors le join localement, sans transfert r√©seau co√ªteux.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#calcul-distribu√©-calcul-vectoriel-jointures-1",
    "href": "slides_v.html#calcul-distribu√©-calcul-vectoriel-jointures-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√©, calcul vectoriel : jointures",
    "text": "Calcul distribu√©, calcul vectoriel : jointures\nBroadcast automatique : par d√©faut, Spark choisit automatiquement le broadcast join si la table √† diffuser fait moins de 10 MB (param√®tre configurable).\nAu-del√†, il utilise un shuffle join classique (plus lent).\nAstuce right_join : dans sparklyr, la position de la table peut influencer le plan choisi par Spark :\npetite_table %&gt;% left_join(grosse_table) ‚Üí Spark n‚Äôessaie pas forc√©ment de diffuser la petite.\ngrosse_table %&gt;% right_join(petite_table) ‚Üí Spark choisit plus volontiers un broadcast join.\nüëâ Bonne pratique : forcer avec sdf_broadcast() quand on sait que la table est petite, plut√¥t que de compter sur ce comportement implicite.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#calcule-distribu√©-calcul-vectoriel",
    "href": "slides_v.html#calcule-distribu√©-calcul-vectoriel",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcule distribu√©, calcul vectoriel",
    "text": "Calcule distribu√©, calcul vectoriel\n\n\n\n\n\n\n\n\nOp√©ration logique\nSolution non vectorielle\nSolution vectorielle\n\n\n\n\nsituation √† m + 1, 2‚Ä¶\nBoucle for sur les mois\nutilisation de lag() et lead(), ou auto jointure sur les dates\n\n\nR√©p√©ter une fonction\nuser defined functions (UDF)\n√©viter les UDF\n\n\nop√©ration groupe de lignes\ngroup_by %&gt;% mutate %&gt;% distinct\ngroup_by %&gt;% summarise\n\n\njoindre petite table avec grosse table\npetite_table %&gt;% left_join(grosse_table)\nbroadcast join\n\n\njoindre deux grosses tables\nboucle for en divisant les tables en petits morceaux\nunique jointure et partition par la clef de jointure\n\n\nconstruire un panel cylindr√©\nboucle for\ncross_join() de deux tables",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-m√©moire-du-driver",
    "href": "slides_v.html#la-m√©moire-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La m√©moire du driver",
    "text": "La m√©moire du driver",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#lutilisation-de-la-m√©moire-du-driver",
    "href": "slides_v.html#lutilisation-de-la-m√©moire-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver\nCette configuration permet de collecter des statistiques descriptives et de petites tables sans g√™ner les autres utilisateurs.\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"20Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\n\n\nBonne pratique de partage des ressources\n\n\nLe driver est dans la bulle Midares, qui a vocation √† √™tre r√©duite suite √† la g√©n√©ralisation du cluster.\n\nLa bulle Midares a besoin de RAM pour fonctionner, 100% des ressources ne sont donc pas disponibles pour sparklyr.\nPour permettre le travail simultan√© fluide de 10 utilisateurs, la m√©moire allou√©e au driver recommand√©e pour chaque utilisateur est de maximum 20 Go.\nIl existe des alternatives pour ne pas collecter des r√©sultats trop volumineux dans le driver.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#programmer-sans-collecter",
    "href": "slides_v.html#programmer-sans-collecter",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer sans collecter",
    "text": "Programmer sans collecter\nLa programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu‚Äô√† la fin pour que Catalyst optimise l‚Äôensemble du programme\nLa principale diff√©rence avec la programmation en R classique est que la visualisation de tables compl√®tes volumineuses n‚Äôest pas toujours possible et n‚Äôest pas recommand√©e :\n\ngoulets d‚Äô√©tranglement m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R : erreurs Out of Memory\nlongue : √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R en r√©seau = lent ;\nbeaucoup moins efficace que l‚Äôexport direct en parquet du r√©sultat (qui fonctionne toujours) : charger ensuite sa table finale en data frame R classique pour effectuer l‚Äô√©tude.\n\nS‚Äôil est n√©cessaire de collecter, il faut pr√©voir beaucoup de RAM pour le driver avec le param√®tre spark.driver.memory, ce qui emp√™che les autres utilisateurs de travailler.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#programmer-sans-collecter-1",
    "href": "slides_v.html#programmer-sans-collecter-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer sans collecter",
    "text": "Programmer sans collecter\nLes r√©sultats qu‚Äôil est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :\n\nune table filtr√©e avec les variables n√©cessaires √† l‚Äô√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;\ndes statistiques descriptives synth√©tiques ;\nles premi√®res lignes de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;\nune table agr√©g√©e pour un graphique par exemple, √† l‚Äôaide de la fonction summarise().",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#programmer-sans-collecter-2",
    "href": "slides_v.html#programmer-sans-collecter-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer sans collecter",
    "text": "Programmer sans collecter\nJe sais que la cr√©ation de ma table donne le r√©sultat souhait√©e (car j‚Äôai regard√© ce dont elle a l‚Äôair avvec head()), maintenant je vais l‚Äôappeler une dizaine de fois pour collecter uniquement des statistiques descriptives.\nQue se passe-t-il √† chaque fois que je collecte une statistique descriptive ?\n\nLa cr√©ation de la table va √™tre ex√©cut√©e √† nouveau : tr√®s long ?\nComment faire ?\n\nCache\n\n\nLa cr√©ation de la table est ex√©cut√©e une seule fois, le r√©sultat est conserv√© en m√©moire vive\n\nma_table_spark &lt;- MMO_2017 %&gt;%\n  filter(DebutCTT &gt; as.Date(\"2017-06-01\")) %&gt;%\n  mutate(duree_CTT = DATEDIFF(FinCTT,DebutCTT) + 1) %&gt;%\n  sdf_register(name = \"ma_table_spark\")\n\ntbl_cache(\"ma_table_spark\")\n\n\n\n\n\nR√©ponse attendue : c‚Äôest une action donc ca d√©clenche la cr√©ation de la table Indice : cr√©ation de table contient uniquement des transformations",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#optimiser-la-m√©moire-conclusion",
    "href": "slides_v.html#optimiser-la-m√©moire-conclusion",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Optimiser la m√©moire : conclusion",
    "text": "Optimiser la m√©moire : conclusion\nPour programmer en spark sans aucune erreur :\n\nD√©clencher une action avec plusieurs transformations pour laisser Catalyst optimiser\nNe pas collecter tout une table\nPersister ou cacher une table qu‚Äôon va appeler plusieurs fois pour ne collecter que des statistiques descriptives\nNe pas persister trop de tables : occupe de la m√©moire RAM\nConsulter le programme exemple sur la bulle CASD si besoin",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#partitionnement",
    "href": "slides_v.html#partitionnement",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Partitionnement",
    "text": "Partitionnement\nLe format .parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.\nLe partitionnement a un impact sur la mani√®re dont les donn√©es sont organis√©es physiquement sur le syst√®me de fichiers.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#partitionnement-1",
    "href": "slides_v.html#partitionnement-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Partitionnement",
    "text": "Partitionnement\n\n\n\n\n\n\n\n\n\n\nPartitions\n2\n5\n1000\n\n\n\n\nColonne qui a servi au partitionnement\n74,50%\n46,30%\n16,66%\n\n\nVers une autre colonne\n89,51%\n191,01%\n556,99%\n\n\nSelect distinct(*)\n136,79%\n163,68%\n1194,88%\n\n\n\n\n\nspark_write_parquet(ma_table, \"hdfs:///resultats/ma_table.parquet\", partition_by = c(\"age\",\"sex\"))",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#√©viter-le-probl√®me-des-ex√©cuteurs-inactifs",
    "href": "slides_v.html#√©viter-le-probl√®me-des-ex√©cuteurs-inactifs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "√âviter le probl√®me des ex√©cuteurs inactifs",
    "text": "√âviter le probl√®me des ex√©cuteurs inactifs\nSupposons que le jeu de donn√©es ait 8 partitions, un ex√©cuteur (avec seulement 1 core) ne peut ex√©cuter qu‚Äôune t√¢che(task) √† la fois, et une partition = une t√¢che.\nCas 1 : 6 ex√©cuteurs, au dernier tour, il ne reste que 2 t√¢ches, 4 ex√©cuteurs seront inactifs. Cas 2 : 4 ex√©cuteurs, 2*4, aucun ex√©cuteur inactif.\n\n# Get the number of partitions\nnum_partitions &lt;- sdf_num_partitions(df)\nprint(num_partitions)\n\n# Repartition the DataFrame to a specific number of partitions\ndf_repartitioned &lt;- sdf_repartition(df, partitions = 10)\n\n\nLe nombre de partitions doit √™tre divisible par le nombre d‚Äôex√©cuteurs.\nLe nombre de partitions doit √™tre sup√©rieur au nombre d‚Äôex√©cuteurs.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#√©viter-trop-de-partitions",
    "href": "slides_v.html#√©viter-trop-de-partitions",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "√âviter trop de partitions",
    "text": "√âviter trop de partitions\nLa cr√©ation de t√¢ches entra√Æne des surcharges, qui doivent toujours √™tre inf√©rieures √† 50 % du temps total d‚Äôex√©cution de la t√¢che.\n\n\n\n\n# Repartition the DataFrame to a specific number of partitions\ndf_repartitioned &lt;- sdf_repartition(df, partitions = 10)\n\n# Repartition the DataFrame by a specific column, e.g., \"commune_code\".\n# The partition number will be the distinct value number\ndf_repartitioned &lt;- sdf_repartition(df, partition_by = \"commune_code\")\n\n\nLa r√©partition est une op√©ration tr√®s co√ªteuse, utilisez-la judicieusement.\nEn g√©n√©ral, la taille recommand√©e des partitions est d‚Äôenviron 128 √† 512 Mo.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#optimiser-la-configuration-des-ex√©cuteurs",
    "href": "slides_v.html#optimiser-la-configuration-des-ex√©cuteurs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Optimiser la configuration des ex√©cuteurs",
    "text": "Optimiser la configuration des ex√©cuteurs\nConfiguration recommand√©e :\n\nUn ex√©cuteur devrait avoir entre 3 et 5 cores.\nPour chaque core, il faut r√©server entre 4 et 8 Go de m√©moire.\n\nEn mode cluster, chaque ex√©cuteur fonctionne dans une JVM (la JVM n√©cessite une m√©moire suppl√©mentaire et du CPU pour ex√©cuter le GC).\n\n√âvitez 1 core par ex√©cuteur.\n√âvitez trop de cores dans un seul ex√©cuteur, cela peut causer des probl√®mes de contention de threads ou la surcharge du garbage collector.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#t√¢ches-maximales-en-parall√®les",
    "href": "slides_v.html#t√¢ches-maximales-en-parall√®les",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "T√¢ches Maximales en parall√®les",
    "text": "T√¢ches Maximales en parall√®les\n\n\n\n\n\n\n\nParall√©lisation\n\n\nMax_Parallel_Tasks = Number_of_Executors * Cores_per_Executor\n\n\n\n\nPar exemple, une session Spark dispose de la configuration suivante :\n\nconf[\"spark.executor.memory\"] &lt;- \"32Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 5\n\n5 executor * 4 core = 20 t√¢ches en parall√®le. Pour un jeu de donn√©es de 200 partitions, il faut 10 tours pour terminer tous les calculs.\n\n‚ñ∂Ô∏èIl n‚Äôexiste pas de configuration universelle optimale pour tous, seulement la meilleure configuration pour vos t√¢ches.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#sparkui",
    "href": "slides_v.html#sparkui",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "SparkUI",
    "text": "SparkUI\nSpark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d‚Äôoptimiser les traitements :\n\nDAGGCM√©moire\n\n\n\n\n\nV√©rifier que le gc time est inf√©rieur √† 10% du temps pour ex√©cuter la t√¢che ‚úÖ\n\n\n\nV√©rifier que la storage memory ne sature pas la m√©moire ‚úÖ",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#sparkhistory",
    "href": "slides_v.html#sparkhistory",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Sparkhistory",
    "text": "Sparkhistory\n\nSparkhistory pour des traitements de sessions ferm√©es\n\nLe sparkhistory entra√Æne l‚Äôenregistrement de logs assez lourdes, il est donc d√©sactiv√© par d√©faut. Pour l‚Äôactiver sur un programme :\n\nconf &lt;- spark_config()\nconf[\"spark.eventLog.enabled\"] &lt;- \"true\"\nconf[\"spark.eventLog.dir\"] &lt;- \"hdfs://midares-deb11-nn-01.midares.local:9000/spark-logs\"\nconf[\"appName\"] &lt;- \"un_nom_de_traitement\"\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#pyspark",
    "href": "slides_v.html#pyspark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pyspark",
    "text": "Pyspark",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#le-stockage-distribu√©-avec-hdfs-1",
    "href": "slides_v.html#le-stockage-distribu√©-avec-hdfs-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le stockage distribu√© avec HDFS",
    "text": "Le stockage distribu√© avec HDFS\nHadoop Distributed File System (HDFS)\n\nstockage sur diff√©rentes machines : les diff√©rents ordinateurs workers du cluster\ndonn√©es divis√©es en blocs plus petits de taille fixe et r√©partis sur les machines : aucune table de MiDAS n‚Äôexiste en entier sur le cluster\nchaque bloc est r√©pliqu√© trois fois : il existe trois fois les 10 premi√®res lignes de la table FNA sur trois ordinateurs diff√©rents du cluster (r√©silience)\nun NameNode supervise les m√©tadonn√©es et g√®re la structure du syst√®me de fichiers : il sait o√π sont quels fichiers\nles DataNodes stockent effectivement les blocs de donn√©es : les datanodes sont en fait les disques durs des workers du cluster, chaque ordinateur du cluster dispose d‚Äôun disque avec une partie des donn√©es MiDAS\nle syst√®me HDFS est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local",
    "crumbs": [
      "Slides"
    ]
  }
]