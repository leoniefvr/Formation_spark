[
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "Sur le cluster\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\nLes ressources physiques utilis√©es :\n\npar le driver : avec spark.driver.memory\npar chaque worker avec spark.executor.memory et spark.executor.cores\nle nombre de worker avec spark.executor.instances\nLa file sur laquelle on travaille avec spark.yarn.queue\n\nle nombre de partitions de chaque spark_data_frame avec spark.sql.shuffle.partitions\nla limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec spark.driver.maxResultSize\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\nLes files du cluster spark Midares",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html",
    "href": "bonnes_pratiques.html",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "D√©connexion ou fermeture R pour lib√©rer les ressources üõë\nNe plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nPyspark ou Sparklyr pour la production ‚ùì\nUtilisation parcimonieuse des ressources ‚öñÔ∏è\nEnvoi des erreurs sparklyr : d√©p√¥t tel quel d‚Äôun code sparklyr qui g√©n√®re une erreur dans le dossier de l‚Äôespace commun (documents publics) erreurs_sparklyr üì©",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "slides.html#au-programme",
    "href": "slides.html#au-programme",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Au programme",
    "text": "Au programme\n\nMiDAS : une base de donn√©es volumineuse üíæ\nManipuler un appariement : une op√©ration co√ªteuse üí≤\nInitiation au calcul distribu√© : quelles ressources r√©server ? üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nSparklyr : la solution ergonomique de spark sous R üë®‚Äçüíª\nPour aller plus loin ‚è©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS croise trois bases de donn√©es administratives exhaustives :\n\nles donn√©es sur l‚Äôinscription et l‚Äôindemnisation des demandeurs d‚Äôemploi de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;\nles donn√©es sur les b√©n√©ficiaires de minima sociaux (RSA, PPA, AAH) et les caract√©ristiques des m√©nages de la CNAF : Allstat-FR6 ;\nles donn√©es sur les contrats salari√©s de la DSN : MMO de la Dares.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nChaque vague de MiDAS correspond √† environ 600 Go de donn√©es au format sas. Les vagues fonctionnent par empilement :\n\nle gain de profondeur temporelle et l‚Äôentr√©e dans le champ de nouvelles personnes\nles vagues sont appariables entre elles",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS est l‚Äôune des bases de donn√©es les plus volumineuses du SSP :\nLes administrations dont les donn√©es sont comparables √† MiDAS utilisent un cluster Spark : Insee, Drees, Acoss‚Ä¶\n‚ñ∂Ô∏èLe cluster spark est la solution la plus efficiente pour traiter des donn√©es de cette ampleur. Apprendre √† l‚Äôutiliser pourra vous √™tre utile dans d‚Äôautres contextes que celui de la Dares.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#structure-de-lappariement",
    "href": "slides.html#structure-de-lappariement",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Structure de l‚Äôappariement",
    "text": "Structure de l‚Äôappariement\n\n\n\n\n\n\n\nPourquoi Spark ?\n\n\nLa manipulation des donn√©es MiDAS en l‚Äô√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-format-parquet",
    "href": "slides.html#le-format-parquet",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le format parquet",
    "text": "Le format parquet\nLes donn√©es sont converties au format parquet d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l‚Äôespace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :\n\nil compresse efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv\nil est orient√© colonnes\nil permet le chargement efficace en m√©moire des donn√©es\nIl permet le stockage partitionn√© des donn√©es\nil permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires sur disque\nIl est ind√©pendant du logiciel utilis√© : il peut donc √™tre trait√© par spark et par R.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#lespace-midares",
    "href": "slides.html#lespace-midares",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôespace MiDares",
    "text": "L‚Äôespace MiDares\n\nRessourcesSch√©ma\n\n\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n512 Go de m√©moire vive (ou RAM) : passage √† 256 Go\n\n\n\n\n\n\n\nLa m√©moire vive\n\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa volatilit√© (toutes les donn√©es sont perdues si l‚Äôordinateur n‚Äôest plus aliment√©) et par l‚Äôacc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.\n\n\n\n\nUn processeur (ou CPU) compos√© de 32 coeurs : passage √† 16 coeurs\n\n\n\n\n\n\n\nLe processeur\n\n\nLe processeur permet d‚Äôex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel‚Ä¶ Il est compos√© d‚Äôun ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu‚Äôune seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu‚Äôil a de coeurs. Un processeur se caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d‚Äôop√©rations qu‚Äôil est capable d‚Äôeffetuer par seconde.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#programmer-en-m√©moire-vive",
    "href": "slides.html#programmer-en-m√©moire-vive",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer en m√©moire vive",
    "text": "Programmer en m√©moire vive\n\nR : la m√©moire vive, √©tat dans l‚Äôenvironnement\nSAS : lecture/√©criture sur le disque\nMiDAS au format sas &gt;&gt; taille de la m√©moire vive disponible du serveur CASD ‚Äì&gt; format .parquet\nImpossible de charger tout MiDAS en m√©moire vive\nDes solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :\narrow (avec des requ√™tes dplyr)\nduckDB : recommand√© par le SSPLab pour des donn√©es jusqu‚Äô√† 100Go\n‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "href": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les traitements co√ªteux en puissance de calcul",
    "text": "Les traitements co√ªteux en puissance de calcul\n\nles jointures\nles op√©rations en group_by()\ndistinct()\n‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n‚ñ∂Ô∏è Erreur ‚Äúout of memory‚Äù.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#un-traitement-peu-co√ªteux",
    "href": "slides.html#un-traitement-peu-co√ªteux",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement peu co√ªteux",
    "text": "Un traitement peu co√ªteux\n\nCe traitement est peu co√ªteux :\n\nchargement d‚Äôune seule colonne en RAM : format parquet orient√© colonnes\npeu de m√©moire d‚Äôex√©cution : R est un langage vectoris√©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#un-traitement-co√ªteux",
    "href": "slides.html#un-traitement-co√ªteux",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement co√ªteux",
    "text": "Un traitement co√ªteux\n\nCe traitement n√©cessite :\n\nle chargement de davantage de colonnes en m√©moire vive ;\ndavantage de m√©moire d‚Äôex√©cution pour effectuer l‚Äôintersection (inner_join()).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "href": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√© et calcul parall√®le",
    "text": "Calcul distribu√© et calcul parall√®le\n\nCalcul non distribu√©Calcul distribu√© avec spark\n\n\nLorsqu‚Äôun traitement Big Data est demand√© par l‚Äôutilisateur dans la session R, plusieurs probl√®mes peuvent se poser :\n\nla taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R\nle temps de calcul : si plusieurs √©tapes sont n√©cessaires pour un traitement, elles sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)\nl‚Äôoptimisation du programme\n\n\n\nLe calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :\n\nchargement des donn√©es en m√©moire parcimonieux et non syst√©matique\nex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents\noptimisation automatique du code",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-cluster-de-calcul-midares-mode-interactif",
    "href": "slides.html#le-cluster-de-calcul-midares-mode-interactif",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le cluster de calcul Midares : mode interactif",
    "text": "Le cluster de calcul Midares : mode interactif",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#spark",
    "href": "slides.html#spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Spark",
    "text": "Spark\n\nApache Spark : librairie open source d√©velopp√©e dans le langage scala\nScala : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java\n\nval TopHorrorsIGN2022 = Seq(\n  (9, \"Pearl\"),\n  (6, \"The Sadness\"),\n  (6, \"Offseason\"),\n  (7, \"Hatching\"),\n  (8, \"x\")\n).toDF(\"IMDB Rating\", \"IGN Movie Picks\")\n\nval TopHorrorsTheAVClub2022 = Seq(\n  (7, \"Nope\"),\n  (9, \"Pearl\"),\n  (8, \"x\"),\n  (5, \"Barbarian\"),\n  (5, \"Bones And All\")\n).toDF(\"IMDB Rating\", \"AVC Movie Picks\")\n\nimport org.apache.spark.sql.functions.col\n\nval cols = List(col(\"IGN Movie Picks\"), col(\"AVC Movie Picks\"))\n\nval query = TopHorrorsIGN2022(\n  \"IGN Movie Picks\"\n) === TopHorrorsTheAVClub2022(\"AVC Movie Picks\")\n\nval outerJoin = TopHorrorsIGN2022\n  .join(TopHorrorsTheAVClub2022, query, \"outer\")\n  .select(cols: _*)\n\nouterJoin.show()\n\nscala adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de spark et optimiser au maximum les traitements en spark\nspark est compatible avec les langages scala, R, python, java, et peut interpr√©ter des commandes SQL.\nDeux packages existent sous R :\n\nsparkR propos√© par Apache Spark\nsparklyr, qui permet d‚Äôutiliser directement des commandes dplyr traduites en spark par le package.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#mode-local-concurrence",
    "href": "slides.html#mode-local-concurrence",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : concurrence",
    "text": "Mode local : concurrence",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#mode-local-concurrence-1",
    "href": "slides.html#mode-local-concurrence-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : concurrence",
    "text": "Mode local : concurrence\nEn mode local :\n\nune unique machine Java\nparall√©lisation des t√¢ches sur diff√©rents coeurs de cette machine virtuelle\npas de stockage distribu√©, ca n‚Äôest pas du calcul distribu√© √† proprement parler\nacc√©l√©ration par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources\nSur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local\n‚ñ∂Ô∏èmode local √† √©viter absolument",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#mode-cluster-non-concurrence",
    "href": "slides.html#mode-cluster-non-concurrence",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode cluster : non concurrence",
    "text": "Mode cluster : non concurrence\n\nLe mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d‚Äôun serveur. Ces machines communiquent en r√©seau.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#installation-de-spark-sous-casd",
    "href": "slides.html#installation-de-spark-sous-casd",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Installation de spark sous CASD",
    "text": "Installation de spark sous CASD\nVoir la fiche d√©di√©e sur le site",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#sparklyr-et-sparkr",
    "href": "slides.html#sparklyr-et-sparkr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Sparklyr et SparkR",
    "text": "Sparklyr et SparkR\nDeux packages permettent de programmer avec Spark sous R :\n\nSparkR : ce package, maintenu par Apache Spark, permet d‚Äôutiliser une syntaxe proche de spark, scala, ou directement du code SQL pour manipuler des donn√©es dans une session R.\nSparklyr : ce package permet d‚Äôutiliser directement la syntaxe dplyr dans une session Spark sous R.\n\nSparklyr fonctionne selon ces √©tapes :\n\nLa JVM driver spark est instanci√©e dans la bulle Midares pour utiliser sparklyr.\nLes instructions dplyr appel√©es sur un spark_data_frame sont traduites par les fonctions du package sparklyr en scala, puis envoy√©es au driver.\nLe programme en scala est ex√©cut√© sur le cluster.\nSi une erreur est renvoy√©e par le driver, elle est interpr√©t√©e par R avant d‚Äô√™tre affich√©e en session R.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#sparklyr-mode-cluster",
    "href": "slides.html#sparklyr-mode-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Sparklyr mode cluster",
    "text": "Sparklyr mode cluster",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#configuration-cluster",
    "href": "slides.html#configuration-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Configuration cluster",
    "text": "Configuration cluster\nDeux √©tapes majeures dans le traitement de donn√©es sous R diff√®re en sparklyr par rapport √† une programmation classique en dplyr :\n\nConfigurationImport-export\n\n\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\nLes ressources physiques utilis√©es :\n\npar le driver : avec spark.driver.memory (avec parcimonie)\npar chaque worker avec spark.executor.memory(valeur max 140 Go) et spark.executor.cores (valeur max 8 coeurs)\nle nombre de workers avec spark.executor.instances (2 ou 3 suffisent)\nLa file sur laquelle on travaille avec spark.yarn.queue (prod ou dev)\n\nle nombre de partitions de chaque spark_data_frame avec spark.sql.shuffle.partitions (200 par d√©faut)\nla limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec spark.driver.maxResultSize (0 est la meilleure option)\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\nLes donn√©es doivent √™tre disponibles dans les workers sous forme de spark_data_frame :\n\ncach√© en m√©moire directement : si utilis√©es plusieurs fois pour gagner du temps\nlaiss√© sur disque tant qu‚Äôaucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire\n‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.\nPour passer un data.frame R en spark_data_frame : copy_to()",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#configuration-des-ressources-cluster",
    "href": "slides.html#configuration-des-ressources-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Configuration des ressources cluster",
    "text": "Configuration des ressources cluster",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "href": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire dans un worker",
    "text": "L‚Äôutilisation de la m√©moire dans un worker\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nNe pas charger plusieurs fois les m√™mes donn√©es en cache, ou si besoin augmenter la part de la m√©moire allou√©e au stockage avec spark.memory.storageFraction.\n\n\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.memory.fraction\"] &lt;- 0.8\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.memory.fraction\"] &lt;- 0.8\nconf[\"spark.memory.storageFraction\"] &lt;- 0.4\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-stockage-distribu√©-hdfs",
    "href": "slides.html#le-stockage-distribu√©-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le stockage distribu√© : HDFS",
    "text": "Le stockage distribu√© : HDFS\n\nstockage sur diff√©rentes machines : ici les noeuds du cluster spark, c‚Äôest-√†-dire les diff√©rents ordinateurs workers du cluster\ndonn√©es divis√©es en blocs plus petits de taille fixe et r√©partis sur les machines\nchaque bloc est r√©pliqu√© trois fois pour √™tre r√©silient face aux pannes\nun NameNode supervise les m√©tadonn√©es et g√®re la structure du syst√®me de fichiers\nles DataNodes stockent effectivement les blocs de donn√©es\nle syst√®me HDFS est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local\n\n\n\n\n\n\nLes exports sur HDFS\n\n\nLorsqu‚Äôon exporte une table depuis notre session R vers HDFS, celle-ci est automatiquement partitionn√©e, comme le reste des donn√©es.\nAinsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\nIl est possible de ma√Ætriser le nombre de partitions avec la commande sdf_coalesce(partitions = 5) du package sparklyr.\nL‚Äôid√©al est d‚Äôadapter le nombre de partitions √† la taille d‚Äôun bloc : un bloc mesure 128 MB. Lorsqu‚Äôun bloc disque est utilis√©, m√™me √† 1%, il n‚Äôest pas utilisable pour un autre stockage.\nExporter un fichier de 1MB en 200 partitions r√©serve 200 blocs inutilement.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-stockage-distribu√©-hdfs-1",
    "href": "slides.html#le-stockage-distribu√©-hdfs-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le stockage distribu√© : HDFS",
    "text": "Le stockage distribu√© : HDFS\n\n‚ñ∂Ô∏è Les r√©plications de donn√©es ont deux fonctions :\n\naugementer la flexibilit√© de la distribution des traitements\naugmenter la r√©silience en cas de panne d‚Äôun noeud",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#la-lazy-evaluation",
    "href": "slides.html#la-lazy-evaluation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark distingue deux types d‚Äôop√©rations :\n\nles transformations : ce sont des op√©rations qui prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul lorsqu‚Äôelles sont appel√©es.\nPar exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d‚Äôex√©cution :\n\nune_transformation &lt;- un_spark_data_frame %&gt;%\n  group_by(identifiant) %&gt;%\n  mutate(une_somme = sum(revenus))\n\nles actions : ce sont des op√©rations qui demandent le calcul d‚Äôun r√©sultat et qui d√©clenchent le calcul et l‚Äôex√©cution de toutes les transformations compil√©es jusqu‚Äô√† l‚Äôappel de l‚Äôaction.\nPar exemple, le programme ci-dessous d√©clenche le calcul de la cellule une_transformation et de la moyenne des revenus :\n\nrevenu_moyen &lt;- une_transformation %&gt;%\n  summarise(revenu_moyen = mean(une_somme)) %&gt;%\n  print()\n\nLes principales actions sont : print(), collect(), head(), tbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#la-lazy-evaluation-1",
    "href": "slides.html#la-lazy-evaluation-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark optimise automatiquement les programmes soumis :\n\nCompilation des transformations\nInt√©gration dans un plan d‚Äôex√©cution : √©ventuelles erreurs du programme soulev√©es avant l‚Äôex√©cution\nOptimisation du plan logique par le module Catalyst (driver Spark)\nPar exemple si j‚Äô√©cris le programme :\n\nnon_optimal &lt;- table_1 %&gt;%\n  mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %&gt;%\n  filter(debut_contrat &gt;= as.Date(\"2023-01-01\"))\n\nCatalyst r√©√©crit :\n\nnon_optimal &lt;- table_1 %&gt;%\n  filter(debut_contrat &gt;= as.Date(\"2023-01-01\")) %&gt;%\n  mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))\n\nCette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu‚Äôune action d√©clenche l‚Äôex√©cution.\nR√©alisation de plans physiques possibles et s√©lection du meilleur plan physique (au regard de la localisation des donn√©es requises).\nD√©clencher le moins d‚Äôactions possibles dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.\nPour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-plan-dex√©cution",
    "href": "slides.html#le-plan-dex√©cution",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le plan d‚Äôex√©cution",
    "text": "Le plan d‚Äôex√©cution\n\nsource : documentation CASD disponible √† Documentation Data Science",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#r√©cup√©rer-un-r√©sultat",
    "href": "slides.html#r√©cup√©rer-un-r√©sultat",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "R√©cup√©rer un r√©sultat",
    "text": "R√©cup√©rer un r√©sultat\nLes r√©sultats qu‚Äôil est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :\n\nune table filtr√©e avec les variables n√©cessaires √† l‚Äô√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;\ndes statistiques descriptives synth√©tiques ;\nles premi√®res lignes de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;\nune table agr√©g√©e pour un graphique par exemple, √† l‚Äôaide de la fonction summarise().",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "href": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-du-driver-1",
    "href": "slides.html#lutilisation-de-la-m√©moire-du-driver-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver\nLorsqu‚Äôil est n√©cessaire de collecter une table volumineuse, il faut donc pr√©voir assez de m√©moire RAM pour le driver.\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\n\n\nBonne pratique de partage des ressources\n\n\nLe driver est instanci√© dans la bulle Midares, qui a vocation √† √™tre r√©duite suite √† la g√©n√©ralisation du cluster.\n\nLa bulle Midares a besoin de RAM minimale pour fonctionner, 100% des ressources ne sont donc pas disponibles pour sparklyr.\nPour permettre le travail simultan√© fluide de 10 utilisateurs, la m√©moire allou√©e au driver recommand√©e pour chaque utilisateur est de 15 Go.\nL‚Äôexport d‚Äôune table sdf directement au format .parquet est une alternative plus rapide, plus efficiente et qui permet par la suite de charger ses donn√©es en R classique et de travailler sur un df R sans utiliser sparklyr.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#comment-tester-son-code-pour-collecter-le-moins-possible",
    "href": "slides.html#comment-tester-son-code-pour-collecter-le-moins-possible",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Comment tester son code pour collecter le moins possible ?",
    "text": "Comment tester son code pour collecter le moins possible ?\nLa programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu‚Äô√† la fin pour que Catalyst optimise l‚Äôensemble du programme\nLa principale diff√©rence avec la programmation en R classique est que la visualisation de tables compl√®tes volumineuses n‚Äôest pas recommand√©e :\n\ngoulets d‚Äô√©tranglement m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;\nlongue : √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;\nbeaucoup moins efficace que l‚Äôexport direct en parquet du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l‚Äô√©tude.\n\nS‚Äôil est n√©cessaire de collecter, il faut pr√©voir beaucoup de RAM pour le driver avec le param√®tre spark.driver.memory.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#ce-qui-change-pour-lutilisateur",
    "href": "slides.html#ce-qui-change-pour-lutilisateur",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ce qui change pour l‚Äôutilisateur",
    "text": "Ce qui change pour l‚Äôutilisateur\nLa majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences sont les suivantes :\n\npour effectuer des op√©rations avec les dates, il faut utiliser les fonctions Hive sp√©cifiques.\narrange() ne fonctionne pas sur un spark_data_frame, il faut lui substituer window_order().\ndes fonctions sp√©cifiques aux spark data frames : sdf_bind_rows() pour empiler les lignes par exemple.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#quelques-fonctions-sp√©cifiques",
    "href": "slides.html#quelques-fonctions-sp√©cifiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques fonctions sp√©cifiques",
    "text": "Quelques fonctions sp√©cifiques\n\nDatesTableauStatistiques\n\n\nLes fonctions de lubridate()ne sont pas adapt√©es au spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nAdd_months\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\n\n\n\n\n\n\n\nFormat\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\nTri dans un groupe pour effectuer un calcul s√©quentiel\n\nODD_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                memory = FALSE)\n\nODD_premier &lt;- ODD_spark %&gt;%\n  group_by(id_midas) %&gt;%\n  window_order(id_midas, KDPOD) %&gt;%\n  mutate(date_premier_droit = first(KDPOD)) %&gt;%\n  ungroup() %&gt;%\n  distinct(id_midas, KROD3, date_premier_droit) %&gt;%\n  head(5)\n\nTri pour une sortie : sdf_sort() , arrange() ne fonctionne pas\nConcat√©ner les lignes (ou les colonnes sdf_bind_cols())\n\nODD_1 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &lt;= as.Date(\"2017-12-31\")) %&gt;%\n  mutate(groupe = \"temoins\")\n\nODD_2 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &gt;= as.Date(\"2021-12-31\")) %&gt;%\n  mutate(groupe = \"traites\")\n\nODD_evaluation &lt;- sdf_bind_rows(ODD_1, ODD_2)\n\nD√©doublonner une table\n\ndroits_dans_PJC &lt;- PJC_spark %&gt;%\n  sdf_distinct(id_midas, KROD3)\n\nprint(head(droits_dans_PJC, 5))\n\nPJC_dedoublonnee &lt;- PJC_spark %&gt;%\n  sdf_drop_duplicates()\n\nprint(head(PJC_dedoublonnee, 5))\n\nPivot : les fonctions du packag tidyr ne fonctionnent pas sur donn√©es spark\n\nODD_sjr_moyen &lt;- ODD_spark %&gt;%\n  mutate(groupe = ifelse(KDPOD &lt;= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %&gt;%\n  sdf_pivot(groupe ~ KCRGC,\n    fun.aggregate = list(KQCSJP = \"mean\")\n  )\n\n\n\n\n\nR√©sum√© statistique : sdf_describe() , summary()ne fonctionne pas.\nDimension : sdf_dim, la fonction nrow()ne fonctionne pas.\nQuantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, sdf_quantile()\nEchantillonnage al√©atoire : sdf_random_split",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#quelques-tips-doptimisation",
    "href": "slides.html#quelques-tips-doptimisation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques tips d‚Äôoptimisation",
    "text": "Quelques tips d‚Äôoptimisation\n\nJointuresPersistChargementExport et partitions\n\n\nPour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse\n\nSolution rapide :\n\ntable_finale &lt;- table_volumineuse_comme_PJC %&gt;%\n  right_join(petite_table_mon_champ)\n\nSolution lente :\n\ntable_finale &lt;- petite_table_mon_champ %&gt;%\n  left_join(table_volumineuse_comme_PJC)\n\n\n\nLorsqu‚Äôune table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c‚Äôest-√†-dire enregistrer ce spark_data_framesur le disque ou dans la m√©moire des noeuds.\n\ntable_1 &lt;- mon_champ %&gt;%\n  left_join(ODD, by = c(\"id_midas\", \"KROD3\")) %&gt;%\n  rename(duree_potentielle_indemnisation = KPJDXP,\n         SJR = KQCSJP,\n         date_debut_indemnisation = KDPOD) %&gt;%\n  sdf_persist()\n\nduree &lt;- table_1 %&gt;%\n  summarise(duree_moy = mean(duree_potentielle_indemnisation),\n            duree_med = median(duree_potentielle_indemnisation)) %&gt;%\n  collect()\n\nSJR &lt;- table_1 %&gt;%\n  summarise(SJR_moy = mean(SJR),\n            SJR_med = median(SJR)) %&gt;%\n  collect()\n\n\n\nLorsqu‚Äôon charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.\nAttention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l‚Äôex√©cution.\n\n\nLe format .parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.\nSi les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d‚Äôune m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul\n\nspark_write_parquet(DE, partition_by = c(\"REGIND\"))",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#forcer-le-calcul",
    "href": "slides.html#forcer-le-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Forcer le calcul",
    "text": "Forcer le calcul\nQuelques actions :\n\ncollecter la table enti√®re üõë\n\nspark_data_frame_1 %&gt;%\n  collect()\n\nafficher les premi√®res lignes\n\nspark_data_frame_1 %&gt;%\n  head(10)\n\nMettre les donner en cache\n\nspark_data_frame_1 %&gt;%\n  sdf_register() %&gt;%\n  tbl_cache()\n\nsc %&gt;% spark_session() %&gt;% invoke(\"catalog\") %&gt;% \n  invoke(\"clearCache\")",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#les-erreurs-en-sparklyr",
    "href": "slides.html#les-erreurs-en-sparklyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les erreurs en sparklyr",
    "text": "Les erreurs en sparklyr\nsparklyr traduit le code dplyr fourni en scala, mais interpr√®te √©galement les messages d‚Äôerreurs envoy√©s du cluster vers la session R.\nsparklyr n‚Äôest cependant pas performant pour interpr√©ter ces erreurs.\nN‚Äôh√©sitez pas √† enregistrer le code g√©n√©rant un message d‚Äôerreur dans Documents publics/erreurs_sparklyr\nUn test du code pas-√†-pas permet d‚Äôisoler le probl√®me.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#bonnes-pratiques",
    "href": "slides.html#bonnes-pratiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\n\nD√©connexion ou fermeture R pour lib√©rer les ressources üõë\nNe plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nPyspark ou Sparklyr pour la production ‚ùì\nUtilisation parcimonieuse des ressources ‚öñÔ∏è\nEnvoi des erreurs sparklyr üì©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#larchitecture-map-reduce",
    "href": "slides.html#larchitecture-map-reduce",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôarchitecture Map Reduce",
    "text": "L‚Äôarchitecture Map Reduce",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "href": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La gestion de la m√©moire avec spark",
    "text": "La gestion de la m√©moire avec spark\nLes shuffles sont les op√©rations les plus gourmandes en temps.\n\n\n\n\n\n\nQu‚Äôest-ce qu‚Äôun shuffle ?\n\n\nUn shuffle est un √©change de donn√©es entre diff√©rents noeuds du cluster.\nNous avons vu qu‚Äôutiliser sparkdans un cluster implique de distribuer √©galement le stockage des donn√©es.\nPar exemple :\n\nje demande un traitement sur la table PJC du FNA\nsi un noeud contenant d√©j√† les donn√©es de PJC est disponible, le cluster manager envoie le traitement √† ce noeud\nsi tous les noeuds contenant les donn√©es de PJC sont d√©j√† r√©serv√©s, alors le cluster manager demande le traitement √† un autre noeud, par exemple le noeud 1\nil demande √† un noeud contenant les donn√©es PJC, par exemple le noeud 4, d‚Äôenvoyer ces donn√©es au noeud 1 qui va ex√©cuter le traitement\ncet √©change de donn√©es est en r√©seau filaire : un √©change filaire est beaucoup plus lent qu‚Äôun envoi interne par le disque du noeud 1 √† la RAM du noeud 1\nc‚Äôest pourquoi pour optimiser un programme spark, il est possible de limiter les shuffles",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#sparkui-un-outil-doptimisation",
    "href": "slides.html#sparkui-un-outil-doptimisation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "SparkUI : un outil d‚Äôoptimisation",
    "text": "SparkUI : un outil d‚Äôoptimisation\nSpark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d‚Äôoptimiser les traitements :\n\nDAGGCM√©moire\n\n\n\n\n\nV√©rifier que le gc time est inf√©rieur √† 10% du temps pour ex√©cuter la t√¢che ‚úÖ\n\n\n\nV√©rifier que la storage memory ne sature pas la m√©moire ‚úÖ",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#utiliser-les-interfaces",
    "href": "slides.html#utiliser-les-interfaces",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Utiliser les interfaces",
    "text": "Utiliser les interfaces\n\nyarn : disponibilit√© des ressources\n\nSparkhistory pour des traitements de sessions ferm√©es",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#ma-session-ne-sinstancie-jamais",
    "href": "slides.html#ma-session-ne-sinstancie-jamais",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ma session ne s‚Äôinstancie jamais",
    "text": "Ma session ne s‚Äôinstancie jamais\nSi l‚Äôinstruction sc &lt;- spark_connect(master = \"yarn\", config = conf prend plus de 10 minutes, il est utile d‚Äôouvrir l‚Äôinterface de yarn pour v√©rifier que la file n‚Äôest pas d√©j√† enti√®rement occup√©e. L‚Äôerreur peut ne survenir qu‚Äôau bout d‚Äôune vingtaine de minutes : le job est ACCEPTED dans yarn, ou FAILED si la session n‚Äôa pas pu √™tre instanci√©e par manque de ressources disponibles.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#exporter-de-hdfs-au-local",
    "href": "slides.html#exporter-de-hdfs-au-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Exporter de HDFS au local",
    "text": "Exporter de HDFS au local",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#pyspark-mode-cluster",
    "href": "slides.html#pyspark-mode-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pyspark : mode cluster",
    "text": "Pyspark : mode cluster",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#les-avantages-de-pyspark",
    "href": "slides.html#les-avantages-de-pyspark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les avantages de pyspark",
    "text": "Les avantages de pyspark\n\nMode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è\nSpark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è\nPlusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª\nStabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ\nLisibilit√© du code üëì\nTemps de connexion et d‚Äôex√©cution r√©duit ‚è≤Ô∏è\nUtilisation optimale de SparkUI üìä",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#merci-pour-votre-attention",
    "href": "slides.html#merci-pour-votre-attention",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Merci pour votre attention !",
    "text": "Merci pour votre attention !",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atelier d‚Äôinitiation √† spark avec R en mode cluster",
    "section": "",
    "text": "Bienvenue ! üëã\nCe site est une aide √† l‚Äôutilisation de spark avec R sur un cluster de calcul Spark √† destination de d√©butants en R ne connaissant pas du tout le fonctionnement et la programmation en spark.\nTu vas apprendre ici (je l‚Äôesp√®re üôÇ) :\n\nl‚Äôint√©r√™t d‚Äôutiliser Spark pour manipuler des donn√©es volumineuses telles que l‚Äôappariement MiDAS ‚è≥\nle fonctionnement de Spark sur un cluster de calcul üî¢\nl‚Äôutilisation de Spark sous R avec le package sparklyr, tr√®s facile si tu connais dplyr üë®‚Äçüíª\nquelques pistes d‚Äôoptimisation d‚Äôun programme avec Spark üí°\n\nDe quoi profiter ensuite du temps gagn√© gr√¢ce au calcul distribu√© ! üöÄ",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "dates.html",
    "href": "dates.html",
    "title": "Les dates avec sparklyr",
    "section": "",
    "text": "Les fonctions de lubridate()ne sont pas adapt√©es aux spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nFormat\n\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\n\n\nAdd months\n\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.",
    "crumbs": [
      "Les dates avec sparklyr"
    ]
  },
  {
    "objectID": "dates.html#dates",
    "href": "dates.html#dates",
    "title": "Les dates avec sparklyr",
    "section": "",
    "text": "Les fonctions de lubridate()ne sont pas adapt√©es aux spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nFormat\n\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\n\n\nAdd months\n\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.",
    "crumbs": [
      "Les dates avec sparklyr"
    ]
  },
  {
    "objectID": "ressources.html",
    "href": "ressources.html",
    "title": "Ressources",
    "section": "",
    "text": "Documentation CASD : Documentation Data Science\nDocumentation MiDAS : Lien vers le site\nDocumentation du package sparklyr : Package sparklyr",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "ressources.html#listes-imbriqu√©es-avec-spark",
    "href": "ressources.html#listes-imbriqu√©es-avec-spark",
    "title": "Ressources",
    "section": "Listes imbriqu√©es avec spark",
    "text": "Listes imbriqu√©es avec spark\nListe imbriqu√©es Lucie\nappend des bases reduce union -&gt; r√©cup√©rer le programme",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "oom.html",
    "href": "oom.html",
    "title": "Out of memory",
    "section": "",
    "text": "L‚Äôexport direct d‚Äôun spark data frame en parquet est quasiment instantann√© : tu peux ensuite charger la table parquet en data frame R si tu souhaites la traiter en m√©moire vive.\nCet export direct n‚Äôest pas fonctionnel en spark local : encore une raison de passer sur le cluster Spark !\nSi le collect()est in√©vitable, il faut que la m√©moire RAM allou√©e au driver soit assez importante pour r√©cup√©rer le r√©sultat : elle est param√©trable avec l‚Äôoption spark.driver.memory de la configuration spark. Lorsque tu utilises une session sparklyr, le driver est dans la bulle Midares, donc th√©oriquement la limite physique de RAM que tu peux allouer au driver correspond √† la taille de bulle (mais d‚Äôautres coll√®gues auront aussi besoin de ces ressources).\n\n\n\n\n\nSi tu as utilis√© des fonctions telles que compute() dans ton programme, elles peuvent √™tre √† l‚Äôorigine d‚Äôune erreur Out of memory : ces commandes ne sont pas bien adapt√©es √† Spark et les r√©sultats temporaires stock√©s en m√©moire vive ne sont pas visibles dans l‚Äôespace d√©di√© sur SparkUI onglet Storage, il semblerait que ces r√©sultats temporaires soient stock√©s ‚Äúau mauvais endroit‚Äù et occupent de l‚Äôespace sans que l‚Äôutilisateur y ait acc√®s.\nPrivil√©gier les fonctions tbl_cache() pour forcer l‚Äôex√©cution du programme.\n\n\n\n\n\nForcer l‚Äôex√©cution du programme le plus tard possible pour permettre √† Spark d‚Äôoptimiser tout le programme et d‚Äôutiiliser les ressources de la mani√®re la plus parcimonieuse possible.\nTester son programme sur une toute petite partie des tables, √©tape par √©tape, en for√ßant l‚Äôex√©cution (appel d‚Äôune action telle que print() ou collect() ) √† chaque √©tape pour la phase de d√©buggage, puis supprimer toutes les actions interm√©diaires non n√©cessaires du programme pour que Spark optimise tout le programme.\n\n\n\n\n\nSi une table est charg√©e avec la fonction spark_read_parquet(sc, path = \"mon_chemin_vers_la_table\", memory = FALSE) et l‚Äôoption memory=FALSE , les donn√©es ne seront charg√©es du disque √† la m√©moire cache qu‚Äôen cas de n√©cessit√©, c‚Äôest-√†-dire si une action d√©clenche des transformations qui utilisent une partie de ces donn√©es.\nCharger une table en cache avec memory = TRUE force la mise en cache de toutes les partitions de cette table, ce qui immobilise des ressources et peut favoriser la survenue d‚Äôerreurs Ouf of memory. Si cette table n‚Äôest pas utilis√©e en totalit√© plusieurs fois dans le programme, ce chargement n‚Äôest pas optimal.\nS‚Äôil s‚Äôav√®re optimal de charger les donn√©es en cache, alors il faut donner √† chaque ex√©cuteur une quantit√© de m√©moire vive suffisante pour laisser des ressources pour l‚Äôex√©cution.\nPour consulter la taille des donn√©es charg√©es en cache, utiliser SparkUI, onglet Storage.\n\n\n\n\nPar souci de parcimonie et pour faciliter le travail de nos coll√®gues sur des ressources partag√©es, cette option doit rester l‚Äôultime recours pour √©viter l‚Äôerreur Out of memory ü§ù",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#collecter-le-moins-possible",
    "href": "oom.html#collecter-le-moins-possible",
    "title": "Out of memory",
    "section": "",
    "text": "L‚Äôexport direct d‚Äôun spark data frame en parquet est quasiment instantann√© : tu peux ensuite charger la table parquet en data frame R si tu souhaites la traiter en m√©moire vive.\nCet export direct n‚Äôest pas fonctionnel en spark local : encore une raison de passer sur le cluster Spark !\nSi le collect()est in√©vitable, il faut que la m√©moire RAM allou√©e au driver soit assez importante pour r√©cup√©rer le r√©sultat : elle est param√©trable avec l‚Äôoption spark.driver.memory de la configuration spark. Lorsque tu utilises une session sparklyr, le driver est dans la bulle Midares, donc th√©oriquement la limite physique de RAM que tu peux allouer au driver correspond √† la taille de bulle (mais d‚Äôautres coll√®gues auront aussi besoin de ces ressources).",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#eviter-les-fonctions-sp√©cifiques-√†-sparklyr",
    "href": "oom.html#eviter-les-fonctions-sp√©cifiques-√†-sparklyr",
    "title": "Out of memory",
    "section": "",
    "text": "Si tu as utilis√© des fonctions telles que compute() dans ton programme, elles peuvent √™tre √† l‚Äôorigine d‚Äôune erreur Out of memory : ces commandes ne sont pas bien adapt√©es √† Spark et les r√©sultats temporaires stock√©s en m√©moire vive ne sont pas visibles dans l‚Äôespace d√©di√© sur SparkUI onglet Storage, il semblerait que ces r√©sultats temporaires soient stock√©s ‚Äúau mauvais endroit‚Äù et occupent de l‚Äôespace sans que l‚Äôutilisateur y ait acc√®s.\nPrivil√©gier les fonctions tbl_cache() pour forcer l‚Äôex√©cution du programme.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#laisser-spark-travailler-pour-nous",
    "href": "oom.html#laisser-spark-travailler-pour-nous",
    "title": "Out of memory",
    "section": "",
    "text": "Forcer l‚Äôex√©cution du programme le plus tard possible pour permettre √† Spark d‚Äôoptimiser tout le programme et d‚Äôutiiliser les ressources de la mani√®re la plus parcimonieuse possible.\nTester son programme sur une toute petite partie des tables, √©tape par √©tape, en for√ßant l‚Äôex√©cution (appel d‚Äôune action telle que print() ou collect() ) √† chaque √©tape pour la phase de d√©buggage, puis supprimer toutes les actions interm√©diaires non n√©cessaires du programme pour que Spark optimise tout le programme.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#optimiser-le-chargement-en-cache-des-donn√©es",
    "href": "oom.html#optimiser-le-chargement-en-cache-des-donn√©es",
    "title": "Out of memory",
    "section": "",
    "text": "Si une table est charg√©e avec la fonction spark_read_parquet(sc, path = \"mon_chemin_vers_la_table\", memory = FALSE) et l‚Äôoption memory=FALSE , les donn√©es ne seront charg√©es du disque √† la m√©moire cache qu‚Äôen cas de n√©cessit√©, c‚Äôest-√†-dire si une action d√©clenche des transformations qui utilisent une partie de ces donn√©es.\nCharger une table en cache avec memory = TRUE force la mise en cache de toutes les partitions de cette table, ce qui immobilise des ressources et peut favoriser la survenue d‚Äôerreurs Ouf of memory. Si cette table n‚Äôest pas utilis√©e en totalit√© plusieurs fois dans le programme, ce chargement n‚Äôest pas optimal.\nS‚Äôil s‚Äôav√®re optimal de charger les donn√©es en cache, alors il faut donner √† chaque ex√©cuteur une quantit√© de m√©moire vive suffisante pour laisser des ressources pour l‚Äôex√©cution.\nPour consulter la taille des donn√©es charg√©es en cache, utiliser SparkUI, onglet Storage.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#si-tout-ceci-ne-fonctionne-pas-augmenter-les-ressources-dans-la-configuration",
    "href": "oom.html#si-tout-ceci-ne-fonctionne-pas-augmenter-les-ressources-dans-la-configuration",
    "title": "Out of memory",
    "section": "",
    "text": "Par souci de parcimonie et pour faciliter le travail de nos coll√®gues sur des ressources partag√©es, cette option doit rester l‚Äôultime recours pour √©viter l‚Äôerreur Out of memory ü§ù",
    "crumbs": [
      "Out of memory : help !"
    ]
  }
]