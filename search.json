[
  {
    "objectID": "slides.html#au-programme",
    "href": "slides.html#au-programme",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Au programme",
    "text": "Au programme\n\nMiDAS : une base de donn√©es volumineuse üíæ\nManipuler un appariement : une op√©ration co√ªteuse üí≤\nInitiation au calcul distribu√© : quelles ressources r√©server ? üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nSparklyr : la solution ergonomique de spark sous R üë®‚Äçüíª\nPour aller plus loin ‚è©"
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS croise trois bases de donn√©es administratives exhaustives :\n\nles donn√©es sur l‚Äôinscription et l‚Äôindemnisation des demandeurs d‚Äôemploi de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;\nles donn√©es sur les b√©n√©ficiaires de minima sociaux (RSA, PPA, AAH) et les caract√©ristiques des m√©nages de la CNAF : Allstat-FR6 ;\nles donn√©es sur les contrats salari√©s de la DSN : MMO de la Dares."
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nChaque vague de MiDAS correspond √† environ 600 Go de donn√©es au format sas. Les vagues fonctionnent par empilement :\n\nle gain de profondeur temporelle et l‚Äôentr√©e dans le champ de nouvelles personnes\nles vagues sont appariables entre elles"
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS est l‚Äôune des bases de donn√©es les plus volumineuses du SSP :\nLes administrations dont les donn√©es sont comparables √† MiDAS utilisent un cluster Spark : Insee, Drees, Acoss‚Ä¶\n‚ñ∂Ô∏èLe cluster spark est la solution la plus efficiente pour traiter des donn√©es de cette ampleur. Apprendre √† l‚Äôutiliser pourra vous √™tre utile dans d‚Äôautres contextes que celui de la Dares."
  },
  {
    "objectID": "slides.html#structure-de-lappariement",
    "href": "slides.html#structure-de-lappariement",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Structure de l‚Äôappariement",
    "text": "Structure de l‚Äôappariement\n\n\n\n\n\n\n\nPourquoi Spark ?\n\n\nLa manipulation des donn√©es MiDAS en l‚Äô√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains."
  },
  {
    "objectID": "slides.html#le-format-parquet",
    "href": "slides.html#le-format-parquet",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le format parquet",
    "text": "Le format parquet\nLes donn√©es sont converties au format parquet d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l‚Äôespace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :\n\nil compresse efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv\nil est orient√© colonnes\nil permet le chargement efficace en m√©moire des donn√©es\nIl permet le stockage partitionn√© des donn√©es\nil permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires sur disque\nIl est ind√©pendant du logiciel utilis√© : il peut donc √™tre trait√© par spark et par R."
  },
  {
    "objectID": "slides.html#lespace-midares",
    "href": "slides.html#lespace-midares",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôespace MiDares",
    "text": "L‚Äôespace MiDares\n\nRessourcesSch√©ma\n\n\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n512 Go de m√©moire vive (ou RAM) : passage √† 256 Go\n\n\n\n\n\n\n\nLa m√©moire vive\n\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa volatilit√© (toutes les donn√©es sont perdues si l‚Äôordinateur n‚Äôest plus aliment√©) et par l‚Äôacc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.\n\n\n\n\nUn processeur (ou CPU) compos√© de 32 coeurs : passage √† 16 coeurs\n\n\n\n\n\n\n\nLe processeur\n\n\nLe processeur permet d‚Äôex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel‚Ä¶ Il est compos√© d‚Äôun ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu‚Äôune seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu‚Äôil a de coeurs. Un processeur se caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d‚Äôop√©rations qu‚Äôil est capable d‚Äôeffetuer par seconde."
  },
  {
    "objectID": "slides.html#programmer-en-m√©moire-vive",
    "href": "slides.html#programmer-en-m√©moire-vive",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer en m√©moire vive",
    "text": "Programmer en m√©moire vive\n\nR : la m√©moire vive, √©tat dans l‚Äôenvironnement\nSAS : lecture/√©criture sur le disque\nMiDAS au format sas &gt;&gt; taille de la m√©moire vive disponible du serveur CASD ‚Äì&gt; format .parquet\nImpossible de charger tout MiDAS en m√©moire vive\nDes solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :\narrow (avec des requ√™tes dplyr)\nduckDB : recommand√© par le SSPLab pour des donn√©es jusqu‚Äô√† 100Go\n‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements."
  },
  {
    "objectID": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "href": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les traitements co√ªteux en puissance de calcul",
    "text": "Les traitements co√ªteux en puissance de calcul\n\nles jointures\nles op√©rations en group_by()\ndistinct()\n‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n‚ñ∂Ô∏è Erreur ‚Äúout of memory‚Äù."
  },
  {
    "objectID": "slides.html#un-traitement-peu-co√ªteux-un-traitement-map",
    "href": "slides.html#un-traitement-peu-co√ªteux-un-traitement-map",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement peu co√ªteux : un traitement MAP",
    "text": "Un traitement peu co√ªteux : un traitement MAP\n\nCe traitement est peu co√ªteux :\n\nchargement d‚Äôune seule colonne en RAM : format parquet orient√© colonnes\npeu de m√©moire d‚Äôex√©cution : R est un langage vectoris√©"
  },
  {
    "objectID": "slides.html#un-traitement-co√ªteux-un-traitement-reduce",
    "href": "slides.html#un-traitement-co√ªteux-un-traitement-reduce",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement co√ªteux : un traitement REDUCE",
    "text": "Un traitement co√ªteux : un traitement REDUCE\n\nCe traitement n√©cessite :\n\nle chargement de davantage de colonnes en m√©moire vive ;\ndavantage de m√©moire d‚Äôex√©cution pour effectuer l‚Äôintersection (inner_join())."
  },
  {
    "objectID": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "href": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√© et calcul parall√®le",
    "text": "Calcul distribu√© et calcul parall√®le\n\n\nCalcul non distribu√©\nLes probl√©matiques Big Data en R sont les suivantes :\n\nla taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R\nle temps de calcul : les √©tapes du traitement sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)\nl‚Äôoptimisation du programme\n\n\nCalcul distribu√© spark\nLe calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :\n\nchargement des donn√©es en m√©moire parcimonieux et non syst√©matique\nex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents\noptimisation automatique du code"
  },
  {
    "objectID": "slides.html#un-traitement-map-distribu√©",
    "href": "slides.html#un-traitement-map-distribu√©",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement MAP distribu√©",
    "text": "Un traitement MAP distribu√©\n\nSi les donn√©es sont stock√©es sur diff√©rents ordinateurs :\n\nles calculs peuvent √™tre effectu√©s en parall√®le ;\ngain de temps li√© √† l‚Äôaugmentation des ressources informatiques pour effectuer le calcul et √† la parall√©lisation.\n\nLes traitements MAP se pr√™tent parfaitement au calcul distribu√© et parall√®le."
  },
  {
    "objectID": "slides.html#un-traitement-reduce-distribu√©",
    "href": "slides.html#un-traitement-reduce-distribu√©",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement REDUCE distribu√©",
    "text": "Un traitement REDUCE distribu√©\n\nSi les donn√©es sont stock√©es sur diff√©rents ordinateurs :\n\nil faut les rappatrier au m√™me endroit pour effectuer la jointure ;\ncet √©change est effectu√© en r√©seau entre les ordinateurs : l‚Äôenvoi r√©seau a un co√ªt non n√©gligeable en temps.\n\nLes traitements REDUCE ne se pr√™tent pas bien au calcul distribu√© et parall√®le."
  },
  {
    "objectID": "slides.html#spark",
    "href": "slides.html#spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Spark",
    "text": "Spark\n\nApache Spark : librairie open source d√©velopp√©e dans le langage scala\nScala : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java\n\nval TopHorrorsIGN2022 = Seq(\n  (9, \"Pearl\"),\n  (6, \"The Sadness\"),\n  (6, \"Offseason\"),\n  (7, \"Hatching\"),\n  (8, \"x\")\n).toDF(\"IMDB Rating\", \"IGN Movie Picks\")\n\nval TopHorrorsTheAVClub2022 = Seq(\n  (7, \"Nope\"),\n  (9, \"Pearl\"),\n  (8, \"x\"),\n  (5, \"Barbarian\"),\n  (5, \"Bones And All\")\n).toDF(\"IMDB Rating\", \"AVC Movie Picks\")\n\nimport org.apache.spark.sql.functions.col\n\nval cols = List(col(\"IGN Movie Picks\"), col(\"AVC Movie Picks\"))\n\nval query = TopHorrorsIGN2022(\n  \"IGN Movie Picks\"\n) === TopHorrorsTheAVClub2022(\"AVC Movie Picks\")\n\nval outerJoin = TopHorrorsIGN2022\n  .join(TopHorrorsTheAVClub2022, query, \"outer\")\n  .select(cols: _*)\n\nouterJoin.show()\n\nscala adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de spark et optimiser au maximum les traitements en spark\nspark est compatible avec les langages scala, R, python, java, et peut interpr√©ter des commandes SQL.\nDeux packages existent sous R :\n\nsparkR propos√© par Apache Spark\nsparklyr, qui permet d‚Äôutiliser directement des commandes dplyr traduites en spark par le package."
  },
  {
    "objectID": "slides.html#installation-de-spark-sous-casd",
    "href": "slides.html#installation-de-spark-sous-casd",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Installation de spark sous CASD",
    "text": "Installation de spark sous CASD\nVoir la fiche d√©di√©e sur le site"
  },
  {
    "objectID": "slides.html#la-machine-virtuelle-java",
    "href": "slides.html#la-machine-virtuelle-java",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La machine virtuelle Java",
    "text": "La machine virtuelle Java\nSpark est r√©dig√© en scala, un langage qui a besoin d‚Äôune machine virtuelle Java pour √™tre ex√©cut√©. La machine virtuelle Java est scalable : l‚Äôutilisateur peut choisir quelles ressources physiques elle a le droit d‚Äôutiliser sur l‚Äôensemble des ressources physiques disponibles sur l‚Äôordinateur. C‚Äôest un mini ordinateur cr√©√© par spark √† l‚Äôint√©rieur de notre propre ordinateur, qui utilise les ressources de ce dernier.\n\n\n\n\n\n\nMachine virtuelle\n\n\nUne machine virtuelle a les m√™mes caract√©ristiques qu‚Äôun ordinateur :\n\nun syst√®me d‚Äôexploitation : Windows, Linux, MacOS\ndes ressources physiques : CPU, RAM et stockage disque\n\nLa diff√©rence avec un ordinateur : une machine virtuelle peut √™tre cr√©√©e sur un serveur physique en r√©servant une petite partie des ressources du serveur seulement, ce qui permet de cr√©er plusieurs ordinateurs diff√©rents sur une seule infractructure physique"
  },
  {
    "objectID": "slides.html#deux-mani√®res-dutiliser-spark",
    "href": "slides.html#deux-mani√®res-dutiliser-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Deux mani√®res d‚Äôutiliser Spark",
    "text": "Deux mani√®res d‚Äôutiliser Spark\n\n\nAvec un seul ordinateur\nCe mode est appel√© Spark local.\n\nune unique machine virtuelle Java est cr√©√©e pour ex√©cuter le code spark\nt√¢ches parall√©lis√©es sur les diff√©rents coeurs (CPU) du processeur de la machine virtuelle Java\nl‚Äôordinateur sur lequel est cr√©√©e cette machine virtuelle Java est la bulle MiDARES, √©quivalent d‚Äôun unique gros ordinateur\n\n\nSur un cluster de calcul\nUn cluster de calcul est un ensemble d‚Äôordinateurs ou machines virtuelles connect√©s en r√©seau.\n\nune machine virtuelle Java est cr√©√©e par spark dans chaque ordinateur du cluster\nt√¢ches parall√©lis√©es sur les diff√©rents ordinateurs du cluster\nla session R reste sur la bulle MiDARES, le code R est traduit en scala puis envoy√© sur le cluster pour √™tre ex√©cut√©."
  },
  {
    "objectID": "slides.html#mode-local-sch√©ma",
    "href": "slides.html#mode-local-sch√©ma",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : sch√©ma",
    "text": "Mode local : sch√©ma"
  },
  {
    "objectID": "slides.html#mode-local-√†-√©viter",
    "href": "slides.html#mode-local-√†-√©viter",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : √† √©viter",
    "text": "Mode local : √† √©viter\nEn mode local :\n\nles ressources utilis√©es par la machine virtuelle sont celles de la bulle\nil faut allouer suffisamment de coeurs √† la JVM pour parall√©liser\nm√™me si l‚Äôutilisateur choisit des ressources faibles, les ressources r√©elles utilis√©es dans une session spark peuvent √™tre plus √©lev√©es : mauvaise gestion de l‚Äôallocation des ressources\nacc√©l√©ration sensible par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources\nSur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local : l‚Äôutilisation simultan√©e de spark par plusieurs membres de la bulle entra√Ænent des ralentissements consid√©rables\n‚ñ∂Ô∏èmode local √† √©viter absolument"
  },
  {
    "objectID": "slides.html#le-cluster-de-calcul-midares-pr√©sentation",
    "href": "slides.html#le-cluster-de-calcul-midares-pr√©sentation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le cluster de calcul Midares : pr√©sentation",
    "text": "Le cluster de calcul Midares : pr√©sentation"
  },
  {
    "objectID": "slides.html#se-connecter-√†-spark-sur-un-cluster",
    "href": "slides.html#se-connecter-√†-spark-sur-un-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Se connecter √† Spark sur un cluster",
    "text": "Se connecter √† Spark sur un cluster\nSe connecter √† spark revient √† demander √† spark de cr√©er toutes les JVM demand√©es capables de lire du scala.\nPour se connecter √† spark depuis R avec le package sparklyr :\n\nlibrary(sparklyr)\n\nconf &lt;- spark_config()\nconf$spark.executor.instances &lt;- 5\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\nLe param√®tre spark.executor.instances correspond au nombre d‚Äôordinateurs sur lequel on souhaite parall√©liser le travail d‚Äôex√©cution de code. Ici, l‚Äôutilisateur demande 5 ordinateurs du cluster.\nNous verrons plus loin quels param√®tres nous devons pr√©ciser dans le fichier de configuration."
  },
  {
    "objectID": "slides.html#une-connexion",
    "href": "slides.html#une-connexion",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une connexion",
    "text": "Une connexion\nToutes les JVM demand√©es (5) sont instanci√©es dans les ordinateurs du cluster, avec les param√®tres d√©finis."
  },
  {
    "objectID": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr",
    "href": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La vie d‚Äôun programme r√©dig√© en sparklyr",
    "text": "La vie d‚Äôun programme r√©dig√© en sparklyr\nAvec sparklyr, il est possible de programmer directement en dplyr pour utiliser spark.\n\n# un data frame que j'envoie dans spark\nun_df &lt;- data.frame(c(1,2,3), c(\"A\", \"B\", \"C\"))\nnames(un_df) &lt;- c(\"col_num\", \"col_char\")\n\n# C'est maintenant un spark_data_frame\ncopy_to(sc, un_df)\n    \nun_df_transforme &lt;- un_df %&gt;%\n    mutate(une_nouvelle_col = col_num*2)\n\nSi j‚Äôex√©cute ce programme, je ne pourrai pas ouvrir un_df_transforme, d‚Äôailleurs, il ne se sera rien pass√©."
  },
  {
    "objectID": "slides.html#la-lazy-evaluation",
    "href": "slides.html#la-lazy-evaluation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark distingue deux types d‚Äôop√©rations :\n\nles transformations : ce sont des op√©rations qui prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul lorsqu‚Äôelles sont appel√©es.\nPar exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d‚Äôex√©cution :\n\nune_transformation &lt;- un_spark_data_frame %&gt;%\n  group_by(identifiant) %&gt;%\n  mutate(une_somme = sum(revenus))\n\nles actions : ce sont des op√©rations qui demandent le calcul d‚Äôun r√©sultat et qui d√©clenchent le calcul et l‚Äôex√©cution de toutes les transformations compil√©es jusqu‚Äô√† l‚Äôappel de l‚Äôaction.\nPar exemple, le programme ci-dessous d√©clenche le calcul de la cellule une_transformation et de la moyenne des revenus :\n\nrevenu_moyen &lt;- une_transformation %&gt;%\n  summarise(revenu_moyen = mean(une_somme)) %&gt;%\n  print()\n\nLes principales actions sont : print(), collect(), head(), tbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser)."
  },
  {
    "objectID": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr-1",
    "href": "slides.html#la-vie-dun-programme-r√©dig√©-en-sparklyr-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La vie d‚Äôun programme r√©dig√© en sparklyr",
    "text": "La vie d‚Äôun programme r√©dig√© en sparklyr\nPrenons l‚Äôexemple d‚Äôun programme contenant une action."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver",
    "href": "slides.html#le-r√¥le-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver",
    "text": "Le r√¥le du driver\n\n\nLe programme R est traduit en scala gr√¢ce au package sparklyr\nLe driver √©value le programme, il lit le code scala mais n‚Äôex√©cute rien du tout\nS‚Äôil remarque une erreur, l‚Äôerreur est envoy√©e directement √† l‚Äôutilisateur en session R avant l‚Äôex√©cution du programme : c‚Äôest la force de la lazy evaluation."
  },
  {
    "objectID": "slides.html#le-plan-dex√©cution",
    "href": "slides.html#le-plan-dex√©cution",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le plan d‚Äôex√©cution",
    "text": "Le plan d‚Äôex√©cution\n\nsource : documentation CASD disponible √† Documentation Data Science\nAJOUTER UN DAG"
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver-catalyst",
    "href": "slides.html#le-r√¥le-du-driver-catalyst",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver : Catalyst",
    "text": "Le r√¥le du driver : Catalyst\nLe driver contient un programme nomm√© Catalyst qui optimise le code scala automatiquement.\nSpark optimise automatiquement les programmes soumis :\n\nCompilation des transformations pour soulever les √©ventuelles erreurs\nInt√©gration dans un plan d‚Äôex√©cution contenant les √©tapes n√©cessaires pour parvenir au r√©sultat demand√© par le programme\nOptimisation du plan logique par le module Catalyst (driver Spark)\n\nPar exemple si j‚Äô√©cris le programme :\n\nnon_optimal &lt;- table_1 %&gt;%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\"))\n\nCatalyst r√©√©crit :\n\noptimal &lt;- table_1 %&gt;%   \n    filter(debut_contrat &gt;= as.Date(\"2023-01-01\")) %&gt;%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))\n\nCette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu‚Äôune action d√©clenche l‚Äôex√©cution."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver-catalyst-1",
    "href": "slides.html#le-r√¥le-du-driver-catalyst-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver : Catalyst",
    "text": "Le r√¥le du driver : Catalyst"
  },
  {
    "objectID": "slides.html#le-r√¥le-du-driver-catalyst-2",
    "href": "slides.html#le-r√¥le-du-driver-catalyst-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du driver : Catalyst",
    "text": "Le r√¥le du driver : Catalyst\n\nR√©alisation de plans physiques possibles et s√©lection du meilleur plan physique (au regard de la localisation des donn√©es requises). Le plan physique est la distribution des diff√©rents calculs aux machines du cluster.\nD√©clencher le moins d‚Äôactions possibles dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.\nPour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-cluster-manager",
    "href": "slides.html#le-r√¥le-du-cluster-manager",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du cluster manager",
    "text": "Le r√¥le du cluster manager\n\nLe cluster manager distribue les traitements physiques aux ordinateurs du cluster :\n\nil conna√Æt le meilleur plan physique fourni par Catalyst ;\nil conna√Æt les ressources disponibles et occup√©es par toutes les machines du cluster ;\nil affecte les ressources disponibles √† la session spark."
  },
  {
    "objectID": "slides.html#le-r√¥le-du-worker",
    "href": "slides.html#le-r√¥le-du-worker",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le r√¥le du worker",
    "text": "Le r√¥le du worker\n\nLe worker effectue le morceau de programme qu‚Äôon lui affecte et renvoie le r√©sultat au driver, qui lui-m√™me affiche le r√©sultat en session R :\n\nil ne conna√Æt que les t√¢ches qu‚Äôon lui a affect√©es ;\nil peut communiquer avec le driver en r√©seau pour renvoyer un r√©sultat ;\nil peut communiquer avec les autres workers en r√©seau pour partager des donn√©es ou des r√©sultats interm√©diaires : c‚Äôest un shuffle."
  },
  {
    "objectID": "slides.html#o√π-sont-les-donn√©es",
    "href": "slides.html#o√π-sont-les-donn√©es",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π sont les donn√©es ?",
    "text": "O√π sont les donn√©es ?"
  },
  {
    "objectID": "slides.html#o√π-sont-les-donn√©es-1",
    "href": "slides.html#o√π-sont-les-donn√©es-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π sont les donn√©es ?",
    "text": "O√π sont les donn√©es ?"
  },
  {
    "objectID": "slides.html#transfert-de-la-bulle-√†-hdfs",
    "href": "slides.html#transfert-de-la-bulle-√†-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Transfert de la bulle √† HDFS",
    "text": "Transfert de la bulle √† HDFS"
  },
  {
    "objectID": "slides.html#transfert-de-hdfs-√†-la-bulle",
    "href": "slides.html#transfert-de-hdfs-√†-la-bulle",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Transfert de HDFS √† la bulle",
    "text": "Transfert de HDFS √† la bulle"
  },
  {
    "objectID": "slides.html#mais-o√π-sont-r√©ellement-les-donn√©es-hdfs",
    "href": "slides.html#mais-o√π-sont-r√©ellement-les-donn√©es-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mais o√π sont r√©ellement les donn√©es ? HDFS",
    "text": "Mais o√π sont r√©ellement les donn√©es ? HDFS\nHadoop Distributed File System (HDFS)\n\nstockage sur diff√©rentes machines : ici les noeuds du cluster spark, c‚Äôest-√†-dire les diff√©rents ordinateurs workers du cluster\ndonn√©es divis√©es en blocs plus petits de taille fixe et r√©partis sur les machines : aucune table de MiDAS n‚Äôexiste en entier sur le cluster\nchaque bloc est r√©pliqu√© trois fois : il existe trois fois les 10 premi√®res lignes de la table FNA sur trois ordinateurs diff√©rents du cluster (r√©silience)\nun NameNode supervise les m√©tadonn√©es et g√®re la structure du syst√®me de fichiers\nles DataNodes stockent effectivement les blocs de donn√©es : les datanodes sont en fait les disques des workers du cluster, chaque ordinateur du cluster dispose d‚Äôun disque avec une partie des donn√©es MiDAS\nle syst√®me HDFS est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local"
  },
  {
    "objectID": "slides.html#param√©trer-sa-session",
    "href": "slides.html#param√©trer-sa-session",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Param√©trer sa session",
    "text": "Param√©trer sa session\nIl faut pr√©ciser quelles ressources r√©server pour chaque unit√© spark : le driver, le nombre d‚Äôordinateurs workers (appel√©es instances), la RAM, le nombre de coeurs\nLa configuration par d√©faut est :\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\nLes ressources physiques utilis√©es :\n\npar le driver : avec spark.driver.memory (avec parcimonie)\npar chaque worker avec spark.executor.memory(valeur max 140 Go) et spark.executor.cores (valeur max 8 coeurs)\nle nombre de workers avec spark.executor.instances (2 ou 3 suffisent)\nLa file sur laquelle on travaille avec spark.yarn.queue (prod ou dev)\n\nle nombre de partitions de chaque spark_data_frame avec spark.sql.shuffle.partitions (200 par d√©faut)\nla limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec spark.driver.maxResultSize (0 est la meilleure option)\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)"
  },
  {
    "objectID": "slides.html#mode-cluster-non-concurrence-gr√¢ce-au-cluster-manager",
    "href": "slides.html#mode-cluster-non-concurrence-gr√¢ce-au-cluster-manager",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode cluster : non concurrence gr√¢ce au cluster manager",
    "text": "Mode cluster : non concurrence gr√¢ce au cluster manager\n\nLe mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d‚Äôun serveur. Ces machines communiquent en r√©seau.\nCapture d‚Äô√©cran r√©servation des ressources\nIl est donc n√©cessaire de se d√©connecter pour lib√©rer les ressources : des ressources r√©serv√©es, m√™me lorsqu‚Äôaucun programme ne tourne, ne peuvent jamais √™tre affect√©es √† d‚Äôautres utilisateurs."
  },
  {
    "objectID": "slides.html#importer-les-donn√©es-depuis-hdfs-sous-r",
    "href": "slides.html#importer-les-donn√©es-depuis-hdfs-sous-r",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Importer les donn√©es depuis HDFS sous R",
    "text": "Importer les donn√©es depuis HDFS sous R\nLes donn√©es doivent √™tre disponibles dans les workers sous forme de spark_data_frame :\n\ncach√© en m√©moire directement : si utilis√©es de tr√®s nombreuses fois pour gagner du temps\nlaiss√© sur disque tant qu‚Äôaucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire\n‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.\nPour passer un data.frame R en spark_data_frame : copy_to()\n\n\npjc_df_spark &lt;- spark_read_parquet(sc,\n                                  path = \"hdfs:///dataset/MiDAS_v4/FNA/pjc.parquet\",\n                                  memory = TRUE)\n\npjc_filtree &lt;- pjc_df_spark %&gt;%\n  filter(KDDPJ &gt;= as.Date(\"2022-01-01\"))\n\nspark_write_parquet(pjc_filtree, \"hdfs:///tmp/pjc_filtree.parquet\")\n\npjc_df_spark &lt;- copy_to(sc, \"PJC\")"
  },
  {
    "objectID": "slides.html#les-exports-sur-hdfs",
    "href": "slides.html#les-exports-sur-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les exports sur HDFS",
    "text": "Les exports sur HDFS\n\n\n\n\n\n\nLes exports sur HDFS\n\n\nLorsqu‚Äôon exporte une table depuis notre session R vers HDFS, celle-ci est automatiquement partitionn√©e, comme le reste des donn√©es.\nAinsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\nIl est possible de ma√Ætriser le nombre de partitions avec la commande sdf_coalesce(partitions = 5) du package sparklyr.\nL‚Äôid√©al est d‚Äôadapter le nombre de partitions √† la taille d‚Äôun bloc : un bloc mesure 128 MB. Lorsqu‚Äôun bloc disque est utilis√©, m√™me √† 1%, il n‚Äôest pas utilisable pour un autre stockage.\nExporter un fichier de 1MB en 200 partitions r√©serve 200 blocs inutilement."
  },
  {
    "objectID": "slides.html#les-shuffles",
    "href": "slides.html#les-shuffles",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les shuffles",
    "text": "Les shuffles\n\nComme nous l‚Äôavons vu, les traitements REDUCE ne se pr√™tent pas tr√®s bien au calcul distribu√© :\n\naugmenter le nombre de workers augmente la probabilit√© de devoir effectuer des shuffles\nil est recommand√© de se limiter √† deux workers comme dans la configuration propos√©e\nr√©server d‚Äôautres ressources n‚Äôest souvent pas efficient et monopolise les ressources pour les autres utilisateurs."
  },
  {
    "objectID": "slides.html#r√©cup√©rer-un-r√©sultat",
    "href": "slides.html#r√©cup√©rer-un-r√©sultat",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "R√©cup√©rer un r√©sultat",
    "text": "R√©cup√©rer un r√©sultat\nLes r√©sultats qu‚Äôil est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :\n\nune table filtr√©e avec les variables n√©cessaires √† l‚Äô√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;\ndes statistiques descriptives synth√©tiques ;\nles premi√®res lignes de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;\nune table agr√©g√©e pour un graphique par exemple, √† l‚Äôaide de la fonction summarise()."
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "href": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver"
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-du-driver-1",
    "href": "slides.html#lutilisation-de-la-m√©moire-du-driver-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver\nLorsqu‚Äôil est n√©cessaire de collecter une table volumineuse, il faut donc pr√©voir assez de m√©moire RAM pour le driver.\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\n\n\n\nBonne pratique de partage des ressources\n\n\nLe driver est instanci√© dans la bulle Midares, qui a vocation √† √™tre r√©duite suite √† la g√©n√©ralisation du cluster.\n\nLa bulle Midares a besoin de RAM minimale pour fonctionner, 100% des ressources ne sont donc pas disponibles pour sparklyr.\nPour permettre le travail simultan√© fluide de 10 utilisateurs, la m√©moire allou√©e au driver recommand√©e pour chaque utilisateur est de 15 Go.\nL‚Äôexport d‚Äôune table sdf directement au format .parquet est une alternative plus rapide, plus efficiente et qui permet par la suite de charger ses donn√©es en R classique et de travailler sur un df R sans utiliser sparklyr."
  },
  {
    "objectID": "slides.html#comment-tester-son-code-pour-collecter-le-moins-possible",
    "href": "slides.html#comment-tester-son-code-pour-collecter-le-moins-possible",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Comment tester son code pour collecter le moins possible ?",
    "text": "Comment tester son code pour collecter le moins possible ?\nLa programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu‚Äô√† la fin pour que Catalyst optimise l‚Äôensemble du programme\nLa principale diff√©rence avec la programmation en R classique est que la visualisation de tables compl√®tes volumineuses n‚Äôest pas recommand√©e :\n\ngoulets d‚Äô√©tranglement m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;\nlongue : √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;\nbeaucoup moins efficace que l‚Äôexport direct en parquet du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l‚Äô√©tude.\n\nS‚Äôil est n√©cessaire de collecter, il faut pr√©voir beaucoup de RAM pour le driver avec le param√®tre spark.driver.memory."
  },
  {
    "objectID": "slides.html#ce-qui-change-pour-lutilisateur",
    "href": "slides.html#ce-qui-change-pour-lutilisateur",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ce qui change pour l‚Äôutilisateur",
    "text": "Ce qui change pour l‚Äôutilisateur\nLa majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences principales sont les suivantes :\n\n\n\n\n\n\n\n\nFonctionnalit√©\ntidyverse\nsparklyr\n\n\n\n\nimport d‚Äôun fichier .parquet\nread_parquet\nspark_read_parquet()\n\n\ntri d‚Äôun tableau\narrange()\nwindow_order() ou sdf_sort()\n\n\nop√©rations sur les dates\nlubridate\nfonctions Hive\n\n\nempiler des tableaux\nbind_rows()\nsdf_bind_rows()\n\n\nnombre de lignes d‚Äôun tableau\nnrow()\nsdf_nrow()\n\n\nfaire pivoter un tableau\ntidyr\nsdf_pivot()\n\n\nexport d‚Äôun spark_data_frame\n\nspark_write_parquet()"
  },
  {
    "objectID": "slides.html#quelques-fonctions-sp√©cifiques",
    "href": "slides.html#quelques-fonctions-sp√©cifiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques fonctions sp√©cifiques",
    "text": "Quelques fonctions sp√©cifiques\n\nDatesTableauStatistiques\n\n\nLes fonctions de lubridate()ne sont pas adapt√©es au spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nAdd_months\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\n\n\n\n\n\n\n\nFormat\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\nTri dans un groupe pour effectuer un calcul s√©quentiel\n\nODD_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                memory = FALSE)\n\nODD_premier &lt;- ODD_spark %&gt;%\n  group_by(id_midas) %&gt;%\n  window_order(id_midas, KDPOD) %&gt;%\n  mutate(date_premier_droit = first(KDPOD)) %&gt;%\n  ungroup() %&gt;%\n  distinct(id_midas, KROD3, date_premier_droit) %&gt;%\n  head(5)\n\nTri pour une sortie : sdf_sort() , arrange() ne fonctionne pas\nConcat√©ner les lignes (ou les colonnes sdf_bind_cols())\n\nODD_1 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &lt;= as.Date(\"2017-12-31\")) %&gt;%\n  mutate(groupe = \"temoins\")\n\nODD_2 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &gt;= as.Date(\"2021-12-31\")) %&gt;%\n  mutate(groupe = \"traites\")\n\nODD_evaluation &lt;- sdf_bind_rows(ODD_1, ODD_2)\n\nD√©doublonner une table\n\ndroits_dans_PJC &lt;- PJC_spark %&gt;%\n  sdf_distinct(id_midas, KROD3)\n\nprint(head(droits_dans_PJC, 5))\n\nPJC_dedoublonnee &lt;- PJC_spark %&gt;%\n  sdf_drop_duplicates()\n\nprint(head(PJC_dedoublonnee, 5))\n\nPivot : les fonctions du packag tidyr ne fonctionnent pas sur donn√©es spark\n\nODD_sjr_moyen &lt;- ODD_spark %&gt;%\n  mutate(groupe = ifelse(KDPOD &lt;= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %&gt;%\n  sdf_pivot(groupe ~ KCRGC,\n    fun.aggregate = list(KQCSJP = \"mean\")\n  )\n\n\n\n\n\nR√©sum√© statistique : sdf_describe() , summary()ne fonctionne pas.\nDimension : sdf_dim, la fonction nrow()ne fonctionne pas.\nQuantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, sdf_quantile()\nEchantillonnage al√©atoire : sdf_random_split"
  },
  {
    "objectID": "slides.html#quelques-tips-doptimisation",
    "href": "slides.html#quelques-tips-doptimisation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques tips d‚Äôoptimisation",
    "text": "Quelques tips d‚Äôoptimisation\n\nJointuresPersistChargementExport et partitions\n\n\nPour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse\n\nSolution rapide :\n\ntable_finale &lt;- table_volumineuse_comme_PJC %&gt;%\n  right_join(petite_table_mon_champ)\n\nSolution lente :\n\ntable_finale &lt;- petite_table_mon_champ %&gt;%\n  left_join(table_volumineuse_comme_PJC)\n\n\n\nLorsqu‚Äôune table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c‚Äôest-√†-dire enregistrer ce spark_data_framesur le disque ou dans la m√©moire des noeuds.\n\ntable_1 &lt;- mon_champ %&gt;%\n  left_join(ODD, by = c(\"id_midas\", \"KROD3\")) %&gt;%\n  rename(duree_potentielle_indemnisation = KPJDXP,\n         SJR = KQCSJP,\n         date_debut_indemnisation = KDPOD) %&gt;%\n  sdf_persist()\n\nduree &lt;- table_1 %&gt;%\n  summarise(duree_moy = mean(duree_potentielle_indemnisation),\n            duree_med = median(duree_potentielle_indemnisation)) %&gt;%\n  collect()\n\nSJR &lt;- table_1 %&gt;%\n  summarise(SJR_moy = mean(SJR),\n            SJR_med = median(SJR)) %&gt;%\n  collect()\n\n\n\nLorsqu‚Äôon charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.\nAttention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l‚Äôex√©cution.\n\n\nLe format .parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.\nSi les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d‚Äôune m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul\n\nspark_write_parquet(DE, partition_by = c(\"REGIND\"))\n\n\n\n\n\n\n\nExports simultan√©s\n\n\nHDFS supporte les exports simultan√©s, mais le temp d‚Äôexport est plus long lorsque le NameNode est requ√™t√© par plusieurs personnes simultan√©ment : d‚Äôapr√®s les tests cluster\n\npour un petit export (5 minutes), le temps peut √™tre multipli√© par 4 ;\npour un gros export (15 minutes), le temps peut √™tre multipli√© par 2."
  },
  {
    "objectID": "slides.html#forcer-le-calcul",
    "href": "slides.html#forcer-le-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Forcer le calcul",
    "text": "Forcer le calcul\nQuelques actions :\n\ncollecter la table enti√®re üõë\n\nspark_data_frame_1 %&gt;%\n  collect()\n\nafficher les premi√®res lignes\n\nspark_data_frame_1 %&gt;%\n  head(10)\n\nMettre les donner en cache\n\nspark_data_frame_1 %&gt;%\n  sdf_register() %&gt;%\n  tbl_cache()\n\nsc %&gt;% spark_session() %&gt;% invoke(\"catalog\") %&gt;% \n  invoke(\"clearCache\")"
  },
  {
    "objectID": "slides.html#les-erreurs-en-sparklyr",
    "href": "slides.html#les-erreurs-en-sparklyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les erreurs en sparklyr",
    "text": "Les erreurs en sparklyr\nsparklyr traduit le code dplyr fourni en scala, mais interpr√®te √©galement les messages d‚Äôerreurs envoy√©s du cluster vers la session R.\nsparklyr n‚Äôest cependant pas performant pour interpr√©ter ces erreurs.\nN‚Äôh√©sitez pas √† enregistrer le code g√©n√©rant un message d‚Äôerreur dans Documents publics/erreurs_sparklyr\nUn test du code pas-√†-pas permet d‚Äôisoler le probl√®me."
  },
  {
    "objectID": "slides.html#bonnes-pratiques",
    "href": "slides.html#bonnes-pratiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\n\nD√©connexion ou fermeture R pour lib√©rer les ressources üõë\nNe plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nPyspark ou Sparklyr pour la production ‚ùì\nUtilisation parcimonieuse des ressources ‚öñÔ∏è\nEnvoi des erreurs sparklyr üì©"
  },
  {
    "objectID": "slides.html#larchitecture-map-reduce",
    "href": "slides.html#larchitecture-map-reduce",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôarchitecture Map Reduce",
    "text": "L‚Äôarchitecture Map Reduce"
  },
  {
    "objectID": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "href": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La gestion de la m√©moire avec spark",
    "text": "La gestion de la m√©moire avec spark\nLes shuffles sont les op√©rations les plus gourmandes en temps.\n\n\n\n\n\n\nQu‚Äôest-ce qu‚Äôun shuffle ?\n\n\nUn shuffle est un √©change de donn√©es entre diff√©rents noeuds du cluster.\nNous avons vu qu‚Äôutiliser spark dans un cluster implique de distribuer √©galement le stockage des donn√©es.\nPar exemple :\n\nje demande un traitement sur la table PJC du FNA\nsi un noeud contenant d√©j√† les donn√©es de PJC est disponible, le cluster manager envoie le traitement √† ce noeud\nsi tous les noeuds contenant les donn√©es de PJC sont d√©j√† r√©serv√©s, alors le cluster manager demande le traitement √† un autre noeud, par exemple le noeud 1\nil demande √† un noeud contenant les donn√©es PJC, par exemple le noeud 4, d‚Äôenvoyer ces donn√©es au noeud 1 qui va ex√©cuter le traitement\ncet √©change de donn√©es est en r√©seau filaire : un √©change filaire est beaucoup plus lent qu‚Äôun envoi interne par le disque du noeud 1 √† la RAM du noeud 1\nc‚Äôest pourquoi pour optimiser un programme spark, il est possible de limiter les shuffles"
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "href": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire dans un worker",
    "text": "L‚Äôutilisation de la m√©moire dans un worker\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nNe pas charger plusieurs fois les m√™mes donn√©es en cache, ou si besoin augmenter la part de la m√©moire allou√©e au stockage avec spark.memory.storageFraction.\n\n\n\n\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.memory.fraction\"] &lt;- 0.8\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.memory.fraction\"] &lt;- 0.8\nconf[\"spark.memory.storageFraction\"] &lt;- 0.4\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)"
  },
  {
    "objectID": "slides.html#sparkui-un-outil-doptimisation",
    "href": "slides.html#sparkui-un-outil-doptimisation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "SparkUI : un outil d‚Äôoptimisation",
    "text": "SparkUI : un outil d‚Äôoptimisation\nSpark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d‚Äôoptimiser les traitements :\n\nDAGGCM√©moire\n\n\n\n\n\nV√©rifier que le gc time est inf√©rieur √† 10% du temps pour ex√©cuter la t√¢che ‚úÖ\n\n\n\nV√©rifier que la storage memory ne sature pas la m√©moire ‚úÖ"
  },
  {
    "objectID": "slides.html#utiliser-les-interfaces",
    "href": "slides.html#utiliser-les-interfaces",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Utiliser les interfaces",
    "text": "Utiliser les interfaces\n\nyarn : disponibilit√© des ressources\n\nSparkhistory pour des traitements de sessions ferm√©es\n\nLe sparkhistory entra√Æne l‚Äôenregistrement de logs assez lourdes, il est donc d√©sactiv√© par d√©faut. Pour l‚Äôactiver sur un programme :\n\nconf &lt;- spark_config()\nconf[\"spark.eventLog.enabled\"] &lt;- \"true\"\nconf[\"spark.eventLog.dir\"] &lt;- \"hdfs://midares-deb11-nn-01.midares.local:9000/spark-logs\"\nconf[\"appName\"] &lt;- \"un_nom_de_traitement\"\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)"
  },
  {
    "objectID": "slides.html#ma-session-ne-sinstancie-jamais",
    "href": "slides.html#ma-session-ne-sinstancie-jamais",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ma session ne s‚Äôinstancie jamais",
    "text": "Ma session ne s‚Äôinstancie jamais\nSi l‚Äôinstruction sc &lt;- spark_connect(master = \"yarn\", config = conf prend plus de 10 minutes, il est utile d‚Äôouvrir l‚Äôinterface de yarn pour v√©rifier que la file n‚Äôest pas d√©j√† enti√®rement occup√©e. L‚Äôerreur peut ne survenir qu‚Äôau bout d‚Äôune vingtaine de minutes : le job est ACCEPTED dans yarn, ou FAILED si la session n‚Äôa pas pu √™tre instanci√©e par manque de ressources disponibles."
  },
  {
    "objectID": "slides.html#exporter-de-hdfs-au-local",
    "href": "slides.html#exporter-de-hdfs-au-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Exporter de HDFS au local",
    "text": "Exporter de HDFS au local"
  },
  {
    "objectID": "slides.html#pyspark-mode-cluster",
    "href": "slides.html#pyspark-mode-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pyspark : mode cluster",
    "text": "Pyspark : mode cluster"
  },
  {
    "objectID": "slides.html#les-avantages-de-pyspark",
    "href": "slides.html#les-avantages-de-pyspark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les avantages de pyspark",
    "text": "Les avantages de pyspark\n\nMode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è\nSpark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è\nPlusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª\nStabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ\nLisibilit√© du code üëì\nTemps de connexion et d‚Äôex√©cution r√©duit ‚è≤Ô∏è\nUtilisation optimale de SparkUI üìä"
  },
  {
    "objectID": "slides.html#merci-pour-votre-attention",
    "href": "slides.html#merci-pour-votre-attention",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Merci pour votre attention !",
    "text": "Merci pour votre attention !"
  },
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "Sur le cluster\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\nLes ressources physiques utilis√©es :\n\npar le driver : avec spark.driver.memory\npar chaque worker avec spark.executor.memory et spark.executor.cores\nle nombre de worker avec spark.executor.instances\nLa file sur laquelle on travaille avec spark.yarn.queue\n\nle nombre de partitions de chaque spark_data_frame avec spark.sql.shuffle.partitions\nla limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec spark.driver.maxResultSize\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\nLes files du cluster spark Midares",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "bonnes_pratiques.html",
    "href": "bonnes_pratiques.html",
    "title": "Bonnes pratiques",
    "section": "",
    "text": "D√©connexion ou fermeture R pour lib√©rer les ressources üõë\nNe plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nPyspark ou Sparklyr pour la production ‚ùì\nUtilisation parcimonieuse des ressources ‚öñÔ∏è\nEnvoi des erreurs sparklyr : d√©p√¥t tel quel d‚Äôun code sparklyr qui g√©n√®re une erreur dans le dossier de l‚Äôespace commun (documents publics) erreurs_sparklyr üì©",
    "crumbs": [
      "Bonnes pratiques"
    ]
  },
  {
    "objectID": "slides_v.html#au-programme",
    "href": "slides_v.html#au-programme",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Au programme",
    "text": "Au programme\n\nMiDAS : une base de donn√©es volumineuse\nUtiliser MiDAS avec R : un d√©fi\nSparklyr : l‚Äôoutil ergonomique de spark en R\nOptimiser la m√©moire : pourquoi et comment\nLes bonnes pratiques\nPour aller plus loin",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#quest-ce-que-midas",
    "href": "slides_v.html#quest-ce-que-midas",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Qu‚Äôest-ce que MiDAS ?",
    "text": "Qu‚Äôest-ce que MiDAS ?",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#une-des-bases-les-plus-volumineuses-du-ssp",
    "href": "slides_v.html#une-des-bases-les-plus-volumineuses-du-ssp",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une des bases les plus volumineuses du SSP",
    "text": "Une des bases les plus volumineuses du SSP\n\nLes administrations dont les donn√©es sont comparables √† MiDAS utilisent un cluster Spark : Insee, Drees, Acoss‚Ä¶\n‚ñ∂Ô∏èLe cluster spark est une solution la tr√®s efficiente pour traiter des donn√©es de cette ampleur.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#concr√®tement-quest-ce-que-midas",
    "href": "slides_v.html#concr√®tement-quest-ce-que-midas",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Concr√®tement, qu‚Äôest-ce que MiDAS ?",
    "text": "Concr√®tement, qu‚Äôest-ce que MiDAS ?\n\n\n\n\n\n\n\nPourquoi Spark ?\n\n\nLa manipulation des donn√©es MiDAS en l‚Äô√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#o√π-est-midas-sur-la-bulle",
    "href": "slides_v.html#o√π-est-midas-sur-la-bulle",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π est MiDAS sur la bulle ?",
    "text": "O√π est MiDAS sur la bulle ?\nDisponible dans l‚Äôespace commun (= Documents publics) : C:\\Users\\Public\\Documents\\MiDAS_parquet\\Vague X\n\nAu format parquet :\n\ncompression efficace des donn√©es : taux de compression de 5 √† 10 par rapport au format csv\norient√© colonnes\nchargement efficace en m√©moire des donn√©es\nstockage partitionn√© des donn√©es avec write_dataset()\ntraiter des donn√©es sur disque\nind√©pendant du logiciel utilis√© : R, python, spark‚Ä¶",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-documentation-en-ligne",
    "href": "slides_v.html#la-documentation-en-ligne",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La documentation en ligne",
    "text": "La documentation en ligne\n\n\n\nDocumentation en ligne\n\nDictionnaire des donn√©es\nFiches pr√©sentant les concepts de l‚Äôindemnisation, du retour √† l‚Äôemploi\nExemples d‚Äôimpl√©mentation en R\nConseils quallit√© des variables",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#une-bulle-casd",
    "href": "slides_v.html#une-bulle-casd",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une bulle CASD",
    "text": "Une bulle CASD\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n512 Go de m√©moire vive (ou RAM) : passage √† 256 Go\nUn processeur (ou CPU) compos√© de 32 coeurs : passage √† 16 coeurs",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#une-bulle-casd-1",
    "href": "slides_v.html#une-bulle-casd-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une bulle CASD",
    "text": "Une bulle CASD\n\nLa m√©moire viveLe processeur\n\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) :\n\npar sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs\npar sa volatilit√© (toutes les donn√©es sont perdues si l‚Äôordinateur n‚Äôest plus aliment√©)\npar l‚Äôacc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.\n\n\n\nLe processeur :\n\npermet d‚Äôex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel\nest compos√© d‚Äôun ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu‚Äôune seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu‚Äôil a de coeurs\nse caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d‚Äôop√©rations qu‚Äôil est capable d‚Äôeffetuer par seconde.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#traiter-midas-en-r-les-limites",
    "href": "slides_v.html#traiter-midas-en-r-les-limites",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Traiter MiDAS en R : les limites",
    "text": "Traiter MiDAS en R : les limites\n\nCharger les donn√©es en m√©moire vive\n\n\n  path_fna &lt;- \"C:/Users/Public/Documents/MiDAS_parquet/Vague 4/FNA/\"\n  \n  PJC &lt;- read_parquet(paste0(path_fna, \"pjc.parquet\"), memory = TRUE)\n  ODD &lt;- read_parquet(paste0(path_fna, \"odd.parquet\"), memory = TRUE)\n\n\n\nR√©aliser des op√©rations co√ªteuses en ressources\n\n\njointure &lt;- PJC %&gt;%\n  rename(KROD1 = KROD3) %&gt;%\n  left_join(ODD, by = c(\"id_midas\", \"KROD1\"))\n\n\n\n\nLe partage des ressources de la bulle\n\nChaque utilisateur peut mobiliser toutes les ressouces de la bulle.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#traitement-l√©ger-versus-co√ªteux",
    "href": "slides_v.html#traitement-l√©ger-versus-co√ªteux",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Traitement l√©ger versus co√ªteux",
    "text": "Traitement l√©ger versus co√ªteux\n\nMAP = l√©gerREDUCE = co√ªteuxREDUCE en R\n\n\n\n\n\n\n\n\nCe traitement est peu co√ªteux :\n\nchargement d‚Äôune seule colonne en RAM : format parquet orient√© colonnes\npeu de m√©moire d‚Äôex√©cution : R est un langage vectoris√©\n\n\n\n\n\n\n\n\n\n\nCe traitement n√©cessite :\n\nle chargement de davantage de colonnes en m√©moire vive ;\ndavantage de m√©moire d‚Äôex√©cution pour effectuer l‚Äôintersection (inner_join()).\n\n\n\n\n\nles jointures\nles op√©rations en group_by()\ndistinct()\n‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n‚ñ∂Ô∏è Erreur ‚Äúout of memory‚Äù.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#pourquoi-spark-1",
    "href": "slides_v.html#pourquoi-spark-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pourquoi spark ?",
    "text": "Pourquoi spark ?\n\n\n\n\n\n\n\n\nSolution test√©e\nAvantage\nLimites rencontr√©e\n\n\n\n\nPackage ¬´ data.table ¬ª\nCalculs parall√©lis√©s\npour bases &lt; RAM\nSyntaxe tr√®s diff√©rente de dplyr\n\n\nFormat ¬´ parquet ¬ª +\npackage ¬´ arrow ¬ª\nStockage moins lourd\nChargement efficient\nTaille en m√©moire inchang√©e\n\n\nDuckDB\nGestionnaire de BDD\nPour des bases &lt; 100 Go\nFonctions et options non cod√©es\n\n\nSpark en mode local\nTraitements distribu√©s\nConsomme beaucoup de ressources\nInadapt√© pour une unique bulle\nN√©cessite le ¬´ collect() ¬ª",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#un-gain-de-temps-consid√©rable",
    "href": "slides_v.html#un-gain-de-temps-consid√©rable",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un gain de temps consid√©rable",
    "text": "Un gain de temps consid√©rable\n\n\n\n\n\n\n\n\n\n\nCalcul de la dur√©e moyenne d‚Äôun contrat\nRetour √† l‚Äôemploi salari√© des indemnisables\n\n\n\n\nClassique R\n4 heures\nCrash\n\n\nArrow + duckdb\n8 minutes\n3 heures seul sur la bulle\n\n\nArrow + spark local\n1 minute\n2 minutes\n\n\n\nMais alors, pourquoi le cluster ? ü§î",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#o√π-est-midas-2√®me-√©dition",
    "href": "slides_v.html#o√π-est-midas-2√®me-√©dition",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "O√π est Midas, 2√®me √©dition",
    "text": "O√π est Midas, 2√®me √©dition\nLe cluster a son propre explorateur de fichiers √† mettre en favori dans son navigateur : https://midares-deb11-nn-01.midares.local:9870/",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#un-cluster-de-calcul",
    "href": "slides_v.html#un-cluster-de-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un cluster de calcul",
    "text": "Un cluster de calcul",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#connexion",
    "href": "slides_v.html#connexion",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Connexion",
    "text": "Connexion\n\nTraitement l√©gerTraitement lourd\n\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"60Go\"\nconf[\"spark.executor.cores\"] &lt;- 4\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(dbplyr)\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"140Go\"\nconf[\"spark.executor.cores\"] &lt;- 8\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#chargement-des-donn√©es-en-spark",
    "href": "slides_v.html#chargement-des-donn√©es-en-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Chargement des donn√©es en spark",
    "text": "Chargement des donn√©es en spark\n\n\n### Depuis HDFS\nmmo_17_df_spark &lt;- spark_read_parquet(sc,\n                                  path = \"hdfs:///dataset/MiDAS_v4/mmo/mmo_2017.parquet\",\n                                  memory = FALSE)\n\n### Passer un dataframe R en spark\nmon_data_frame &lt;- data.frame(c(\"Anna\", \"Paul\"), c(15, 20))\nmon_data_frame_spark &lt;- copy_to(sc, \"mon_data_frame\")\n\n\n‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : par d√©faut, memory = FALSE",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#sparklyr-cest-comme-dplyr",
    "href": "slides_v.html#sparklyr-cest-comme-dplyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Sparklyr, c‚Äôest comme dplyr",
    "text": "Sparklyr, c‚Äôest comme dplyr\nEnsuite, vous pouvez programmer avec dplyr !\n\nmmo_17_df_spark &lt;- mmo_17_df_spark %&gt;%\n  rename(debut_contrat = DebutCTT) %&gt;%\n  filter(debut_contrat &gt;= as.Date(\"2017-01-01\") & debut_contrat &lt; as.Date(\"2017-02-01\")) %&gt;%\n  mutate(mois_debut_contrat = substr(debut_contrat,6,7))",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-lazy-evaluation",
    "href": "slides_v.html#la-lazy-evaluation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark distingue deux types d‚Äôop√©rations :\n\nles transformations : prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul\nPar exemple, le programme ci-dessous ne d√©clenche pas d‚Äôex√©cution :\n\n\nmmo_17_df_spark_mois &lt;- mmo_17_df_spark %&gt;%\n  rename(debut_contrat = DebutCTT) %&gt;%\n  filter(debut_contrat &gt;= as.Date(\"2017-01-01\") & debut_contrat &lt; as.Date(\"2017-06-01\")) %&gt;%\n  mutate(mois_debut_contrat = substr(debut_contrat,6,7))\n\n\nles actions : forcent le calcul d‚Äôun r√©sultat pour le r√©cup√©rer et d√©clenchent l‚Äôex√©cution de toutes les transformations compil√©es jusqu‚Äô√† l‚Äôappel de l‚Äôaction.\nPar exemple, le programme ci-dessous d√©clenche le calcul de toute la cellule pr√©c√©dente :\n\n\nnb_debut_contrat_fev_17 &lt;- mmo_17_df_spark_mois %&gt;%\n  group_by(mois_debut_contrat) %&gt;%\n  summarise(nb_contrats = n()) %&gt;%\n  print()",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#r√©cup√©rer-un-r√©sultat",
    "href": "slides_v.html#r√©cup√©rer-un-r√©sultat",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "R√©cup√©rer un r√©sultat",
    "text": "R√©cup√©rer un r√©sultat\nLes principales actions sont :\n\nprint()\ncollect()\nhead()\ntbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#presque-tout-comme-dplyr",
    "href": "slides_v.html#presque-tout-comme-dplyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "‚Ä¶ presque tout comme dplyr",
    "text": "‚Ä¶ presque tout comme dplyr\n\nDatesTableauStatistiques\n\n\nLes fonctions de lubridate()ne sont pas adapt√©es au spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nAdd_months\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\n\n\n\n\n\n\n\nFormat\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\nTri dans un groupe pour effectuer un calcul s√©quentiel\n\nODD_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                memory = FALSE)\n\nODD_premier &lt;- ODD_spark %&gt;%\n  group_by(id_midas) %&gt;%\n  window_order(id_midas, KDPOD) %&gt;%\n  mutate(date_premier_droit = first(KDPOD)) %&gt;%\n  ungroup() %&gt;%\n  distinct(id_midas, KROD3, date_premier_droit) %&gt;%\n  head(5)\n\nTri pour une sortie : sdf_sort() , arrange() ne fonctionne pas\nConcat√©ner les lignes (ou les colonnes sdf_bind_cols())\n\nODD_1 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &lt;= as.Date(\"2017-12-31\")) %&gt;%\n  mutate(groupe = \"temoins\")\n\nODD_2 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &gt;= as.Date(\"2021-12-31\")) %&gt;%\n  mutate(groupe = \"traites\")\n\nODD_evaluation &lt;- sdf_bind_rows(ODD_1, ODD_2)\n\nD√©doublonner une table\n\ndroits_dans_PJC &lt;- PJC_spark %&gt;%\n  sdf_distinct(id_midas, KROD3)\n\nprint(head(droits_dans_PJC, 5))\n\nPJC_dedoublonnee &lt;- PJC_spark %&gt;%\n  sdf_drop_duplicates()\n\nprint(head(PJC_dedoublonnee, 5))\n\nPivot : les fonctions du packag tidyr ne fonctionnent pas sur donn√©es spark\n\nODD_sjr_moyen &lt;- ODD_spark %&gt;%\n  mutate(groupe = ifelse(KDPOD &lt;= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %&gt;%\n  sdf_pivot(groupe ~ KCRGC,\n    fun.aggregate = list(KQCSJP = \"mean\")\n  )\n\n\n\n\n\nR√©sum√© statistique : sdf_describe() , summary()ne fonctionne pas.\nDimension : sdf_dim, la fonction nrow()ne fonctionne pas.\nQuantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, sdf_quantile()\nEchantillonnage al√©atoire : sdf_random_split",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#exporter-des-donn√©es",
    "href": "slides_v.html#exporter-des-donn√©es",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Exporter des donn√©es",
    "text": "Exporter des donn√©es\nExport des spark data frames directement sous HDFS.\n\nma_table &lt;- data.frame(c(\"Anne\", \"Paul\"), c(25,30))\n\nma_table_spark &lt;- copy_to(sc, ma_table)\n\nspark_write_parquet(ma_table_spark, \"hdfs:///resultats/ma_table.parquet\")\n\nPossibilit√© de r√©cup√©rer ce fichier sur la bulle MiDARES = en local.\n\n\n\n\n\n\nExports simultan√©s\n\n\nHDFS supporte les exports simultan√©s, mais le temp d‚Äôexport est plus long lorsque le NameNode est requ√™t√© par plusieurs personnes simultan√©ment : d‚Äôapr√®s les tests cluster\n\npour un petit export (5 minutes), le temps peut √™tre multipli√© par 4 ;\npour un gros export (15 minutes), le temps peut √™tre multipli√© par 2.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#si-on-souhaite-la-r√©cup√©rer-en-local",
    "href": "slides_v.html#si-on-souhaite-la-r√©cup√©rer-en-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Si on souhaite la r√©cup√©rer en local",
    "text": "Si on souhaite la r√©cup√©rer en local\n\n\n\n\n\n\nLes exports sur HDFS\n\n\nLorsqu‚Äôon exporte une table depuis notre session R vers HDFS, celle-ci est automatiquement partitionn√©e, comme le reste des donn√©es.\nAinsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\nIl est possible de ma√Ætriser le nombre de partitions avec la commande sdf_coalesce(partitions = 5) du package sparklyr.\nL‚Äôid√©al est d‚Äôadapter le nombre de partitions √† la taille d‚Äôun bloc : un bloc mesure 128 MB. Lorsqu‚Äôun bloc disque est utilis√©, m√™me √† 1%, il n‚Äôest pas utilisable pour un autre stockage.\nExporter un fichier de 1MB en 200 partitions r√©serve 200 blocs inutilement.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#t√©l√©charger-des-donn√©es-en-local",
    "href": "slides_v.html#t√©l√©charger-des-donn√©es-en-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "T√©l√©charger des donn√©es en local",
    "text": "T√©l√©charger des donn√©es en local",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#pas-besoin-doptimiser-son-code",
    "href": "slides_v.html#pas-besoin-doptimiser-son-code",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pas besoin d‚Äôoptimiser son code !",
    "text": "Pas besoin d‚Äôoptimiser son code !",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#la-m√©moire-du-driver",
    "href": "slides_v.html#la-m√©moire-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La m√©moire du driver",
    "text": "La m√©moire du driver",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#programmer-sans-collecter",
    "href": "slides_v.html#programmer-sans-collecter",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer sans collecter",
    "text": "Programmer sans collecter",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#cacher-une-table",
    "href": "slides_v.html#cacher-une-table",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Cacher une table",
    "text": "Cacher une table",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#spark-local-non",
    "href": "slides_v.html#spark-local-non",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Spark local : non",
    "text": "Spark local : non",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#inutile-de-prendre-toutes-les-ressources",
    "href": "slides_v.html#inutile-de-prendre-toutes-les-ressources",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Inutile de prendre toutes les ressources",
    "text": "Inutile de prendre toutes les ressources",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#fermer-sa-session",
    "href": "slides_v.html#fermer-sa-session",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Fermer sa session",
    "text": "Fermer sa session",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#mutualiser-les-exp√©riences",
    "href": "slides_v.html#mutualiser-les-exp√©riences",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mutualiser les exp√©riences",
    "text": "Mutualiser les exp√©riences",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#partitionnement",
    "href": "slides_v.html#partitionnement",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Partitionnement",
    "text": "Partitionnement\nLe format .parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.\nSi les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d‚Äôune m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul :\n\nspark_write_parquet(ma_table, \"hdfs:///resultats/ma_table.parquet\", partition_by = c(\"region\"))",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#sparkui",
    "href": "slides_v.html#sparkui",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "SparkUI",
    "text": "SparkUI",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#yarn",
    "href": "slides_v.html#yarn",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Yarn",
    "text": "Yarn",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides_v.html#pyspark",
    "href": "slides_v.html#pyspark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pyspark",
    "text": "Pyspark",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atelier d‚Äôinitiation √† spark avec R en mode cluster",
    "section": "",
    "text": "Bienvenue ! üëã\nCe site est une aide √† l‚Äôutilisation de spark avec R sur un cluster de calcul Spark √† destination de d√©butants en R ne connaissant pas du tout le fonctionnement et la programmation en spark.\nTu vas apprendre ici (je l‚Äôesp√®re üôÇ) :\n\nl‚Äôint√©r√™t d‚Äôutiliser Spark pour manipuler des donn√©es volumineuses telles que l‚Äôappariement MiDAS ‚è≥\nle fonctionnement de Spark sur un cluster de calcul üî¢\nl‚Äôutilisation de Spark sous R avec le package sparklyr, tr√®s facile si tu connais dplyr üë®‚Äçüíª\nquelques pistes d‚Äôoptimisation d‚Äôun programme avec Spark üí°\n\nDe quoi profiter ensuite du temps gagn√© gr√¢ce au calcul distribu√© ! üöÄ",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "dates.html",
    "href": "dates.html",
    "title": "Les dates avec sparklyr",
    "section": "",
    "text": "Les fonctions de lubridate()ne sont pas adapt√©es aux spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nFormat\n\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\n\n\nAdd months\n\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.",
    "crumbs": [
      "Les dates avec sparklyr"
    ]
  },
  {
    "objectID": "dates.html#dates",
    "href": "dates.html#dates",
    "title": "Les dates avec sparklyr",
    "section": "",
    "text": "Les fonctions de lubridate()ne sont pas adapt√©es aux spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nFormat\n\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\n\n\nAdd months\n\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.",
    "crumbs": [
      "Les dates avec sparklyr"
    ]
  },
  {
    "objectID": "oom.html",
    "href": "oom.html",
    "title": "Out of memory",
    "section": "",
    "text": "L‚Äôexport direct d‚Äôun spark data frame en parquet est quasiment instantann√© : tu peux ensuite charger la table parquet en data frame R si tu souhaites la traiter en m√©moire vive.\nCet export direct n‚Äôest pas fonctionnel en spark local : encore une raison de passer sur le cluster Spark !\nSi le collect()est in√©vitable, il faut que la m√©moire RAM allou√©e au driver soit assez importante pour r√©cup√©rer le r√©sultat : elle est param√©trable avec l‚Äôoption spark.driver.memory de la configuration spark. Lorsque tu utilises une session sparklyr, le driver est dans la bulle Midares, donc th√©oriquement la limite physique de RAM que tu peux allouer au driver correspond √† la taille de bulle (mais d‚Äôautres coll√®gues auront aussi besoin de ces ressources).\n\n\n\n\n\nSi tu as utilis√© des fonctions telles que compute() dans ton programme, elles peuvent √™tre √† l‚Äôorigine d‚Äôune erreur Out of memory : ces commandes ne sont pas bien adapt√©es √† Spark et les r√©sultats temporaires stock√©s en m√©moire vive ne sont pas visibles dans l‚Äôespace d√©di√© sur SparkUI onglet Storage, il semblerait que ces r√©sultats temporaires soient stock√©s ‚Äúau mauvais endroit‚Äù et occupent de l‚Äôespace sans que l‚Äôutilisateur y ait acc√®s.\nPrivil√©gier les fonctions tbl_cache() pour forcer l‚Äôex√©cution du programme.\n\n\n\n\n\nForcer l‚Äôex√©cution du programme le plus tard possible pour permettre √† Spark d‚Äôoptimiser tout le programme et d‚Äôutiiliser les ressources de la mani√®re la plus parcimonieuse possible.\nTester son programme sur une toute petite partie des tables, √©tape par √©tape, en for√ßant l‚Äôex√©cution (appel d‚Äôune action telle que print() ou collect() ) √† chaque √©tape pour la phase de d√©buggage, puis supprimer toutes les actions interm√©diaires non n√©cessaires du programme pour que Spark optimise tout le programme.\n\n\n\n\n\nSi une table est charg√©e avec la fonction spark_read_parquet(sc, path = \"mon_chemin_vers_la_table\", memory = FALSE) et l‚Äôoption memory=FALSE , les donn√©es ne seront charg√©es du disque √† la m√©moire cache qu‚Äôen cas de n√©cessit√©, c‚Äôest-√†-dire si une action d√©clenche des transformations qui utilisent une partie de ces donn√©es.\nCharger une table en cache avec memory = TRUE force la mise en cache de toutes les partitions de cette table, ce qui immobilise des ressources et peut favoriser la survenue d‚Äôerreurs Ouf of memory. Si cette table n‚Äôest pas utilis√©e en totalit√© plusieurs fois dans le programme, ce chargement n‚Äôest pas optimal.\nS‚Äôil s‚Äôav√®re optimal de charger les donn√©es en cache, alors il faut donner √† chaque ex√©cuteur une quantit√© de m√©moire vive suffisante pour laisser des ressources pour l‚Äôex√©cution.\nPour consulter la taille des donn√©es charg√©es en cache, utiliser SparkUI, onglet Storage.\n\n\n\n\nPar souci de parcimonie et pour faciliter le travail de nos coll√®gues sur des ressources partag√©es, cette option doit rester l‚Äôultime recours pour √©viter l‚Äôerreur Out of memory ü§ù",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#collecter-le-moins-possible",
    "href": "oom.html#collecter-le-moins-possible",
    "title": "Out of memory",
    "section": "",
    "text": "L‚Äôexport direct d‚Äôun spark data frame en parquet est quasiment instantann√© : tu peux ensuite charger la table parquet en data frame R si tu souhaites la traiter en m√©moire vive.\nCet export direct n‚Äôest pas fonctionnel en spark local : encore une raison de passer sur le cluster Spark !\nSi le collect()est in√©vitable, il faut que la m√©moire RAM allou√©e au driver soit assez importante pour r√©cup√©rer le r√©sultat : elle est param√©trable avec l‚Äôoption spark.driver.memory de la configuration spark. Lorsque tu utilises une session sparklyr, le driver est dans la bulle Midares, donc th√©oriquement la limite physique de RAM que tu peux allouer au driver correspond √† la taille de bulle (mais d‚Äôautres coll√®gues auront aussi besoin de ces ressources).",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#eviter-les-fonctions-sp√©cifiques-√†-sparklyr",
    "href": "oom.html#eviter-les-fonctions-sp√©cifiques-√†-sparklyr",
    "title": "Out of memory",
    "section": "",
    "text": "Si tu as utilis√© des fonctions telles que compute() dans ton programme, elles peuvent √™tre √† l‚Äôorigine d‚Äôune erreur Out of memory : ces commandes ne sont pas bien adapt√©es √† Spark et les r√©sultats temporaires stock√©s en m√©moire vive ne sont pas visibles dans l‚Äôespace d√©di√© sur SparkUI onglet Storage, il semblerait que ces r√©sultats temporaires soient stock√©s ‚Äúau mauvais endroit‚Äù et occupent de l‚Äôespace sans que l‚Äôutilisateur y ait acc√®s.\nPrivil√©gier les fonctions tbl_cache() pour forcer l‚Äôex√©cution du programme.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#laisser-spark-travailler-pour-nous",
    "href": "oom.html#laisser-spark-travailler-pour-nous",
    "title": "Out of memory",
    "section": "",
    "text": "Forcer l‚Äôex√©cution du programme le plus tard possible pour permettre √† Spark d‚Äôoptimiser tout le programme et d‚Äôutiiliser les ressources de la mani√®re la plus parcimonieuse possible.\nTester son programme sur une toute petite partie des tables, √©tape par √©tape, en for√ßant l‚Äôex√©cution (appel d‚Äôune action telle que print() ou collect() ) √† chaque √©tape pour la phase de d√©buggage, puis supprimer toutes les actions interm√©diaires non n√©cessaires du programme pour que Spark optimise tout le programme.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#optimiser-le-chargement-en-cache-des-donn√©es",
    "href": "oom.html#optimiser-le-chargement-en-cache-des-donn√©es",
    "title": "Out of memory",
    "section": "",
    "text": "Si une table est charg√©e avec la fonction spark_read_parquet(sc, path = \"mon_chemin_vers_la_table\", memory = FALSE) et l‚Äôoption memory=FALSE , les donn√©es ne seront charg√©es du disque √† la m√©moire cache qu‚Äôen cas de n√©cessit√©, c‚Äôest-√†-dire si une action d√©clenche des transformations qui utilisent une partie de ces donn√©es.\nCharger une table en cache avec memory = TRUE force la mise en cache de toutes les partitions de cette table, ce qui immobilise des ressources et peut favoriser la survenue d‚Äôerreurs Ouf of memory. Si cette table n‚Äôest pas utilis√©e en totalit√© plusieurs fois dans le programme, ce chargement n‚Äôest pas optimal.\nS‚Äôil s‚Äôav√®re optimal de charger les donn√©es en cache, alors il faut donner √† chaque ex√©cuteur une quantit√© de m√©moire vive suffisante pour laisser des ressources pour l‚Äôex√©cution.\nPour consulter la taille des donn√©es charg√©es en cache, utiliser SparkUI, onglet Storage.",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "oom.html#si-tout-ceci-ne-fonctionne-pas-augmenter-les-ressources-dans-la-configuration",
    "href": "oom.html#si-tout-ceci-ne-fonctionne-pas-augmenter-les-ressources-dans-la-configuration",
    "title": "Out of memory",
    "section": "",
    "text": "Par souci de parcimonie et pour faciliter le travail de nos coll√®gues sur des ressources partag√©es, cette option doit rester l‚Äôultime recours pour √©viter l‚Äôerreur Out of memory ü§ù",
    "crumbs": [
      "Out of memory : help !"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "ressources.html",
    "href": "ressources.html",
    "title": "Ressources",
    "section": "",
    "text": "Documentation CASD : Documentation Data Science\nDocumentation MiDAS : Lien vers le site\nDocumentation du package sparklyr : Package sparklyr",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "ressources.html#listes-imbriqu√©es-avec-spark",
    "href": "ressources.html#listes-imbriqu√©es-avec-spark",
    "title": "Ressources",
    "section": "Listes imbriqu√©es avec spark",
    "text": "Listes imbriqu√©es avec spark\nListe imbriqu√©es Lucie\nappend des bases reduce union -&gt; r√©cup√©rer le programme",
    "crumbs": [
      "Ressources"
    ]
  }
]