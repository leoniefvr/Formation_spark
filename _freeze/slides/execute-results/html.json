{
  "hash": "aa5bde45e121c93bdfb6ba887357c42b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Initiation √† Spark avec R en mode cluster\"\nformat: \n  revealjs:\n    incremental: true\n    slide-numbers : true\n---\n\n\n## Au programme {.smaller}\n\n1.  MiDAS : une base de donn√©es volumineuse üíæ\n\n2.  Manipuler un appariement : une op√©ration co√ªteuse üí≤\n\n3.  Initiation au calcul distribu√© : quelles ressources r√©server ? üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\n\n4.  Sparklyr : la solution ergonomique de spark sous R üë®‚Äçüíª\n\n5.  Pour aller plus loin ‚è©\n\n## MiDAS : une base de donn√©es volumineuse {.smaller}\n\nMiDAS croise trois bases de donn√©es administratives exhaustives :\n\n-   les donn√©es sur **l'inscription et l'indemnisation des demandeurs d'emploi** de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;\n\n-   les donn√©es sur les b√©n√©ficiaires de **minima sociaux** (RSA, PPA, AAH) et les caract√©ristiques des **m√©nages** de la CNAF : Allstat-FR6 ;\n\n-   les donn√©es sur les **contrats salari√©s** de la DSN : MMO de la Dares.\n\n## MiDAS : une base de donn√©es volumineuse {.smaller}\n\nChaque vague de MiDAS correspond √† environ **600 Go** de donn√©es au format sas. Les vagues fonctionnent par empilement :\n\n-   le gain de **profondeur temporelle** et l'entr√©e dans le champ de nouvelles personnes\n\n-   les **vagues sont appariables entre elles**\n\n## MiDAS : une base de donn√©es volumineuse {.smaller}\n\nMiDAS est l'une des bases de donn√©es les plus volumineuses du SSP :![Quelques bases du SSP](donnees_ssp.PNG)\n\nLes administrations dont les donn√©es sont comparables √† MiDAS utilisent un cluster Spark : Insee, Drees, Acoss...\n\n‚ñ∂Ô∏èLe cluster spark est la solution la plus efficiente pour traiter des donn√©es de cette ampleur. Apprendre √† l'utiliser pourra vous √™tre utile dans d'autres contextes que celui de la Dares.\n\n## Structure de l'appariement {.smaller}\n\n![](structure_midas.PNG){fig-align=\"center\"}\n\n::: callout-tip\n## Pourquoi Spark ?\n\nLa manipulation des donn√©es MiDAS en l'√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.\n:::\n\n## Le format parquet {.smaller .scrollable}\n\nLes donn√©es sont converties au **format parquet** d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l'espace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :\n\n-   il **compresse** efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv\n\n-   il est orient√© **colonnes**\n\n-   il permet le chargement efficace **en m√©moire** des donn√©es\n\n-   Il permet le **stockage partitionn√©** des donn√©es\n\n-   il permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires **sur disque**\n\n-   Il est **ind√©pendant du logiciel** utilis√© : il peut donc √™tre trait√© par spark et par R.\n\n# Manipuler un appariement : une op√©ration co√ªteuse\n\n## L'espace MiDares {.smaller .scrollable}\n\n::: panel-tabset\n### Ressources\n\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n-   512 Go de m√©moire vive (ou RAM) : passage √† 256 Go\n\n::: callout-note\n## La m√©moire vive\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa **rapidit√©**, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa **volatilit√©** (toutes les donn√©es sont perdues si l'ordinateur n'est plus aliment√©) et par l'acc√®s direct aux informations qui y sont stock√©es, **quasi instantann√©**.\n:::\n\n-   Un processeur (ou CPU) compos√© de 32 coeurs : passage √† 16 coeurs\n\n::: callout-note\n## Le processeur\n\nLe processeur permet d'**ex√©cuter des t√¢ches et des programmes** : convertir un fichier, ex√©cuter un logiciel... Il est compos√© d'un ou de plusieurs **coeurs** : un coeur ne peut ex√©cuter qu'une seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu'il a de coeurs. Un processeur se caract√©rise aussi par sa **fr√©quence** : elle est globalement proportionnelle au nombre d'op√©rations qu'il est capable d'effetuer par seconde.\n:::\n\n### Sch√©ma\n\n![](schema_ordinateur.png)\n:::\n\n## Programmer en m√©moire vive {.smaller}\n\n-   **R : la m√©moire vive, √©tat dans l'environnement**\n\n-   SAS : lecture/√©criture sur le disque\n\n-   MiDAS au format sas \\>\\> taille de la m√©moire vive disponible du serveur CASD --\\> format `.parquet`\n\n-   **Impossible de charger tout MiDAS en m√©moire vive**\n\n    Des solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :\n\n-   `arrow` (avec des requ√™tes `dplyr`)\n\n-   `duckDB` : recommand√© par le SSPLab pour des donn√©es jusqu'√† 100Go\n\n    ‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements.\n\n## Les traitements co√ªteux en puissance de calcul {.smaller}\n\n-   les jointures\n\n-   les op√©rations en `group_by()`\n\n-   `distinct()`\n\n    ‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n\n    ‚ñ∂Ô∏è Erreur \"out of memory\".\n\n## Un traitement peu co√ªteux : un traitement MAP {.smaller}\n\n![](formation%20sparklyr-Page-1.drawio.png){fig-align=\"center\" width=\"800\"}\n\nCe traitement est peu co√ªteux :\n\n-   chargement d'une seule colonne en RAM : format parquet orient√© colonnes\n\n-   peu de m√©moire d'ex√©cution : R est un langage vectoris√©\n\n## Un traitement co√ªteux : un traitement REDUCE {.smaller}\n\n![](formation%20sparklyr-Page-2.drawio.png){fig-align=\"center\" width=\"850\"}\n\nCe traitement n√©cessite :\n\n-   le chargement de davantage de colonnes en m√©moire vive ;\n\n-   davantage de m√©moire d'ex√©cution pour effectuer l'intersection (`inner_join()`).\n\n# Initiation au calcul distribu√©\n\n## Calcul distribu√© et calcul parall√®le {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n### Calcul non distribu√©\n\nLes probl√©matiques Big Data en R sont les suivantes :\n\n-   la taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R\n\n-   le temps de calcul : les √©tapes du traitement sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)\n\n-   l'optimisation du programme\n:::\n\n::: {.column width=\"50%\"}\n### Calcul distribu√© spark\n\nLe calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :\n\n-   chargement des donn√©es en m√©moire parcimonieux et non syst√©matique\n\n-   ex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents\n\n-   optimisation automatique du code\n:::\n:::\n\n## Un traitement MAP distribu√© {.smaller}\n\n![](map_distribue.drawio.png)\n\nSi les donn√©es sont stock√©es sur diff√©rents ordinateurs :\n\n-   les calculs peuvent √™tre effectu√©s en parall√®le ;\n\n-   gain de temps li√© √† l'augmentation des ressources informatiques pour effectuer le calcul et √† la parall√©lisation.\n\nLes traitements MAP se pr√™tent parfaitement au calcul distribu√© et parall√®le.\n\n## Un traitement REDUCE distribu√© {.smaller}\n\n![](reduce_distribue.drawio.png)\n\nSi les donn√©es sont stock√©es sur diff√©rents ordinateurs :\n\n-   il faut les rappatrier au m√™me endroit pour effectuer la jointure ;\n\n-   cet √©change est effectu√© en r√©seau entre les ordinateurs : l'envoi r√©seau a un co√ªt non n√©gligeable en temps.\n\nLes traitements REDUCE ne se pr√™tent pas bien au calcul distribu√© et parall√®le.\n\n## Spark {.smaller .scrollable}\n\n-   Apache Spark : **librairie open source** d√©velopp√©e dans le langage `scala`\n\n-   **Scala** : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    val TopHorrorsIGN2022 = Seq(\n      (9, \"Pearl\"),\n      (6, \"The Sadness\"),\n      (6, \"Offseason\"),\n      (7, \"Hatching\"),\n      (8, \"x\")\n    ).toDF(\"IMDB Rating\", \"IGN Movie Picks\")\n    \n    val TopHorrorsTheAVClub2022 = Seq(\n      (7, \"Nope\"),\n      (9, \"Pearl\"),\n      (8, \"x\"),\n      (5, \"Barbarian\"),\n      (5, \"Bones And All\")\n    ).toDF(\"IMDB Rating\", \"AVC Movie Picks\")\n    \n    import org.apache.spark.sql.functions.col\n    \n    val cols = List(col(\"IGN Movie Picks\"), col(\"AVC Movie Picks\"))\n    \n    val query = TopHorrorsIGN2022(\n      \"IGN Movie Picks\"\n    ) === TopHorrorsTheAVClub2022(\"AVC Movie Picks\")\n    \n    val outerJoin = TopHorrorsIGN2022\n      .join(TopHorrorsTheAVClub2022, query, \"outer\")\n      .select(cols: _*)\n    \n    outerJoin.show()\n    ```\n    :::\n\n\n-   `scala` adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de `spark` et optimiser au maximum les traitements en `spark`\n\n-   `spark` est **compatible avec les langages** `scala`, `R`, `python`, `java`, et peut interpr√©ter des commandes **SQL.**\n\n-   Deux packages existent sous R :\n\n    -   **sparkR** propos√© par Apache Spark\n\n    -   **sparklyr**, qui permet d'utiliser directement des commandes dplyr traduites en spark par le package.\n\n## Installation de spark sous CASD {.smaller}\n\nVoir la fiche d√©di√©e sur le site\n\n## La machine virtuelle Java {.smaller}\n\nSpark est r√©dig√© en scala, un langage qui a besoin d'une machine virtuelle Java pour √™tre ex√©cut√©. La machine virtuelle Java est scalable : l'utilisateur peut choisir quelles ressources physiques elle a le droit d'utiliser sur l'ensemble des ressources physiques disponibles sur l'ordinateur. C'est un mini ordinateur cr√©√© par spark √† l'int√©rieur de notre propre ordinateur, qui utilise les ressources de ce dernier.\n\n::: callout-note\n## Machine virtuelle\n\nUne machine virtuelle a les m√™mes caract√©ristiques qu'un ordinateur :\n\n-   un syst√®me d'exploitation : Windows, Linux, MacOS\n\n-   des ressources physiques : CPU, RAM et stockage disque\n\nLa diff√©rence avec un ordinateur : une machine virtuelle peut √™tre cr√©√©e sur un serveur physique en r√©servant une petite partie des ressources du serveur seulement, ce qui permet de cr√©er plusieurs ordinateurs diff√©rents sur une seule infractructure physique\n:::\n\n## Deux mani√®res d'utiliser Spark {.smaller .scrollable}\n\n::: columns\n::: {.column width=\"50%\"}\n### Avec un seul ordinateur\n\nCe mode est appel√© Spark local.\n\n-   une unique machine virtuelle Java est cr√©√©e pour ex√©cuter le code spark\n\n-   t√¢ches parall√©lis√©es sur les diff√©rents coeurs (CPU) du processeur de la machine virtuelle Java\n\n-   l'ordinateur sur lequel est cr√©√©e cette machine virtuelle Java est la bulle MiDARES, √©quivalent d'un unique gros ordinateur\n:::\n\n::: {.column width=\"50%\"}\n### Sur un cluster de calcul\n\nUn cluster de calcul est un ensemble d'ordinateurs ou machines virtuelles connect√©s en r√©seau.\n\n-   une machine virtuelle Java est cr√©√©e par spark dans chaque ordinateur du cluster\n\n-   t√¢ches parall√©lis√©es sur les diff√©rents ordinateurs du cluster\n\n-   la session R reste sur la bulle MiDARES, le code R est traduit en scala puis envoy√© sur le cluster pour √™tre ex√©cut√©.\n:::\n:::\n\n## Mode local : sch√©ma {.smaller}\n\n![](mode_local.PNG)\n\n## Mode local : √† √©viter {.smaller}\n\nEn mode local :\n\n-   les ressources utilis√©es par la machine virtuelle sont celles de la bulle\n\n-   il faut allouer suffisamment de coeurs √† la JVM pour parall√©liser\n\n-   m√™me si l'utilisateur choisit des ressources faibles, les ressources r√©elles utilis√©es dans une session spark peuvent √™tre plus √©lev√©es : mauvaise gestion de l'allocation des ressources\n\n-   acc√©l√©ration sensible par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources\n\n-   Sur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local : l'utilisation simultan√©e de spark par plusieurs membres de la bulle entra√Ænent des ralentissements consid√©rables\n\n    ‚ñ∂Ô∏èmode local √† √©viter absolument\n\n## Le cluster de calcul Midares : pr√©sentation {.smaller}\n\n![](schema_cluster.drawio.png){fig-align=\"center\" width=\"691\"}\n\n## Se connecter √† Spark sur un cluster {.smaller}\n\nSe connecter √† spark revient √† demander √† spark de cr√©er toutes les JVM demand√©es capables de lire du scala.\n\nPour se connecter √† spark depuis R avec le package `sparklyr` :\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nlibrary(sparklyr)\n\nconf <- spark_config()\nconf$spark.executor.instances <- 5\nsc <- spark_connect(master = \"yarn\", config = conf)\n```\n:::\n\n\nLe param√®tre `spark.executor.instances` correspond au nombre d'ordinateurs sur lequel on souhaite parall√©liser le travail d'ex√©cution de code. Ici, l'utilisateur demande 5 ordinateurs du cluster.\n\nNous verrons plus loin quels param√®tres nous devons pr√©ciser dans le fichier de configuration.\n\n## Une connexion {.smaller}\n\nToutes les JVM demand√©es (5) sont instanci√©es dans les ordinateurs du cluster, avec les param√®tres d√©finis.\n\n![](schema_cluster.drawio.png){fig-align=\"center\" width=\"691\"}\n\n## La vie d'un programme r√©dig√© en sparklyr {.smaller}\n\nAvec `sparklyr`, il est possible de programmer directement en `dplyr` pour utiliser spark.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# un data frame que j'envoie dans spark\nun_df <- data.frame(c(1,2,3), c(\"A\", \"B\", \"C\"))\nnames(un_df) <- c(\"col_num\", \"col_char\")\n\n# C'est maintenant un spark_data_frame\ncopy_to(sc, un_df)\n    \nun_df_transforme <- un_df %>%\n    mutate(une_nouvelle_col = col_num*2)\n```\n:::\n\n\nSi j'ex√©cute ce programme, je ne pourrai pas ouvrir `un_df_transforme`, d'ailleurs, il ne se sera rien pass√©.\n\n## La lazy evaluation {.smaller .scrollable}\n\nSpark distingue deux types d'op√©rations :\n\n-   **les transformations :** ce sont des op√©rations qui prennent en entr√©e un `spark_data_frame` et retournent un `spark_data_frame`, elles ne d√©clenchent aucun calcul lorsqu'elles sont appel√©es.\n\n    Par exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d'ex√©cution :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    une_transformation <- un_spark_data_frame %>%\n      group_by(identifiant) %>%\n      mutate(une_somme = sum(revenus))\n    ```\n    :::\n\n\n-   **les actions :** ce sont des op√©rations qui demandent le calcul d'un r√©sultat et qui d√©clenchent le calcul et l'ex√©cution de toutes les transformations compil√©es jusqu'√† l'appel de l'action.\n\n    Par exemple, le programme ci-dessous d√©clenche le calcul de la cellule `une_transformation` et de la moyenne des revenus :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    revenu_moyen <- une_transformation %>%\n      summarise(revenu_moyen = mean(une_somme)) %>%\n      print()\n    ```\n    :::\n\n\n    Les principales actions sont : `print()`, `collect()`, `head()`, `tbl_cache()` (√©crire un `spark_data_frame` en m√©moire pour le r√©utiliser).\n\n## La vie d'un programme r√©dig√© en sparklyr\n\nPrenons l'exemple d'un programme contenant une action.\n\n\n::: {.cell}\n\n:::\n\n\n## Le r√¥le du driver {.smaller}\n\n![](schema_cluster.drawio.png){fig-align=\"center\" width=\"400\"}\n\n-   Le programme R est traduit en scala gr√¢ce au package `sparklyr`\n\n-   Le driver √©value le programme, il lit le code `scala` mais n'ex√©cute rien du tout\n\n-   S'il remarque une erreur, l'erreur est envoy√©e directement √† l'utilisateur en session R avant l'ex√©cution du programme : c'est la force de la lazy evaluation.\n\n## Le plan d'ex√©cution {.smaller}\n\n![](catalyst.jpg)\n\nsource : documentation CASD disponible √† [Documentation Data Science](https://casd-eu.gitbook.io/data-science/)\n\nAJOUTER UN DAG\n\n## Le r√¥le du driver : Catalyst {.smaller .scrollable}\n\nLe driver contient un programme nomm√© Catalyst qui optimise le code `scala` automatiquement.\n\nSpark optimise automatiquement les programmes soumis :\n\n1.  Compilation des transformations pour soulever les √©ventuelles erreurs\n\n2.  Int√©gration dans un **plan d'ex√©cution** contenant les √©tapes n√©cessaires pour parvenir au r√©sultat demand√© par le programme\n\n3.  Optimisation du plan logique par le module **Catalyst** (driver Spark)\n\nPar exemple si j'√©cris le programme :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnon_optimal <- table_1 %>%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %>%   \n    filter(debut_contrat >= as.Date(\"2023-01-01\"))\n```\n:::\n\n\n```         \nCatalyst r√©√©crit :\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimal <- table_1 %>%   \n    filter(debut_contrat >= as.Date(\"2023-01-01\")) %>%   \n    mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))\n```\n:::\n\n\nCette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu'une action d√©clenche l'ex√©cution.\n\n## Le r√¥le du driver : Catalyst {.smaller .scrollable}\n\n![](dag.webp)\n\n## Le r√¥le du driver : Catalyst {.smaller .scrollable}\n\n4.  R√©alisation de plans physiques possibles et s√©lection du **meilleur plan physique** (au regard de la localisation des donn√©es requises). Le plan physique est la distribution des diff√©rents calculs aux machines du cluster.\n\n5.  **D√©clencher le moins d'actions possibles** dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.\n\n6.  Pour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique.\n\n## Le r√¥le du cluster manager {.smaller}\n\n![](schema_cluster.drawio.png){fig-align=\"center\" width=\"400\"}\n\nLe cluster manager distribue les traitements physiques aux ordinateurs du cluster :\n\n-   il conna√Æt le meilleur plan physique fourni par Catalyst ;\n\n-   il conna√Æt les ressources disponibles et occup√©es par toutes les machines du cluster ;\n\n-   il affecte les ressources disponibles √† la session spark.\n\n## Le r√¥le du worker {.smaller}\n\n![](schema_cluster.drawio.png){fig-align=\"center\" width=\"400\"}\n\nLe worker effectue le morceau de programme qu'on lui affecte et renvoie le r√©sultat au driver, qui lui-m√™me affiche le r√©sultat en session R :\n\n-   il ne conna√Æt que les t√¢ches qu'on lui a affect√©es ;\n\n-   il peut communiquer avec le driver en r√©seau pour renvoyer un r√©sultat ;\n\n-   il peut communiquer avec les autres workers en r√©seau pour partager des donn√©es ou des r√©sultats interm√©diaires : c'est un shuffle.\n\n## O√π sont les donn√©es ? {.smaller}\n\n![](schema_cluster.drawio.png){fig-align=\"center\" width=\"400\"}\n\n## O√π sont les donn√©es ?\n\n![](hdfs_browse.png)\n\n## Transfert de la bulle √† HDFS {.smaller}\n\n![](hdfs_dowload.PNG)\n\n## Transfert de HDFS √† la bulle {.smaller}\n\n![](hdfs_midas.PNG)\n\n## Mais o√π sont r√©ellement les donn√©es ? HDFS {.smaller}\n\nHadoop Distributed File System (HDFS)\n\n-   **stockage sur diff√©rentes machines :** ici les noeuds du cluster spark, c'est-√†-dire les diff√©rents ordinateurs workers du cluster\n\n-   donn√©es divis√©es **en blocs** plus petits de taille fixe et r√©partis sur les machines : aucune table de MiDAS n'existe en entier sur le cluster\n\n-   chaque bloc est **r√©pliqu√© trois fois** : il existe trois fois les 10 premi√®res lignes de la table FNA sur trois ordinateurs diff√©rents du cluster (r√©silience)\n\n-   un **NameNode** supervise les **m√©tadonn√©es** et g√®re la structure du syst√®me de fichiers\n\n-   les **DataNodes** stockent effectivement les blocs de donn√©es : les datanodes sont en fait les disques des workers du cluster, chaque ordinateur du cluster dispose d'un disque avec une partie des donn√©es MiDAS\n\n-   le **syst√®me HDFS** est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local\n\n# Programmer avec sparklyr\n\n## Param√©trer sa session {.smaller .scrollable}\n\nIl faut pr√©ciser quelles ressources r√©server pour chaque unit√© spark : le driver, le nombre d'ordinateurs workers (appel√©es instances), la RAM, le nombre de coeurs\n\nLa configuration par d√©faut est :\n\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\n-   Les ressources physiques utilis√©es :\n\n    1.  par le **driver** : avec `spark.driver.memory` **(avec parcimonie)**\n\n    2.  par chaque **worker** avec `spark.executor.memory`(valeur max **140 Go**) et `spark.executor.cores` (valeur max **8 coeurs**)\n\n    3.  le nombre de **workers** avec `spark.executor.instances` **(2 ou 3 suffisent)**\n\n    4.  La **file** sur laquelle on travaille avec `spark.yarn.queue` **(prod ou dev)**\n\n-   le nombre de **partitions** de chaque `spark_data_frame` avec `spark.sql.shuffle.partitions` **(200 par d√©faut)**\n\n-   la **limite de taille des r√©sulats** qui peuvent √™tre **collect√©s** par le driver avec `spark.driver.maxResultSize` **(0 est la meilleure option)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf <- spark_config()\nconf[\"spark.driver.memory\"] <- \"40Go\"\nconf[\"spark.executor.memory\"] <- \"60Go\"\nconf[\"spark.executor.cores\"] <- 4\nconf[\"spark.executor.instances\"] <- 2\ncont[\"spark.yarn.queue\"] <- \"prod\"\nconf[\"spark.driver.maxResultSize\"] <- 0\nconf[\"spark.sql.shuffle.partitions\"] <- 200\n\nsc <- spark_connect(master = \"yarn\", config = conf)\n```\n:::\n\n\n## Mode cluster : non concurrence gr√¢ce au cluster manager {.smaller}\n\n![](mode_cluster.PNG)\n\nLe mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d'un serveur. Ces machines communiquent en r√©seau.\n\nCapture d'√©cran r√©servation des ressources\n\nIl est donc n√©cessaire de se d√©connecter pour lib√©rer les ressources : des ressources r√©serv√©es, m√™me lorsqu'aucun programme ne tourne, ne peuvent jamais √™tre affect√©es √† d'autres utilisateurs.\n\n\n::: {.cell}\n\n:::\n\n\n## Importer les donn√©es depuis HDFS sous R {.smaller .scrollable}\n\nLes donn√©es doivent √™tre disponibles dans les workers sous forme de `spark_data_frame` :\n\n-   **cach√© en m√©moire directement** : si utilis√©es de tr√®s nombreuses fois pour gagner du temps\n\n-   laiss√© sur disque tant qu'aucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire\n\n    ‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.\n\n-   Pour passer un `data.frame` R en spark_data_frame : `copy_to()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npjc_df_spark <- spark_read_parquet(sc,\n                                  path = \"hdfs:///dataset/MiDAS_v4/FNA/pjc.parquet\",\n                                  memory = TRUE)\n\npjc_filtree <- pjc_df_spark %>%\n  filter(KDDPJ >= as.Date(\"2022-01-01\"))\n\nspark_write_parquet(pjc_filtree, \"hdfs:///tmp/pjc_filtree.parquet\")\n\npjc_df_spark <- copy_to(sc, \"PJC\")\n```\n:::\n\n\n## Les exports sur HDFS {.smaller .scrollable}\n\n::: callout-caution\n## Les exports sur HDFS\n\nLorsqu'on exporte une table depuis notre session R vers HDFS, celle-ci est **automatiquement partitionn√©e**, comme le reste des donn√©es.\n\nAinsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\n\nIl est possible de ma√Ætriser le nombre de partitions avec la commande `sdf_coalesce(partitions = 5)` du package `sparklyr`.\n\nL'id√©al est d'**adapter le nombre de partitions √† la taille d'un bloc** : un bloc mesure 128 MB. Lorsqu'un bloc disque est utilis√©, m√™me √† 1%, il n'est pas utilisable pour un autre stockage.\n\nExporter un fichier de 1MB en 200 partitions r√©serve 200 blocs inutilement.\n:::\n\n## Les shuffles {.smaller}\n\n![](reduce_distribue.drawio.png)\n\nComme nous l'avons vu, les traitements REDUCE ne se pr√™tent pas tr√®s bien au calcul distribu√© :\n\n-   augmenter le nombre de workers augmente la probabilit√© de devoir effectuer des shuffles\n\n-   il est recommand√© de se limiter √† deux workers comme dans la configuration propos√©e\n\n-   r√©server d'autres ressources n'est souvent pas efficient et monopolise les ressources pour les autres utilisateurs.\n\n## R√©cup√©rer un r√©sultat {.smaller}\n\nLes r√©sultats qu'il est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :\n\n-   **une table filtr√©e** avec les variables n√©cessaires √† l'√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;\n\n-   des **statistiques descriptives synth√©tiques ;**\n\n-   les **premi√®res lignes** de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;\n\n-   une **table agr√©g√©e** pour un graphique par exemple, √† l'aide de la fonction `summarise()`.\n\n## L'utilisation de la m√©moire du driver {.smaller}\n\n![](collect.drawio.png)\n\n## L'utilisation de la m√©moire du driver {.smaller .scrollable}\n\nLorsqu'il est n√©cessaire de collecter une table volumineuse, il faut donc pr√©voir assez de m√©moire RAM pour le driver.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\nconf <- spark_config()\nconf[\"spark.driver.memory\"] <- \"40Go\"\nconf[\"spark.executor.memory\"] <- \"80Go\"\nconf[\"spark.executor.cores\"] <- 5\nconf[\"spark.executor.instances\"] <- 2\ncont[\"spark.yarn.queue\"] <- \"prod\"\nconf[\"spark.driver.maxResultSize\"] <- 0\nconf[\"spark.sql.shuffle.partitions\"] <- 200\n\nsc <- spark_connect(master = \"yarn\", config = conf)\n```\n:::\n\n\n::: callout-caution\n## Bonne pratique de partage des ressources\n\nLe driver est instanci√© dans la bulle Midares, qui a vocation √† √™tre r√©duite suite √† la g√©n√©ralisation du cluster.\n\n-   La bulle Midares a besoin de RAM minimale pour fonctionner, 100% des ressources ne sont donc pas disponibles pour `sparklyr`.\n\n-   Pour permettre le **travail simultan√© fluide de 10 utilisateurs**, la m√©moire allou√©e au driver recommand√©e pour chaque utilisateur est de **15 Go**.\n\n-   **L'export d'une table** `sdf` directement au format `.parquet` est une alternative plus rapide, plus efficiente et qui permet par la suite de charger ses donn√©es en R classique et de travailler sur un `df` R sans utiliser `sparklyr`.\n:::\n\n## Comment tester son code pour collecter le moins possible ? {.smaller .scrollable}\n\nLa programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu'√† la fin pour que Catalyst optimise l'ensemble du programme\n\nLa principale diff√©rence avec la programmation en R classique est que **la visualisation de tables compl√®tes volumineuses n'est pas recommand√©e** :\n\n-   **goulets d'√©tranglement** m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;\n\n-   **longue :** √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;\n\n-   **beaucoup moins efficace que l'export direct en parquet** du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l'√©tude.\n\nS'il est n√©cessaire de collecter, il faut pr√©voir **beaucoup de RAM pour le driver avec le param√®tre** `spark.driver.memory`**.**\n\n# Sparklyr : la solution ergonomique de spark sous R\n\n## Ce qui change pour l'utilisateur {.smaller}\n\nLa majorit√© des commandes `dplyr` fonctionnent sur un spark_data_frame avec le package `sparklyr`. Les divergences principales sont les suivantes :\n\n| Fonctionnalit√©                 | tidyverse      | sparklyr                         |\n|--------------------------|-----------------|-----------------------------|\n| import d'un fichier `.parquet` | `read_parquet` | `spark_read_parquet()`           |\n| tri d'un tableau               | `arrange()`    | `window_order()` ou `sdf_sort()` |\n| op√©rations sur les dates       | `lubridate`    | fonctions Hive                   |\n| empiler des tableaux           | `bind_rows()`  | `sdf_bind_rows()`                |\n| nombre de lignes d'un tableau  | `nrow()`       | `sdf_nrow()`                     |\n| faire pivoter un tableau       | `tidyr`        | `sdf_pivot()`                    |\n| export d'un `spark_data_frame` |                | `spark_write_parquet()`          |\n\n## Quelques fonctions sp√©cifiques {.smaller .scrollable}\n\n::: panel-tabset\n## Dates\n\nLes fonctions de `lubridate()`ne sont pas adapt√©es au `spark_data_frames`.\n\n-   Convertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    date_1 <- as.Date(\"2024-05-26\")\n    ```\n    :::\n\n\n-   Calculer une dur√©e entre deux dates\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    PJC_spark <- spark_read_parquet(sc,\n                                    path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                    memory = FALSE)\n    \n    duree_pjc_df <- PJC_spark %>%\n      rename(date_fin_pjc = as.Date(KDFPJ),\n             date_deb_pjc = as.Date(KDDPJ)) %>%\n      mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %>%\n      head(5)\n    ```\n    :::\n\n\n-   Ajouter ou soustraire des jours ou des mois √† une date\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    duree_pjc_bis_df <- duree_pjc_df %>%\n      mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n             duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n             duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %>%\n      head(5)\n    ```\n    :::\n\n\n::: callout-note\n## Add_months\n\nSi la date en entr√©e est le dernier jour d'un mois, la date retourn√©e avec `add_months(date_entree, int(1))` sera le dernier jour calendaire du mois suivant.\n:::\n\n::: callout-tip\n## Format\n\nLe `int()` est important car ces fonctions Hive n'accepte que les entiers pour l'ajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n:::\n\n## Tableau\n\n-   Tri dans un groupe pour effectuer un calcul s√©quentiel\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ODD_spark <- spark_read_parquet(sc,\n                                    path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                    memory = FALSE)\n    \n    ODD_premier <- ODD_spark %>%\n      group_by(id_midas) %>%\n      window_order(id_midas, KDPOD) %>%\n      mutate(date_premier_droit = first(KDPOD)) %>%\n      ungroup() %>%\n      distinct(id_midas, KROD3, date_premier_droit) %>%\n      head(5)\n    ```\n    :::\n\n\n-   Tri pour une sortie : `sdf_sort()` , `arrange()` ne fonctionne pas\n\n-   Concat√©ner les lignes (ou les colonnes `sdf_bind_cols()`)\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ODD_1 <- ODD_spark %>%\n      filter(KDPOD <= as.Date(\"2017-12-31\")) %>%\n      mutate(groupe = \"temoins\")\n    \n    ODD_2 <- ODD_spark %>%\n      filter(KDPOD >= as.Date(\"2021-12-31\")) %>%\n      mutate(groupe = \"traites\")\n    \n    ODD_evaluation <- sdf_bind_rows(ODD_1, ODD_2)\n    ```\n    :::\n\n\n-   D√©doublonner une table\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    droits_dans_PJC <- PJC_spark %>%\n      sdf_distinct(id_midas, KROD3)\n    \n    print(head(droits_dans_PJC, 5))\n    \n    PJC_dedoublonnee <- PJC_spark %>%\n      sdf_drop_duplicates()\n    \n    print(head(PJC_dedoublonnee, 5))\n    ```\n    :::\n\n\n-   Pivot : les fonctions du packag `tidyr` ne fonctionnent pas sur donn√©es spark\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ODD_sjr_moyen <- ODD_spark %>%\n      mutate(groupe = ifelse(KDPOD <= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %>%\n      sdf_pivot(groupe ~ KCRGC,\n        fun.aggregate = list(KQCSJP = \"mean\")\n      )\n    ```\n    :::\n\n\n## Statistiques\n\n-   R√©sum√© statistique : `sdf_describe()` , `summary()`ne fonctionne pas.\n\n-   Dimension : `sdf_dim`, la fonction `nrow()`ne fonctionne pas.\n\n-   Quantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, `sdf_quantile()`\n\n-   Echantillonnage al√©atoire : `sdf_random_split`\n:::\n\n## Quelques tips d'optimisation {.smaller .scrollable}\n\n::: panel-tabset\n### Jointures\n\nPour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse\n\n![](join.png)\n\nSolution rapide :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable_finale <- table_volumineuse_comme_PJC %>%\n  right_join(petite_table_mon_champ)\n```\n:::\n\n\nSolution lente :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable_finale <- petite_table_mon_champ %>%\n  left_join(table_volumineuse_comme_PJC)\n```\n:::\n\n\n### Persist\n\nLorsqu'une table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c'est-√†-dire enregistrer ce `spark_data_frame`sur le disque ou dans la m√©moire des noeuds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable_1 <- mon_champ %>%\n  left_join(ODD, by = c(\"id_midas\", \"KROD3\")) %>%\n  rename(duree_potentielle_indemnisation = KPJDXP,\n         SJR = KQCSJP,\n         date_debut_indemnisation = KDPOD) %>%\n  sdf_persist()\n\nduree <- table_1 %>%\n  summarise(duree_moy = mean(duree_potentielle_indemnisation),\n            duree_med = median(duree_potentielle_indemnisation)) %>%\n  collect()\n\nSJR <- table_1 %>%\n  summarise(SJR_moy = mean(SJR),\n            SJR_med = median(SJR)) %>%\n  collect()\n```\n:::\n\n\n### Chargement\n\nLorsqu'on charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.\n\nAttention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l'ex√©cution.\n\n### Export et partitions\n\nLe format `.parquet` (avec `arrow`) et le framework `spark` permettent de g√©rer le partitionnement des donn√©es.\n\nSi les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d'une m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_write_parquet(DE, partition_by = c(\"REGIND\"))\n```\n:::\n\n\n::: callout-warning\n## Exports simultan√©s\n\nHDFS supporte les exports simultan√©s, mais le temp d'export est plus long lorsque le NameNode est requ√™t√© par plusieurs personnes simultan√©ment : d'apr√®s les tests cluster\n\n-   pour un petit export (5 minutes), le temps peut √™tre multipli√© par 4 ;\n\n-   pour un gros export (15 minutes), le temps peut √™tre multipli√© par 2.\n:::\n:::\n\n## Forcer le calcul {.smaller}\n\nQuelques actions :\n\n-   collecter la table enti√®re üõë\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_data_frame_1 %>%\n      collect()\n    ```\n    :::\n\n\n-   afficher les premi√®res lignes\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_data_frame_1 %>%\n      head(10)\n    ```\n    :::\n\n\n-   Mettre les donner en cache\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_data_frame_1 %>%\n      sdf_register() %>%\n      tbl_cache()\n    \n    sc %>% spark_session() %>% invoke(\"catalog\") %>% \n      invoke(\"clearCache\")\n    ```\n    :::\n\n\n## Les erreurs en sparklyr {.smaller}\n\n`sparklyr` traduit le code `dplyr` fourni en `scala`, mais interpr√®te √©galement les messages d'erreurs envoy√©s du cluster vers la session R.\n\n`sparklyr` n'est cependant pas performant pour interpr√©ter ces erreurs.\n\nN'h√©sitez pas √† enregistrer le code g√©n√©rant un message d'erreur dans Documents publics/erreurs_sparklyr\n\nUn test du code pas-√†-pas permet d'isoler le probl√®me.\n\n## Bonnes pratiques {.smaller}\n\n-   D√©connexion ou fermeture R pour lib√©rer les ressources üõë\n\n-   Ne plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\n\n-   Pyspark ou Sparklyr pour la production ‚ùì\n\n-   Utilisation parcimonieuse des ressources ‚öñÔ∏è\n\n-   Envoi des erreurs sparklyr üì©\n\n# Pour aller plus loin\n\n## L'architecture Map Reduce\n\n![](map_reduce.png)\n\n## La gestion de la m√©moire avec spark {.smaller .scrollable}\n\nLes **shuffles** sont les op√©rations les plus gourmandes en temps.\n\n::: callout-note\n## Qu'est-ce qu'un shuffle ?\n\nUn shuffle est un **√©change de donn√©es entre diff√©rents noeuds** du cluster.\n\nNous avons vu qu'utiliser `spark` dans un cluster implique de distribuer √©galement le stockage des donn√©es.\n\nPar exemple :\n\n1.  je demande un traitement sur la table PJC du FNA\n\n2.  si un noeud contenant d√©j√† les donn√©es de PJC est disponible, le cluster manager envoie le traitement √† ce noeud\n\n3.  si tous les noeuds contenant les donn√©es de PJC sont d√©j√† r√©serv√©s, alors le cluster manager demande le traitement √† un autre noeud, par exemple le noeud 1\n\n4.  il demande √† un noeud contenant les donn√©es PJC, par exemple le noeud 4, d'envoyer ces donn√©es au noeud 1 qui va ex√©cuter le traitement\n\n5.  cet √©change de donn√©es est en r√©seau filaire : un √©change filaire est beaucoup plus lent qu'un envoi interne par le disque du noeud 1 √† la RAM du noeud 1\n\n6.  c'est pourquoi pour optimiser un programme spark, il est possible de limiter les shuffles\n:::\n\n## L'utilisation de la m√©moire dans un worker {.smaller .scrollable}\n\n::: columns\n::: {.column width=\"50%\"}\n![](memoire_worker_1.drawio.png)\n:::\n\n::: {.column width=\"50%\"}\n![](memoire_worker_2.drawio.png)\n:::\n:::\n\n::: callout-tip\nNe pas charger plusieurs fois les m√™mes donn√©es en cache, ou si besoin augmenter la part de la m√©moire allou√©e au stockage avec `spark.memory.storageFraction`.\n:::\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nconf <- spark_config()\nconf[\"spark.driver.memory\"] <- \"40Go\"\nconf[\"spark.executor.memory\"] <- \"80Go\"\nconf[\"spark.memory.fraction\"] <- 0.8\nconf[\"spark.executor.cores\"] <- 5\nconf[\"spark.executor.instances\"] <- 2\ncont[\"spark.yarn.queue\"] <- \"prod\"\nconf[\"spark.driver.maxResultSize\"] <- 0\nconf[\"spark.sql.shuffle.partitions\"] <- 200\n\nsc <- spark_connect(master = \"yarn\", config = conf)\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nconf <- spark_config()\nconf[\"spark.driver.memory\"] <- \"40Go\"\nconf[\"spark.executor.memory\"] <- \"80Go\"\nconf[\"spark.memory.fraction\"] <- 0.8\nconf[\"spark.memory.storageFraction\"] <- 0.4\nconf[\"spark.executor.cores\"] <- 5\nconf[\"spark.executor.instances\"] <- 2\ncont[\"spark.yarn.queue\"] <- \"prod\"\nconf[\"spark.driver.maxResultSize\"] <- 0\nconf[\"spark.sql.shuffle.partitions\"] <- 200\n\nsc <- spark_connect(master = \"yarn\", config = conf)\n```\n:::\n\n:::\n:::\n\n## SparkUI : un outil d'optimisation {.smaller .scrollable}\n\nSpark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d'optimiser les traitements :\n\n::: panel-tabset\n## DAG\n\n![](dag.webp)\n\n## GC\n\nV√©rifier que le `gc time` est inf√©rieur √† 10% du temps pour ex√©cuter la t√¢che ‚úÖ\n\n![](gc.png)\n\n## M√©moire\n\nV√©rifier que la `storage memory` ne sature pas la m√©moire ‚úÖ\n\n![](gc.png)\n:::\n\n## Utiliser les interfaces {.smaller}\n\n-   **yarn** : disponibilit√© des ressources\n\n    ![](yarn_scheduler.PNG)\n\n-   **Sparkhistory** pour des traitements de sessions ferm√©es\n\nLe sparkhistory entra√Æne l'enregistrement de logs assez lourdes, il est donc d√©sactiv√© par d√©faut. Pour l'activer sur un programme :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf <- spark_config()\nconf[\"spark.eventLog.enabled\"] <- \"true\"\nconf[\"spark.eventLog.dir\"] <- \"hdfs://midares-deb11-nn-01.midares.local:9000/spark-logs\"\nconf[\"appName\"] <- \"un_nom_de_traitement\"\n\nsc <- spark_connect(master = \"yarn\", config = conf)\n```\n:::\n\n\n## Ma session ne s'instancie jamais {.smaller}\n\nSi l'instruction `sc <- spark_connect(master = \"yarn\", config = conf` prend plus de 10 minutes, il est utile d'ouvrir l'interface de yarn pour v√©rifier que la file n'est pas d√©j√† enti√®rement occup√©e. L'erreur peut ne survenir qu'au bout d'une vingtaine de minutes : le job est `ACCEPTED` dans yarn, ou `FAILED` si la session n'a pas pu √™tre instanci√©e par manque de ressources disponibles.\n\n![](yarn_accepted.jpg)\n\n## Exporter de HDFS au local {.smaller}\n\n::: r-stack\n![](hdfs_browse.png){.fragment width=\"1000\" height=\"700\"}\n\n![](hdfs_download.png){.fragment width=\"1000\" height=\"700\"}\n:::\n\n## Pyspark : mode cluster\n\n![](pyspark.drawio.png)\n\n## Les avantages de pyspark {.smaller}\n\n-   Mode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è\n\n-   Spark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è\n\n-   Plusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª\n\n-   Stabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ\n\n-   Lisibilit√© du code üëì\n\n-   Temps de connexion et d'ex√©cution r√©duit ‚è≤Ô∏è\n\n-   Utilisation optimale de SparkUI üìä\n\n## Merci pour votre attention !\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}