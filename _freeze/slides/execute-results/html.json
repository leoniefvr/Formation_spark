{
  "hash": "11d795e5d858d55fa05fc02e1f13242a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Initiation √† Spark sous CASD\"\nformat: \n  revealjs:\n    incremental: true\n    slide-numbers : true\n---\n\n\n## Au programme {.smaller}\n\n1.  MiDAS : une base de donn√©es volumineuse üíæ\n\n2.  Manipuler un appariement : une op√©ration co√ªteuse üí≤\n\n3.  Initiation au calcul distribu√© üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\n\n4.  Sparklyr : la solution ergonomique de sparkl sous R üë®‚Äçüíª\n\n5.  Pour aller plus loin ‚è©\n\n## MiDAS : une base de donn√©es volumineuse {.smaller}\n\nMiDAS croise trois bases de donn√©es administratives exhaustives :\n\n-   les donn√©es sur **l'inscription et l'indemnisation des demandeurs d'emploi** de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;\n\n-   les donn√©es sur les b√©n√©ficiaires de **minima sociaux** (RSA, PPA, AAH) et les caract√©ristiques des **m√©nages** de la CNAF : Allstat-FR6 ;\n\n-   les donn√©es sur les **contrats salari√©s** de la DSN : MMO de la Dares.\n\n## MiDAS : une base de donn√©es volumineuse {.smaller}\n\nChaque vague de MiDAS correspond √† environ **600 Go** de donn√©es au format sas. Les vagues fonctionnent par empilement :\n\n-   le gain de **profondeur temporelle** et l'entr√©e dans le champ de nouvelles personnes\n\n-   **vagues sont appariables entre elles**\n\n## MiDAS : une base de donn√©es volumineuse {.smaller}\n\nMiDAS est l'une des bases de donn√©es les plus volumineuses du SSP :![Quelques bases du SSP](donnees_ssp.PNG)\n\n## Structure de l'appariement {.smaller}\n\n![](structure_midas.PNG){fig-align=\"center\"}\n\n::: callout-tip\n## Pourquoi Spark ?\n\nLa manipulation des donn√©es MiDAS en l'√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.\n:::\n\n## Le format parquet {.smaller .scrollable}\n\nLes donn√©es sont converties au **format parquet** d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l'espace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :\n\n-   il **compresse** efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv\n\n-   il est orient√© **colonnes**\n\n-   il permet le chargement efficace **en m√©moire** des donn√©es\n\n-   Il permet le **stockage partitionn√©** des donn√©es\n\n-   il permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires **sur disque**\n\n-   Il est **ind√©pendant du logiciel** utilis√© : il peut donc √™tre trait√© par spark et par R.\n\n# Manipuler un appariement : une op√©ration co√ªteuse\n\n## L'espace MiDares {.smaller .scrollable}\n\n::: panel-tabset\n### Ressources\n\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n-   512 Go de m√©moire vive (ou RAM)\n\n::: callout-note\n## La m√©moire vive\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa volatilit√© (toutes les donn√©es sont perdues si l'ordinateur n'est plus aliment√©) et par l'acc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.\n:::\n\n-   Un processeur (ou CPU) compos√© de 32 coeurs\n\n::: callout-note\n## Le processeur\n\nLe processeur permet d'ex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel... Il est compos√© d'un ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu'une seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu'il a de coeurs. Un processeur se caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d'op√©ration qu'il est capable d'effetuer par seconde.\n:::\n\n### Sch√©ma\n\n![](schema_ordinateur.png)\n:::\n\n## Programmer en m√©moire vive {.smaller}\n\n-   **R : la m√©moire vive, √©tat dans l'environnement**\n\n-   SAS : lecture/√©criture sur le disque\n\n-   MiDAS au format sas \\>\\> taille de la m√©moire vive disponible du serveur CASD --\\> format parquet\n\n-   **Impossible de charger tout MiDAS en m√©moire vive**\n\n    Des solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :\n\n-   arrow (avec des requ√™tes dplyr)\n\n-   duckDB\n\n    ‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements.\n\n## Les traitements co√ªteux en puissance de calcul {.smaller}\n\n-   les jointures\n\n-   les op√©rations en group_by\n\n-   distinct\n\n    ‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n\n    ‚ñ∂Ô∏è Erreur \"out of memory\".\n\n## Un traitement peu co√ªteux {.smaller}\n\n![](formation%20sparklyr-Page-1.drawio.png){fig-align=\"center\" width=\"800\"}\n\nCe traitement est peu co√ªteux :\n\n-   chargement d'une seule colonne en RAM : format parquet orient√© colonnes\n\n-   peu de m√©moire d'ex√©cution : R est un langage vectoris√©\n\n## Un traitement co√ªteux {.smaller}\n\n![](formation%20sparklyr-Page-2.drawio.png){fig-align=\"center\" width=\"850\"}\n\nCe traitement n√©cessite :\n\n-   le chargement de davantage de colonnes en m√©moire vive ;\n\n-   davantage de m√©moire d'ex√©cution pour effectuer l'intersection (inner_join).\n\n# Initiation au calcul distribu√©\n\n## Calcul distribu√© et calcul parall√®le {.smaller}\n\n::: panel-tabset\n### Calcul non distribu√©\n\nLorsqu'un traitement Big Data est demand√© par l'utilisateur dans la session R, plusieurs probl√®mes peuvent se poser :\n\n-   la taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R\n\n-   le temps de calcul : si plusieurs √©tapes sont n√©cessaires pour un traitement, elles sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)\n\n-   l'optimisation du programme\n\n### Calcul distribu√© avec spark\n\nLe calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :\n\n-   chargement des donn√©es en m√©moire parcimonieux et non syst√©matique\n\n-   ex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents\n\n-   optimisation automatique du code\n:::\n\n## Le cluster de calcul Midares : mode interactif {.smaller}\n\n![](schema_cluster.drawio.png){fig-align=\"center\" width=\"691\"}\n\n## Spark {.smaller}\n\n-   Apache Spark : **librairie open source** d√©velopp√©e dans le langage Scala\n\n-   **Scala** : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java\n\n    Exemple Scala ?\n\n-   Scala adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de Spark et optimiser au maximum les traitements en spark\n\n-   Spark est **compatible avec les langages Scala, R, Python, Java**, et peut interpr√©ter des commandes **SQL.**\n\n-   Deux packages existent sous R :\n\n    -   **sparkR** propos√© par Apache Spark\n\n    -   **sparklyr**, qui permet d'utiliser directement des commandes dplyr traduites en spark par le package.\n\n## Mode local : concurrence {.smaller}\n\n![](mode_local.PNG)\n\n## Mode local : concurrence  {.smaller}\n\nEn mode local :\n\n-   une unique machine Java\n\n-   parall√©lisation des t√¢ches sur diff√©rents coeurs de cette machine virtuelle\n\n-   pas de stockage distribu√©, ca n'est pas du calcul distribu√© √† proprement parler\n\n-   acc√©l√©ration par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources\n\n-   Sur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local\n\n    ‚ñ∂Ô∏èmode local √† √©viter absolument\n\n## Mode cluster : non concurrence {.smaller}\n\n![](mode_cluster.PNG)\n\nLe mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d'un serveur. Ces machines communiquent en r√©seau.\n\n## Installation de spark sous CASD\n\nVoir la fiche d√©dier sur le site\n\n## Le stockage distribu√© : HDFS {.smaller}\n\n-   **stockage sur diff√©rentes machines :** ici les noeuds du cluster spark, c'est-√†-dire les diff√©rents ordinateurs workers du cluster\n\n-   donn√©es divis√©es **en blocs** plus petits de taille fixe et r√©partis sur les machines\n\n-   chaque bloc est **r√©pliqu√© trois fois** pour √™tre r√©silient face aux pannes\n\n-   un **NameNode** supervise les **m√©tadonn√©es** et g√®re la structure du syst√®me de fichiers\n\n-   les **DataNodes** stockent effectivement les blocs de donn√©es\n\n-   le **syst√®me HDFS** est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local\n\n    ::: callout-caution\n    ## Les exports sur HDFS\n\n    Lorsqu'on exporte une table depuis notre session R vers HDFS, celle-ci est **automatiquement partitionn√©e**, comme le reste des donn√©es.\n\n    Ainsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\n\n    Il est possible de ma√Ætriser le nombre de partitions avec la commande **sdf_coalesce**(partitions = 5) du package sparklyr.\n    :::\n\n## Le stockage distribu√© : HDFS {.smaller .scrollable}\n\n![](stockage_distribue.drawio.png){fig-align=\"center\" width=\"2000\"}\n\n‚ñ∂Ô∏è Les r√©plications de donn√©es ont deux fonctions :\n\n-   augementer la **flexibilit√© de la distribution** des traitements\n\n-   augmenter la **r√©silience** en cas de panne d'un noeud\n\n## La lazy evaluation {.smaller .scrollable}\n\nSpark distingue deux types d'op√©rations :\n\n-   **les transformations :** ce sont des op√©rations qui prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul lorsqu'elles sont appel√©es.\n\n    Par exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d'ex√©cution :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    une_transformation <- un_spark_data_frame %>%\n      group_by(identifiant) %>%\n      mutate(une_somme = sum(revenus))\n    ```\n    :::\n\n\n-   **les actions :** ce sont des op√©rations qui demandent le calcul d'un r√©sultat et qui d√©clenchent le calcul et l'ex√©cution de toutes les transformations compil√©es jusqu'√† l'appel de l'action.\n\n    Par exemple, le programme ci-dessous d√©clenche le calcul de la cellule \\`une_transformation\\` et de la moyenne des revenus :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    revenu_moyen <- une_transformation %>%\n      summarise(revenu_moyen = mean(une_somme)) %>%\n      print()\n    ```\n    :::\n\n\n    Les principales actions sont : print(), collect(), head(), tbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser).\n\n## La lazy evaluation {.smaller .scrollable}\n\nSpark optimise automatiquement les programmes soumis :\n\n1.  Compilation des transformations\n\n2.  Int√©gration dans un plan d'ex√©cution : √©ventuelles erreurs du programme soulev√©es avant l'ex√©cution\n\n3.  Optimisation du plan logique par le module Catalyst (driver Spark)\n\n    Par exemple si j'√©cris le programme :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    non_optimal <- table_1 %>%\n      mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %>%\n      filter(debut_contrat >= as.Date(\"2023-01-01\"))\n    ```\n    :::\n\n\n    Catalyst r√©√©crit :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    non_optimal <- table_1 %>%\n      filter(debut_contrat >= as.Date(\"2023-01-01\")) %>%\n      mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))\n    ```\n    :::\n\n\n    Cette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu'une action d√©clenche l'ex√©cution.\n\n4.  D√©clencher le moins d'actions possibles dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.\n\n5.  Pour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique.\n\n## Le plan d'ex√©cution {.smaller}\n\n![](catalyst.jpg)\n\n## R√©cup√©rer un r√©sultat {.smaller}\n\nLes r√©sultats qu'il est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :\n\n-   **une table filtr√©e** avec les variables n√©cessaires √† l'√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;\n\n-   des **statistiques descriptives synth√©tiques ;**\n\n-   les **premi√®res lignes** de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;\n\n-   une **table agr√©g√©e** pour un graphique par exemple, √† l'aide de la fonction summarise().\n\n# Sparklyr : la solution ergonomique de spark sous R\n\n## Sparklyr et SparkR {.smaller}\n\nDeux packages permettent de programmer avec Spark sous R :\n\n-   **SparkR :** ce package, maintenu par Apache Spark, permet d'utiliser une syntaxe proche de spark, scala, ou directement du code SQL pour manipuler des donn√©es dans une session R.\n\n-   **Sparklyr :** ce package permet d'utiliser directement la syntaxe dplyr dans une session Spark sous R.\n\n**Sparklyr** fonctionne selon ces √©tapes :\n\n1.  La **JVM driver spark est instanci√©e dans la bulle Midares** pour utiliser sparklyr.\n\n2.  Les instructions dplyr appel√©es sur un spark_data_frame sont **traduites par les fonctions du package sparklyr en scala**, puis envoy√©es au driver.\n\n3.  Le programme en scala est ex√©cut√© sur le **cluster**.\n\n4.  Si une **erreur** est renvoy√©e par le driver, elle est interpr√©t√©e par R avant d'√™tre affich√©e en session R.\n\n## Configuration cluster {.smaller .scrollable}\n\nDeux √©tapes majeures dans le traitement de donn√©es sous R diff√®re en sparklyr par rapport √† une programmation classique en dplyr :\n\n::: panel-tabset\n### Configuration\n\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\n-   Les ressources physiques utilis√©es :\n\n    1.  par le driver : avec **spark.driver.memory**\n\n    2.  par chaque worker avec **spark.executor.memory** et **spark.executor.cores**\n\n    3.  le nombre de worker avec **spark.executor.instances**\n\n    4.  La file sur laquelle on travaille avec **spark.yarn.queue**\n\n-   le nombre de partitions de chaque spark_data_frame avec **spark.sql.shuffle.partitions**\n\n-   la limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec **spark.driver.maxResultSize**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf <- spark_config()\nconf[\"spark.driver.memory\"] <- \"40Go\"\nconf[\"spark.executor.memory\"] <- \"80Go\"\nconf[\"spark.executor.cores\"] <- 5\nconf[\"spark.executor.instances\"] <- 2\ncont[\"spark.yarn.queue\"] <- \"prod\"\nconf[\"spark.driver.maxResultSize\"] <- 0\nconf[\"spark.sql.shuffle.partitions\"] <- 200\n\nsc <- spark_connect(master = \"yarn\", config = conf)\n```\n:::\n\n\n### Import-export\n\nLes donn√©es doivent √™tre disponibles dans les workers sous forme de **spark_data_frame** :\n\n-   cach√© en m√©moire directement : si utilis√©es plusieurs fois pour gagner du temps\n\n-   laiss√© sur disque tant qu'aucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire\n\n    ‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.\n\n-   Pour passer un data.frame R en spark_data_frame : **copy_to()**\n\n\n::: {.cell}\n\n:::\n\n:::\n\n## L'utilisation de la m√©moire dans un worker {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n![](memoire_worker_1.drawio.png)\n:::\n\n::: {.column width=\"50%\"}\n![](memoire_worker_2.drawio.png)\n:::\n:::\n\n::: callout-tip\nNe pas charger plusieurs fois les m√™mes donn√©es en cache\n:::\n\n## L'utilisation de la m√©moire du driver {.smaller}\n\n![](collect.drawio.png)\n\n## Ce qui change pour l'utilisateur {.smaller}\n\nLa majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences sont les suivantes :\n\n-   pour effectuer des op√©rations avec les dates, il faut utiliser les fonctions Hive sp√©cifiques.\n\n-   arrange() ne fonctionne pas sur un spark_data_frame, il faut lui substituer window_order.\n\n-   des fonctions sp√©cifiques aux spark data frames : sdf_bind_rows pour empiler les lignes par exemple.\n\n## Quelques fonctions sp√©cifiques {.smaller .scrollable}\n\n::: panel-tabset\n## Dates\n\nLes fonctions de `lubridate()`ne sont pas adapt√©es au spark_data_frames.\n\n-   Convertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    date_1 <- as.Date(\"2024-05-26\")\n    ```\n    :::\n\n\n-   Calculer une dur√©e entre deux dates\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    PJC_spark <- spark_read_parquet(sc,\n                                    path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                    memory = FALSE)\n    \n    duree_pjc_df <- PJC_spark %>%\n      rename(date_fin_pjc = as.Date(KDFPJ),\n             date_deb_pjc = as.Date(KDDPJ)) %>%\n      mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %>%\n      head(5)\n    ```\n    :::\n\n\n-   Ajouter ou soustraire des jours ou des mois √† une date\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    duree_pjc_bis_df <- duree_pjc_df %>%\n      mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n             duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n             duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %>%\n      head(5)\n    ```\n    :::\n\n\n::: callout-note\n## Add_months\n\nSi la date en entr√©e est le dernier jour d'un mois, la date retourn√©e avec `add_months(date_entree, int(1))` sera le dernier jour calendaire du mois suivant.\n:::\n\n::: callout-tip\n## Format\n\nLe int() est important car ces fonctions Hive n'accepte que les entiers pour l'ajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n:::\n\n## Tableau\n\n-   Tri dans un groupe pour effectuer un calcul s√©quentiel\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ODD_spark <- spark_read_parquet(sc,\n                                    path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                    memory = FALSE)\n    \n    ODD_premier <- ODD_spark %>%\n      group_by(id_midas) %>%\n      window_order(id_midas, KDPOD) %>%\n      mutate(date_premier_droit = first(KDPOD)) %>%\n      ungroup() %>%\n      distinct(id_midas, KROD3, date_premier_droit) %>%\n      head(5)\n    ```\n    :::\n\n\n-   Tri pour une sortie : `sdf_sort()` , `arrange()` ne fonctionne pas\n\n-   Concat√©ner les lignes (ou les colonnes `sdf_bind_cols()`)\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ODD_1 <- ODD_spark %>%\n      filter(KDPOD <= as.Date(\"2017-12-31\")) %>%\n      mutate(groupe = \"temoins\")\n    \n    ODD_2 <- ODD_spark %>%\n      filter(KDPOD >= as.Date(\"2021-12-31\")) %>%\n      mutate(groupe = \"traites\")\n    \n    ODD_evaluation <- sdf_bind_rows(ODD_1, ODD_2)\n    ```\n    :::\n\n\n-   D√©doublonner une table\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    droits_dans_PJC <- PJC_spark %>%\n      sdf_distinct(id_midas, KROD3)\n    \n    print(head(droits_dans_PJC, 5))\n    \n    PJC_dedoublonnee <- PJC_spark %>%\n      sdf_drop_duplicates()\n    \n    print(head(PJC_dedoublonnee, 5))\n    ```\n    :::\n\n\n-   Pivot : les fonctions du packag `tidyr` ne fonctionnent pas sur donn√©es spark\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ODD_sjr_moyen <- ODD_spark %>%\n      mutate(groupe = ifelse(KDPOD <= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %>%\n      sdf_pivot(groupe ~ KCRGC,\n        fun.aggregate = list(KQCSJP = \"mean\")\n      )\n    ```\n    :::\n\n\n## Statistiques\n\n-   R√©sum√© statistique : `sdf_describe()` , `summary()`ne fonctionne pas.\n\n-   Dimension : `sdf_dim`, la fonction `nrow()`ne fonctionne pas.\n\n-   Quantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, `sdf_quantile()`\n\n-   Echantillonnage al√©atoire : `sdf_random_split`\n:::\n\n## Une r√®gle d'or : tester son code pour collecter le moins possible {.smaller .scrollable}\n\nLa programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu'√† la fin pour que Catalyst optimise l'ensemble du programme\n\nLa principale diff√©rence avec la programmation en R classique est que **la visualisation de tables compl√®tes volumineuses n'est pas recommand√©e** :\n\n-   **goulets d'√©tranglement** m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;\n\n-   **longue :** √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;\n\n-   **beaucoup moins efficace que l'export direct en parquet** du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l'√©tude.\n\nS'il est n√©cessaire de collecter, il faut pr√©voir **beaucoup de RAM pour le driver avec le param√®tre \"spark.driver.memory\".**\n\n## Quelques tips d'optimisation {.smaller .scrollable}\n\n::: panel-tabset\n## Jointures\n\nPour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse\n\n![](join.png)\n\nSolution rapide :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable_finale <- table_volumineuse_comme_PJC %>%\n  right_join(petite_table_mon_champ)\n```\n:::\n\n\nSolution lente :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable_finale <- petite_table_mon_champ %>%\n  left_join(table_volumineuse_comme_PJC)\n```\n:::\n\n\n## Persist\n\nLorsqu'une table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c'est-√†-dire enregistrer ce spark_data_frame sur le disque ou dans la m√©moire des noeuds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable_1 <- mon_champ %>%\n  left_join(ODD, by = c(\"id_midas\", \"KROD3\")) %>%\n  rename(duree_potentielle_indemnisation = KPJDXP,\n         SJR = KQCSJP,\n         date_debut_indemnisation = KDPOD) %>%\n  sdf_persist()\n\nduree <- table_1 %>%\n  summarise(duree_moy = mean(duree_potentielle_indemnisation),\n            duree_med = median(duree_potentielle_indemnisation)) %>%\n  collect()\n\nSJR <- table_1 %>%\n  summarise(SJR_moy = mean(SJR),\n            SJR_med = median(SJR)) %>%\n  collect()\n```\n:::\n\n\n## Chargement\n\nLorsqu'on charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.\n\nAttention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l'ex√©cution.\n\n## Export et partitions\n\nLe format parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.\n\nSi les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d'une m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_write_parquet(DE, partition_by = c(\"REGIND\"))\nsdf_coalesce\n```\n:::\n\n:::\n\n## Forcer le calcul {.smaller}\n\nQuelques actions :\n\n-   collecter la table enti√®re üõë\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_data_frame_1 %>%\n      collect()\n    ```\n    :::\n\n\n-   afficher les premi√®res lignes\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_data_frame_1 %>%\n      head(10)\n    ```\n    :::\n\n\n-   Mettre les donner en cache\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    spark_data_frame_1 %>%\n      sdf_register() %>%\n      tbl_cache()\n    \n    sc %>% spark_session() %>% invoke(\"catalog\") %>% \n      invoke(\"clearCache\")\n    ```\n    :::\n\n\n## Les erreurs en sparklyr {.smaller}\n\nSparklyr traduit le code dplyr fourni en scala, mais interpr√®te √©galement les messages d'erreurs envoy√©s du cluster vers la session R.\n\nSparklyr n'est cependant pas performant pour interpr√©ter ces erreurs.\n\nN'h√©sitez pas √† enregistrer le code g√©n√©rant un message d'erreur dans Documents publics/erreurs_sparklyr\n\nUn test du code pas-√†-pas permet d'isoler le probl√®me.\n\n## Bonnes pratiques {.smaller}\n\n-   D√©connexion ou fermeture R pour lib√©rer les ressources üõë\n\n-   Ne plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\n\n-   Pyspark ou Sparklyr pour la production ‚ùì\n\n-   Utilisation parcimonieuse des ressources ‚öñÔ∏è\n\n-   Envoi des erreurs sparklyr üì©\n\n# Pour aller plus loin\n\n## L'architecture Map Reduce\n\n![](map_reduce.png)\n\n## La gestion de la m√©moire avec spark {.smaller .scrollable}\n\nLes shuffles sont les op√©rations les plus gourmandes en temps.\n\nSpark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d'optimiser les traitements :\n\n::: panel-tabset\n## DAG\n\n![](dag.webp)\n\n## GC\n\n![](gc.png)\n\n## M√©moire\n\n![](gc.png)\n:::\n\n## Utiliser les interfaces {.smaller}\n\n-   **yarn** : disponibilit√© des ressources\n\n    ![](yarn_scheduler.png){width=\"600\"}\n\n-   **Sparkhistory** pour des traitements de sessions ferm√©es\n\n## Exporter de HDFS au local {.smaller}\n\n![](hdfs_browse.png)\n\n## Pyspark : mode cluster\n\n![](pyspark.drawio.png)\n\n## Les avantages de pyspark {.smaller}\n\n-   Mode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è\n\n-   Spark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è\n\n-   Plusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª\n\n-   Stabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ\n\n-   Lisibilit√© du code üëì\n\n-   Temps de connexion et d'ex√©cution r√©duit ‚è≤Ô∏è\n\n-   Utilisation optimale de SparkUI üìä\n\n## Merci pour votre attention !\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}