[
  {
    "objectID": "slides.html#au-programme",
    "href": "slides.html#au-programme",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Au programme",
    "text": "Au programme\n\nMiDAS : une base de donn√©es volumineuse üíæ\nManipuler un appariement : une op√©ration co√ªteuse üí≤\nInitiation au calcul distribu√© üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nSparklyr : la solution ergonomique de sparkl sous R üë®‚Äçüíª\nPour aller plus loin ‚è©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS croise trois bases de donn√©es administratives exhaustives :\n\nles donn√©es sur l‚Äôinscription et l‚Äôindemnisation des demandeurs d‚Äôemploi de France Travail : le Fichier Historique Statistique (FHS) et le Fichier National des Allocataires (FNA) ;\nles donn√©es sur les b√©n√©ficiaires de minima sociaux (RSA, PPA, AAH) et les caract√©ristiques des m√©nages de la CNAF : Allstat-FR6 ;\nles donn√©es sur les contrats salari√©s de la DSN : MMO de la Dares.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nChaque vague de MiDAS correspond √† environ 600 Go de donn√©es au format sas. Les vagues fonctionnent par empilement :\n\nle gain de profondeur temporelle et l‚Äôentr√©e dans le champ de nouvelles personnes\nvagues sont appariables entre elles",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "href": "slides.html#midas-une-base-de-donn√©es-volumineuse-2",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "MiDAS : une base de donn√©es volumineuse",
    "text": "MiDAS : une base de donn√©es volumineuse\nMiDAS est l‚Äôune des bases de donn√©es les plus volumineuses du SSP :",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#structure-de-lappariement",
    "href": "slides.html#structure-de-lappariement",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Structure de l‚Äôappariement",
    "text": "Structure de l‚Äôappariement\n\n\n\n\n\n\n\nPourquoi Spark ?\n\n\nLa manipulation des donn√©es MiDAS en l‚Äô√©tat implique de nombreuses op√©rations de jointures qui n√©cessitent une puissance de calcul et un temps certains.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-format-parquet",
    "href": "slides.html#le-format-parquet",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le format parquet",
    "text": "Le format parquet\nLes donn√©es sont converties au format parquet d√®s leur r√©ception et mises √† disposition sur la bulle CASD du projet MiDares sous l‚Äôespace commun. Le format parquet est un format de donn√©es adapt√© aux donn√©es volumineuses :\n\nil compresse efficacement les donn√©es : taux de compression de 5 √† 10 par rapport au format csv\nil est orient√© colonnes\nil permet le chargement efficace en m√©moire des donn√©es\nIl permet le stockage partitionn√© des donn√©es\nil permet un traitement de cette partition qui conserve les donn√©es non n√©cessaires sur disque\nIl est ind√©pendant du logiciel utilis√© : il peut donc √™tre trait√© par spark et par R.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#lespace-midares",
    "href": "slides.html#lespace-midares",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôespace MiDares",
    "text": "L‚Äôespace MiDares\n\nRessourcesSch√©ma\n\n\nDes ressources partag√©es entre tous les utilsateurs simultan√©s :\n\n512 Go de m√©moire vive (ou RAM)\n\n\n\n\n\n\n\nLa m√©moire vive\n\n\nLa m√©moire vive, aussi appel√©e RAM, se distingue de la m√©moire de stockage (disque) par sa rapidit√©, notamment pour fournir des donn√©es au processeur pour effectuer des calculs, par sa volatilit√© (toutes les donn√©es sont perdues si l‚Äôordinateur n‚Äôest plus aliment√©) et par l‚Äôacc√®s direct aux informations qui y sont stock√©es, quasi instantann√©.\n\n\n\n\nUn processeur (ou CPU) compos√© de 32 coeurs\n\n\n\n\n\n\n\nLe processeur\n\n\nLe processeur permet d‚Äôex√©cuter des t√¢ches et des programmes : convertir un fichier, ex√©cuter un logiciel‚Ä¶ Il est compos√© d‚Äôun ou de plusieurs coeurs : un coeur ne peut ex√©cuter qu‚Äôune seule t√¢che √† la fois. Si le processeur contient plusieurs coeurs, il peut ex√©cuter autant de t√¢ches en parall√®le qu‚Äôil a de coeurs. Un processeur se caract√©rise aussi par sa fr√©quence : elle est globalement proportionnelle au nombre d‚Äôop√©ration qu‚Äôil est capable d‚Äôeffetuer par seconde.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#programmer-en-m√©moire-vive",
    "href": "slides.html#programmer-en-m√©moire-vive",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Programmer en m√©moire vive",
    "text": "Programmer en m√©moire vive\n\nR : la m√©moire vive, √©tat dans l‚Äôenvironnement\nSAS : lecture/√©criture sur le disque\nMiDAS au format sas &gt;&gt; taille de la m√©moire vive disponible du serveur CASD ‚Äì&gt; format parquet\nImpossible de charger tout MiDAS en m√©moire vive\nDes solutions existent pour manipuler les donn√©es sous R sans les charger enti√®rement en m√©moire vive :\narrow (avec des requ√™tes dplyr)\nduckDB\n‚ñ∂Ô∏è Insuffisantes pour les traitements les plus co√ªteux sur MiDAS en R : la partie de la m√©moire vive utilis√©e pour stocker les donn√©es correspond √† autant de puissance de calcul indisponible pour les traitements.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "href": "slides.html#les-traitements-co√ªteux-en-puissance-de-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les traitements co√ªteux en puissance de calcul",
    "text": "Les traitements co√ªteux en puissance de calcul\n\nles jointures\nles op√©rations en group_by\ndistinct\n‚ñ∂Ô∏è Ex√©cution s√©quentielle sur un coeur du processeur + beaucoup de m√©moire vive (donn√©es temporaires)\n‚ñ∂Ô∏è Erreur ‚Äúout of memory‚Äù.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#un-traitement-peu-co√ªteux",
    "href": "slides.html#un-traitement-peu-co√ªteux",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement peu co√ªteux",
    "text": "Un traitement peu co√ªteux\n\nCe traitement est peu co√ªteux :\n\nchargement d‚Äôune seule colonne en RAM : format parquet orient√© colonnes\npeu de m√©moire d‚Äôex√©cution : R est un langage vectoris√©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#un-traitement-co√ªteux",
    "href": "slides.html#un-traitement-co√ªteux",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Un traitement co√ªteux",
    "text": "Un traitement co√ªteux\n\nCe traitement n√©cessite :\n\nle chargement de davantage de colonnes en m√©moire vive ;\ndavantage de m√©moire d‚Äôex√©cution pour effectuer l‚Äôintersection (inner_join).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "href": "slides.html#calcul-distribu√©-et-calcul-parall√®le",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Calcul distribu√© et calcul parall√®le",
    "text": "Calcul distribu√© et calcul parall√®le\n\nCalcul non distribu√©Calcul distribu√© avec spark\n\n\nLorsqu‚Äôun traitement Big Data est demand√© par l‚Äôutilisateur dans la session R, plusieurs probl√®mes peuvent se poser :\n\nla taille des donn√©es : charg√©es en m√©moire pour effectuer les calculs avec R\nle temps de calcul : si plusieurs √©tapes sont n√©cessaires pour un traitement, elles sont effectu√©es de mani√®re s√©quentielle par le processeur (tr√®s long)\nl‚Äôoptimisation du programme\n\n\n\nLe calcul distribu√© avec spark apporte une solution √† ces probl√©matiques :\n\nchargement des donn√©es en m√©moire parcimonieux et non syst√©matique\nex√©cution de t√¢ches en parall√®le sur plusieurs coeurs du processeur, voire sur plusieurs ordinateurs diff√©rents\noptimisation automatique du code",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-cluster-de-calcul-midares-mode-interactif",
    "href": "slides.html#le-cluster-de-calcul-midares-mode-interactif",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le cluster de calcul Midares : mode interactif",
    "text": "Le cluster de calcul Midares : mode interactif",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#spark",
    "href": "slides.html#spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Spark",
    "text": "Spark\n\nApache Spark : librairie open source d√©velopp√©e dans le langage Scala\nScala : langage compil√©, rapide et distribuable qui peut √™tre ex√©cut√© dans une machine virtuelle Java\nExemple Scala ?\nScala adapt√© pour ma√Ætriser toutes les fonctionnalit√©s de Spark et optimiser au maximum les traitements en spark\nSpark est compatible avec les langages Scala, R, Python, Java, et peut interpr√©ter des commandes SQL.\nDeux packages existent sous R :\n\nsparkR propos√© par Apache Spark\nsparklyr, qui permet d‚Äôutiliser directement des commandes dplyr traduites en spark par le package.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#mode-local-concurrence",
    "href": "slides.html#mode-local-concurrence",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : concurrence",
    "text": "Mode local : concurrence",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#mode-cluster-non-concurrence",
    "href": "slides.html#mode-cluster-non-concurrence",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode cluster : non concurrence",
    "text": "Mode cluster : non concurrence\n\nLe mode cluster permet une r√©elle distribution sur diff√©rents noeuds, qui sont en fait des ordinateurs distincts d‚Äôun serveur. Ces machines communiquent en r√©seau.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#installation-de-spark-sous-casd",
    "href": "slides.html#installation-de-spark-sous-casd",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Installation de spark sous CASD",
    "text": "Installation de spark sous CASD\nVoir la fiche d√©dier sur le site",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-stockage-distribu√©-hdfs",
    "href": "slides.html#le-stockage-distribu√©-hdfs",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le stockage distribu√© : HDFS",
    "text": "Le stockage distribu√© : HDFS\n\nstockage sur diff√©rentes machines : ici les noeuds du cluster spark, c‚Äôest-√†-dire les diff√©rents ordinateurs workers du cluster\ndonn√©es divis√©es en blocs plus petits de taille fixe et r√©partis sur les machines\nchaque bloc est r√©pliqu√© trois fois pour √™tre r√©silient face aux pannes\nun NameNode supervise les m√©tadonn√©es et g√®re la structure du syst√®me de fichiers\nles DataNodes stockent effectivement les blocs de donn√©es\nle syst√®me HDFS est reli√© √† la bulle Midares : possible de charger des donn√©es en clique-bouton de la bulle vers HDFS de mani√®re tr√®s rapide et de t√©l√©charger des tables de HDFS pour les r√©cup√©rer en local\n\n\n\n\n\n\nLes exports sur HDFS\n\n\nLorsqu‚Äôon exporte une table depuis notre session R vers HDFS, celle-ci est automatiquement partitionn√©e, comme le reste des donn√©es.\nAinsi, cette table sera stock√©e en plusieurs morceaux sous HDFS et r√©pliqu√©e.\nIl est possible de ma√Ætriser le nombre de partitions avec la commande sdf_coalesce(partitions = 5) du package sparklyr.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-stockage-distribu√©-hdfs-1",
    "href": "slides.html#le-stockage-distribu√©-hdfs-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le stockage distribu√© : HDFS",
    "text": "Le stockage distribu√© : HDFS\n\n‚ñ∂Ô∏è Les r√©plications de donn√©es ont deux fonctions :\n\naugementer la flexibilit√© de la distribution des traitements\naugmenter la r√©silience en cas de panne d‚Äôun noeud",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#la-lazy-evaluation",
    "href": "slides.html#la-lazy-evaluation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark distingue deux types d‚Äôop√©rations :\n\nles transformations : ce sont des op√©rations qui prennent en entr√©e un spark_data_frame et retournent un spark_data_frame, elles ne d√©clenchent aucun calcul lorsqu‚Äôelles sont appel√©es.\nPar exemple, le programme ci-dessous est compil√© instantan√©ment et ne d√©clenche pas d‚Äôex√©cution :\n\nune_transformation &lt;- un_spark_data_frame %&gt;%\n  group_by(identifiant) %&gt;%\n  mutate(une_somme = sum(revenus))\n\nles actions : ce sont des op√©rations qui demandent le calcul d‚Äôun r√©sultat et qui d√©clenchent le calcul et l‚Äôex√©cution de toutes les transformations compil√©es jusqu‚Äô√† l‚Äôappel de l‚Äôaction.\nPar exemple, le programme ci-dessous d√©clenche le calcul de la cellule `une_transformation` et de la moyenne des revenus :\n\nrevenu_moyen &lt;- une_transformation %&gt;%\n  summarise(revenu_moyen = mean(une_somme)) %&gt;%\n  print()\n\nLes principales actions sont : print(), collect(), head(), tbl_cache() (√©crire un spark_data_frame en m√©moire pour le r√©utiliser).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#la-lazy-evaluation-1",
    "href": "slides.html#la-lazy-evaluation-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La lazy evaluation",
    "text": "La lazy evaluation\nSpark optimise automatiquement les programmes soumis :\n\nCompilation des transformations\nInt√©gration dans un plan d‚Äôex√©cution : √©ventuelles erreurs du programme soulev√©es avant l‚Äôex√©cution\nOptimisation du plan logique par le module Catalyst (driver Spark)\nPar exemple si j‚Äô√©cris le programme :\n\nnon_optimal &lt;- table_1 %&gt;%\n  mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat)) %&gt;%\n  filter(debut_contrat &gt;= as.Date(\"2023-01-01\"))\n\nCatalyst r√©√©crit :\n\nnon_optimal &lt;- table_1 %&gt;%\n  filter(debut_contrat &gt;= as.Date(\"2023-01-01\")) %&gt;%\n  mutate(duree_contrat = DATEDIFF(fin_contrat, debut_contrat))\n\nCette optimisation est r√©alis√©e sur toutes les transformations compil√©e avant qu‚Äôune action d√©clenche l‚Äôex√©cution.\nD√©clencher le moins d‚Äôactions possibles dans son programme permet de tirer pleinement parti de Catalyst et de gagner un temps certain.\nPour profiter des avantages de spark, la mani√®re de programmer recommand√©e est diff√©rente de celle pr√©dominante en R classique.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#le-plan-dex√©cution",
    "href": "slides.html#le-plan-dex√©cution",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Le plan d‚Äôex√©cution",
    "text": "Le plan d‚Äôex√©cution\n\nsource : Documentation Data Science",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#r√©cup√©rer-un-r√©sultat",
    "href": "slides.html#r√©cup√©rer-un-r√©sultat",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "R√©cup√©rer un r√©sultat",
    "text": "R√©cup√©rer un r√©sultat\nLes r√©sultats qu‚Äôil est recommand√© de r√©cup√©rer en m√©moire vive en session R sont de la forme suivante :\n\nune table filtr√©e avec les variables n√©cessaires √† l‚Äô√©tude uniquement : sous MiDAS, toutes les jointures, les calculs de variable et les filtres peuvent √™tre effectu√©s de mani√®re efficiente sous la forme de spark_data_frame, sans jamais collecter les donn√©es MiDAS ;\ndes statistiques descriptives synth√©tiques ;\nles premi√®res lignes de la table pour v√©rifier que le programme retourne bien le r√©sultat attendu ;\nune table agr√©g√©e pour un graphique par exemple, √† l‚Äôaide de la fonction summarise().",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#sparklyr-et-sparkr",
    "href": "slides.html#sparklyr-et-sparkr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Sparklyr et SparkR",
    "text": "Sparklyr et SparkR\nDeux packages permettent de programmer avec Spark sous R :\n\nSparkR : ce package, maintenu par Apache Spark, permet d‚Äôutiliser une syntaxe proche de spark, scala, ou directement du code SQL pour manipuler des donn√©es dans une session R.\nSparklyr : ce package permet d‚Äôutiliser directement la syntaxe dplyr dans une session Spark sous R.\n\nSparklyr fonctionne selon ces √©tapes :\n\nLa JVM driver spark est instanci√©e dans la bulle Midares pour utiliser sparklyr.\nLes instructions dplyr appel√©es sur un spark_data_frame sont traduites par les fonctions du package sparklyr en scala, puis envoy√©es au driver.\nLe programme en scala est ex√©cut√© sur le cluster.\nSi une erreur est renvoy√©e par le driver, elle est interpr√©t√©e par R avant d‚Äô√™tre affich√©e en session R.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#configuration-cluster",
    "href": "slides.html#configuration-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Configuration cluster",
    "text": "Configuration cluster\nDeux √©tapes majeures dans le traitement de donn√©es sous R diff√®re en sparklyr par rapport √† une programmation classique en dplyr :\n\nConfigurationImport-export\n\n\nIl est n√©cessaire de configurer la session spark pour √©tablir une connexion entre la session R et un cluster spark. Les param√®tres √† d√©finir sont :\n\nLes ressources physiques utilis√©es :\n\npar le driver : avec spark.driver.memory\npar chaque worker avec spark.executor.memory et spark.executor.cores\nle nombre de worker avec spark.executor.instances\nLa file sur laquelle on travaille avec spark.yarn.queue\n\nle nombre de partitions de chaque spark_data_frame avec spark.sql.shuffle.partitions\nla limite de taille des r√©sulats qui peuvent √™tre collect√©s par le driver avec spark.driver.maxResultSize\n\n\nconf &lt;- spark_config()\nconf[\"spark.driver.memory\"] &lt;- \"40Go\"\nconf[\"spark.executor.memory\"] &lt;- \"80Go\"\nconf[\"spark.executor.cores\"] &lt;- 5\nconf[\"spark.executor.instances\"] &lt;- 2\ncont[\"spark.yarn.queue\"] &lt;- \"prod\"\nconf[\"spark.driver.maxResultSize\"] &lt;- 0\nconf[\"spark.sql.shuffle.partitions\"] &lt;- 200\n\nsc &lt;- spark_connect(master = \"yarn\", config = conf)\n\n\n\nLes donn√©es doivent √™tre disponibles dans les workers sous forme de spark_data_frame :\n\ncach√© en m√©moire directement : si utilis√©es plusieurs fois pour gagner du temps\nlaiss√© sur disque tant qu‚Äôaucune action ne d√©clenche un traitement qui n√©cessite son chargement en m√©moire\n‚ñ∂Ô∏è chargement en m√©moire vive couteux en temps : avec la configuration pr√©sent√©e, le chargement du FNA, du FHS et des MMO prend au moins 25 minutes.\nPour passer un data.frame R en spark_data_frame : copy_to()",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "href": "slides.html#lutilisation-de-la-m√©moire-dans-un-worker",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire dans un worker",
    "text": "L‚Äôutilisation de la m√©moire dans un worker\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nNe pas charger plusieurs fois les m√™mes donn√©es en cache",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "href": "slides.html#lutilisation-de-la-m√©moire-du-driver",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôutilisation de la m√©moire du driver",
    "text": "L‚Äôutilisation de la m√©moire du driver",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#ce-qui-change-pour-lutilisateur",
    "href": "slides.html#ce-qui-change-pour-lutilisateur",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Ce qui change pour l‚Äôutilisateur",
    "text": "Ce qui change pour l‚Äôutilisateur\nLa majorit√© des commandes dplyr fonctionnent sur un spark_data_frame avec le package sparklyr. Les divergences sont les suivantes :\n\npour effectuer des op√©rations avec les dates, il faut utiliser les fonctions Hive sp√©cifiques.\narrange() ne fonctionne pas sur un spark_data_frame, il faut lui substituer window_order.\ndes fonctions sp√©cifiques aux spark data frames : sdf_bind_rows pour empiler les lignes par exemple.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#quelques-fonctions-sp√©cifiques",
    "href": "slides.html#quelques-fonctions-sp√©cifiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques fonctions sp√©cifiques",
    "text": "Quelques fonctions sp√©cifiques\n\nDatesTableauStatistiques\n\n\nLes fonctions de lubridate()ne sont pas adapt√©es au spark_data_frames.\n\nConvertir une cha√Æne de caract√®re de la forme AAAA-MM-DD en Date\n\ndate_1 &lt;- as.Date(\"2024-05-26\")\n\nCalculer une dur√©e entre deux dates\n\nPJC_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/pjc.parquet\",\n                                memory = FALSE)\n\nduree_pjc_df &lt;- PJC_spark %&gt;%\n  rename(date_fin_pjc = as.Date(KDFPJ),\n         date_deb_pjc = as.Date(KDDPJ)) %&gt;%\n  mutate(duree_pjc = datediff(date_fin_pjc, date_deb_pjc) + 1) %&gt;%\n  head(5)\n\nAjouter ou soustraire des jours ou des mois √† une date\n\nduree_pjc_bis_df &lt;- duree_pjc_df %&gt;%\n  mutate(duree_pjc_plus_5 = date_add(duree_pjc, int(5)),\n         duree_pjc_moins_5 = date_sub(duree_pjc, int(5)),\n         duree_pjc_plus_1_mois = add_months(duree_pjc, int(1))) %&gt;%\n  head(5)\n\n\n\n\n\n\n\n\nAdd_months\n\n\nSi la date en entr√©e est le dernier jour d‚Äôun mois, la date retourn√©e avec add_months(date_entree, int(1)) sera le dernier jour calendaire du mois suivant.\n\n\n\n\n\n\n\n\n\nFormat\n\n\nLe int() est important car ces fonctions Hive n‚Äôaccepte que les entiers pour l‚Äôajout de jours : taper uniquement 5 est consid√©r√© comme un flottant dans R.\n\n\n\n\n\n\nTri dans un groupe pour effectuer un calcul s√©quentiel\n\nODD_spark &lt;- spark_read_parquet(sc,\n                                path = \"hdfs:///dataset/MiDAS_v4/odd.parquet\",\n                                memory = FALSE)\n\nODD_premier &lt;- ODD_spark %&gt;%\n  group_by(id_midas) %&gt;%\n  window_order(id_midas, KDPOD) %&gt;%\n  mutate(date_premier_droit = first(KDPOD)) %&gt;%\n  ungroup() %&gt;%\n  distinct(id_midas, KROD3, date_premier_droit) %&gt;%\n  head(5)\n\nTri pour une sortie : sdf_sort() , arrange() ne fonctionne pas\nConcat√©ner les lignes (ou les colonnes sdf_bind_cols())\n\nODD_1 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &lt;= as.Date(\"2017-12-31\")) %&gt;%\n  mutate(groupe = \"temoins\")\n\nODD_2 &lt;- ODD_spark %&gt;%\n  filter(KDPOD &gt;= as.Date(\"2021-12-31\")) %&gt;%\n  mutate(groupe = \"traites\")\n\nODD_evaluation &lt;- sdf_bind_rows(ODD_1, ODD_2)\n\nD√©doublonner une table\n\ndroits_dans_PJC &lt;- PJC_spark %&gt;%\n  sdf_distinct(id_midas, KROD3)\n\nprint(head(droits_dans_PJC, 5))\n\nPJC_dedoublonnee &lt;- PJC_spark %&gt;%\n  sdf_drop_duplicates()\n\nprint(head(PJC_dedoublonnee, 5))\n\nPivot : les fonctions du packag tidyr ne fonctionnent pas sur donn√©es spark\n\nODD_sjr_moyen &lt;- ODD_spark %&gt;%\n  mutate(groupe = ifelse(KDPOD &lt;= as.Date(\"2020-12-31\"), \"controles\", \"traites\")) %&gt;%\n  sdf_pivot(groupe ~ KCRGC,\n    fun.aggregate = list(KQCSJP = \"mean\")\n  )\n\n\n\n\n\nR√©sum√© statistique : sdf_describe() , summary()ne fonctionne pas.\nDimension : sdf_dim, la fonction nrow()ne fonctionne pas.\nQuantiles approximatifs : le calcul des quantiles sur donn√©es distirbu√©es renvoie une approximation car toutes les donn√©es ne peuvent pas √™tre rappatri√©es sur la m√™me machine physique du fait de la volum√©trie, sdf_quantile()\nEchantillonnage al√©atoire : sdf_random_split",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#une-r√®gle-dor-tester-son-code-pour-collecter-le-moins-possible",
    "href": "slides.html#une-r√®gle-dor-tester-son-code-pour-collecter-le-moins-possible",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Une r√®gle d‚Äôor : tester son code pour collecter le moins possible",
    "text": "Une r√®gle d‚Äôor : tester son code pour collecter le moins possible\nLa programmation en spark doit √™tre adapt√©e aux contraintes de volum√©trie des donn√©es : test de chaque √©tape, puis ne forcer le calcul qu‚Äô√† la fin pour que Catalyst optimise l‚Äôensemble du programme\nLa principale diff√©rence avec la programmation en R classique est que la visualisation de tables compl√®tes volumineuses n‚Äôest pas recommand√©e :\n\ngoulets d‚Äô√©tranglement m√™me avec spark, car toutes les donn√©es sont rapatri√©es vers le driver puis vers la session R ;\nlongue : √©change entre tous les noeuds impliqu√©s dans le calcul et le driver, puis un √©change driver-session R ;\nbeaucoup moins efficace que l‚Äôexport direct en parquet du r√©sultat (presque instantann√©) : charger ensuite sa table finale en data frame R classique pour effectuer l‚Äô√©tude.\n\nS‚Äôil est n√©cessaire de collecter, il faut pr√©voir beaucoup de RAM pour le driver avec le param√®tre ‚Äúspark.driver.memory‚Äù.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#quelques-tips-doptimisation",
    "href": "slides.html#quelques-tips-doptimisation",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Quelques tips d‚Äôoptimisation",
    "text": "Quelques tips d‚Äôoptimisation\n\nJointuresPersistChargementExport et partitions\n\n\nPour effectuer ce type de jointure avec deux tables de volum√©tries diff√©rentes : A est petite, B est tr√®s volumineuse\n\nSolution rapide :\n\ntable_finale &lt;- table_volumineuse_comme_PJC %&gt;%\n  right_join(petite_table_mon_champ)\n\nSolution lente :\n\ntable_finale &lt;- petite_table_mon_champ %&gt;%\n  left_join(table_volumineuse_comme_PJC)\n\n\n\nLorsqu‚Äôune table interm√©diaire est utilis√©e plusieurs fois dans un traitement, il est possible de la persister, c‚Äôest-√†-dire enregistrer ce spark_data_frame sur le disque ou dans la m√©moire des noeuds.\n\ntable_1 &lt;- mon_champ %&gt;%\n  left_join(ODD, by = c(\"id_midas\", \"KROD3\")) %&gt;%\n  rename(duree_potentielle_indemnisation = KPJDXP,\n         SJR = KQCSJP,\n         date_debut_indemnisation = KDPOD) %&gt;%\n  sdf_persist()\n\nduree &lt;- table_1 %&gt;%\n  summarise(duree_moy = mean(duree_potentielle_indemnisation),\n            duree_med = median(duree_potentielle_indemnisation)) %&gt;%\n  collect()\n\nSJR &lt;- table_1 %&gt;%\n  summarise(SJR_moy = mean(SJR),\n            SJR_med = median(SJR)) %&gt;%\n  collect()\n\n\n\nLorsqu‚Äôon charge des donn√©es dans le cluster Spark et que la table est appel√©e plusieurs fois dans le programme, il est conseill√© de la charger en m√©moire vive directement.\nAttention, si beaucoup de tables volumineuses sont charg√©es en m√©moire, la fraction de la m√©moire spark d√©di√©e au stockage peut √™tre insuffisante ou bien il peut ne pas rester assez de spark memory pour l‚Äôex√©cution.\n\n\nLe format parquet (avec arrow) et le framework spark permettent de g√©rer le partitionnement des donn√©es.\nSi les op√©rations sont souvent effectu√©es par r√©gions par exemple, il est utile de forcer le stockage des donn√©es d‚Äôune m√™me r√©gion au m√™me endroit physique et acc√©l√®re drastiquement le temps de calcul\n\nspark_write_parquet(DE, partition_by = c(\"REGIND\"))\nsdf_coalesce",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#forcer-le-calcul",
    "href": "slides.html#forcer-le-calcul",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Forcer le calcul",
    "text": "Forcer le calcul\nQuelques actions :\n\ncollecter la table enti√®re üõë\n\nspark_data_frame_1 %&gt;%\n  collect()\n\nafficher les premi√®res lignes\n\nspark_data_frame_1 %&gt;%\n  head(10)\n\nMettre les donner en cache\n\nspark_data_frame_1 %&gt;%\n  sdf_register() %&gt;%\n  tbl_cache()\n\nsc %&gt;% spark_session() %&gt;% invoke(\"catalog\") %&gt;% \n  invoke(\"clearCache\")",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#les-erreurs-en-sparklyr",
    "href": "slides.html#les-erreurs-en-sparklyr",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les erreurs en sparklyr",
    "text": "Les erreurs en sparklyr\nSparklyr traduit le code dplyr fourni en scala, mais interpr√®te √©galement les messages d‚Äôerreurs envoy√©s du cluster vers la session R.\nSparklyr n‚Äôest cependant pas performant pour interpr√©ter ces erreurs.\nN‚Äôh√©sitez pas √† enregistrer le code g√©n√©rant un message d‚Äôerreur dans Documents publics/erreurs_sparklyr\nUn test du code pas-√†-pas permet d‚Äôisoler le probl√®me.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#bonnes-pratiques",
    "href": "slides.html#bonnes-pratiques",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\n\nD√©connexion ou fermeture R pour lib√©rer les ressources üõë\nNe plus utiliser spark en local üñ•Ô∏èüñ•Ô∏èüñ•Ô∏è\nPyspark ou Sparklyr pour la production ‚ùì\nUtilisation parcimonieuse des ressources ‚öñÔ∏è\nEnvoi des erreurs sparklyr üì©",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#larchitecture-map-reduce",
    "href": "slides.html#larchitecture-map-reduce",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "L‚Äôarchitecture Map Reduce",
    "text": "L‚Äôarchitecture Map Reduce",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "href": "slides.html#la-gestion-de-la-m√©moire-avec-spark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "La gestion de la m√©moire avec spark",
    "text": "La gestion de la m√©moire avec spark\nLes shuffles sont les op√©rations les plus gourmandes en temps.\nSpark UI permet de consulter le plan logique et physique du traitement demand√©. Trois outils permettent d‚Äôoptimiser les traitements :\n\nDAGGCM√©moire",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#utiliser-les-interfaces",
    "href": "slides.html#utiliser-les-interfaces",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Utiliser les interfaces",
    "text": "Utiliser les interfaces\n\nyarn : disponibilit√© des ressources\n\nSparkhistory pour des traitements de sessions ferm√©es",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#exporter-de-hdfs-au-local",
    "href": "slides.html#exporter-de-hdfs-au-local",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Exporter de HDFS au local",
    "text": "Exporter de HDFS au local",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#pyspark-mode-cluster",
    "href": "slides.html#pyspark-mode-cluster",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Pyspark : mode cluster",
    "text": "Pyspark : mode cluster",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#les-avantages-de-pyspark",
    "href": "slides.html#les-avantages-de-pyspark",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Les avantages de pyspark",
    "text": "Les avantages de pyspark\n\nMode cluster : une machine du cluster peut prendre le r√¥le de driver üñ•Ô∏è\nSpark context dans le cluster : fermer sa session anaconda ne stoppe pas le traitement ‚ôæÔ∏è\nPlusieurs sessions simultan√©es üë©‚Äçüíªüë©‚Äçüíªüë©‚Äçüíª\nStabilit√© : compatibilit√© assur√©e avec Apache Spark, probl√©matique de production üîÑ\nLisibilit√© du code üëì\nTemps de connexion et d‚Äôex√©cution r√©duit ‚è≤Ô∏è\nUtilisation optimale de SparkUI üìä",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#merci-pour-votre-attention",
    "href": "slides.html#merci-pour-votre-attention",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Merci pour votre attention !",
    "text": "Merci pour votre attention !",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#mode-local-concurrence-1",
    "href": "slides.html#mode-local-concurrence-1",
    "title": "Initiation √† Spark avec R en mode cluster",
    "section": "Mode local : concurrence",
    "text": "Mode local : concurrence\nEn mode local :\n\nune unique machine Java\nparall√©lisation des t√¢ches sur diff√©rents coeurs de cette machine virtuelle\npas de stockage distribu√©, ca n‚Äôest pas du calcul distribu√© √† proprement parler\nacc√©l√©ration par rapport √† un mode de programmation classique s√©quentiel sur un unique coeur si beaucoup de ressources\nSur la bulle CASD, mauvaise gestion de la r√©partition des ressources en spark local\n‚ñ∂Ô∏èmode local √† √©viter absolument",
    "crumbs": [
      "Slides"
    ]
  }
]